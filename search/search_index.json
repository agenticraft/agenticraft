{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd16 Welcome to AgentiCraft","text":"<p>Build AI agents as simple as writing Python</p> <p> </p>"},{"location":"#why-agenticraft","title":"Why AgentiCraft?","text":"<p>Building AI agents should be as simple as writing Python. No complex abstractions, no graph theory, no 500-page documentation. Just clean, simple code that works.</p> <p>5-Minute Quickstart</p> <p>Get your first agent running faster than making coffee. Start here \u2192</p>"},{"location":"#core-principles","title":"Core Principles","text":"<ul> <li> <p>\ud83d\ude80 Simple First   If it's not simple, it's not in core. Every API designed for developer joy.</p> </li> <li> <p>\ud83e\udde0 Transparent Reasoning   See how your agents think, not just what they output. No black boxes.</p> </li> <li> <p>\ud83d\udd0c MCP-Native   First-class Model Context Protocol support. Use any MCP tool seamlessly.</p> </li> <li> <p>\ud83d\udcc8 Production Ready   Built-in observability, templates, and best practices from day one.</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from agenticraft import Agent, tool\n\n@tool\ndef calculate(expression: str) -&gt; float:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return eval(expression, {\"__builtins__\": {}}, {})\n\nagent = Agent(\n    name=\"MathAssistant\",\n    instructions=\"You are a helpful math assistant.\",\n    tools=[calculate]\n)\n\nresponse = agent.run(\"What's 42 * 17 + 238?\")\nprint(response.content)\n# Output: \"Let me calculate that for you: 42 * 17 + 238 = 952\"\n\n# See the reasoning process\nprint(response.reasoning)\n# Output: \"I need to calculate the expression 42 * 17 + 238...\"\n</code></pre>"},{"location":"#what-makes-agenticraft-different","title":"What Makes AgentiCraft Different?","text":"Feature AgentiCraft Others Setup Time 1 minute 5-30 minutes Core Size &lt;2,000 lines 50,000+ lines Time to First Agent 5 minutes 30+ minutes Documentation 100% coverage Variable Reasoning Visibility Built-in Often hidden Production Templates Included DIY"},{"location":"#features-at-a-glance","title":"Features at a Glance","text":""},{"location":"#simple-agent-creation","title":"\ud83c\udfaf Simple Agent Creation","text":"<p><pre><code>agent = Agent(name=\"assistant\")\n</code></pre> That's it. No configuration files, no complex setup.</p>"},{"location":"#transparent-reasoning","title":"\ud83e\udde0 Transparent Reasoning","text":"<pre><code>response = agent.run(\"Complex question\")\nprint(response.reasoning)  # See the thought process\n</code></pre>"},{"location":"#easy-tool-integration","title":"\ud83d\udd27 Easy Tool Integration","text":"<pre><code>@tool\ndef my_tool(param: str) -&gt; str:\n    return f\"Processed: {param}\"\n\nagent = Agent(tools=[my_tool])\n</code></pre>"},{"location":"#simple-workflows","title":"\ud83d\udd04 Simple Workflows","text":"<pre><code>workflow = Workflow(name=\"pipeline\")\nworkflow.add_steps([\n    Step(\"research\", agent=researcher),\n    Step(\"write\", agent=writer, depends_on=[\"research\"])\n])\n</code></pre>"},{"location":"#built-in-observability","title":"\ud83d\udcca Built-in Observability","text":"<pre><code>from agenticraft import Telemetry\n\ntelemetry = Telemetry(export_to=\"http://localhost:4317\")\nagent = Agent(telemetry=telemetry)\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>5-Minute Quickstart   Build your first agent faster than making coffee</p> </li> <li> <p>Core Concepts   Understand the fundamentals</p> </li> <li> <p>API Reference   Detailed documentation for every feature</p> </li> <li> <p>Examples   Learn from real-world implementations</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<p>Join our growing community of developers building with AgentiCraft:</p> <ul> <li>Discord - Get help and share ideas</li> <li>GitHub - Contribute and star the project</li> <li>Blog - Latest updates and tutorials</li> </ul>"},{"location":"#ready-to-build","title":"Ready to Build?","text":"<p>Get Started \u2192 | View Examples \u2192</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to AgentiCraft will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#011-2025-06-11","title":"[0.1.1] - 2025-06-11","text":""},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":""},{"location":"changelog/#dynamic-provider-switching","title":"Dynamic Provider Switching","text":"<ul> <li>Switch between LLM providers at runtime without recreating agents</li> <li>New <code>set_provider()</code> method for instant provider changes</li> <li><code>get_provider_info()</code> to check current provider details</li> <li><code>list_available_providers()</code> to discover available options</li> <li>Explicit provider selection via <code>provider</code> parameter</li> </ul>"},{"location":"changelog/#new-llm-providers","title":"New LLM Providers","text":"<ul> <li>Anthropic Claude - Access to Opus, Sonnet, and Haiku models</li> <li>Ollama - Run models locally (Llama2, Mistral, CodeLlama, and more)</li> </ul>"},{"location":"changelog/#advanced-agent-types","title":"Advanced Agent Types","text":"<ul> <li>ReasoningAgent - Transparent thought process with explainable AI</li> <li>WorkflowAgent - Optimized for complex multi-step processes</li> </ul>"},{"location":"changelog/#performance","title":"\u26a1 Performance","text":"<ul> <li>Provider switching overhead &lt; 50ms</li> <li>Connection pooling for high-throughput scenarios</li> <li>Optimized provider initialization and caching</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Comprehensive feature guides</li> <li>Real-world examples and use cases</li> <li>Complete API reference</li> <li>Performance optimization guide</li> </ul>"},{"location":"changelog/#developer-experience","title":"\ud83d\udd27 Developer Experience","text":"<ul> <li>Improved error messages for provider issues</li> <li>Better type hints and IDE support</li> <li>Enhanced debugging capabilities</li> </ul>"},{"location":"changelog/#upcoming","title":"[Upcoming]","text":""},{"location":"changelog/#planned-features","title":"Planned Features","text":"<ul> <li>Unified streaming interface across all providers</li> <li>Automatic model selection based on task complexity</li> <li>Built-in cost tracking and usage analytics</li> <li>Provider configuration profiles</li> <li>Enhanced tool calling standardization</li> </ul>"},{"location":"philosophy/","title":"Philosophy","text":"<p>AgentiCraft is built on strong principles that guide every design decision. Understanding our philosophy helps you get the most out of the framework.</p>"},{"location":"philosophy/#core-principles","title":"Core Principles","text":""},{"location":"philosophy/#1-simplicity-first","title":"1. Simplicity First \ud83c\udfaf","text":"<p>\"Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.\" - Antoine de Saint-Exup\u00e9ry</p> <p>Every feature in AgentiCraft must pass the simplicity test: - Can it be explained in one sentence? - Can a developer use it without reading docs? - Does it solve a real problem?</p> <p>If the answer to any of these is \"no\", it doesn't belong in core.</p> <p>Example: Creating an agent <pre><code># This is all you need\nagent = Agent(name=\"assistant\")\n</code></pre></p>"},{"location":"philosophy/#2-transparent-by-default","title":"2. Transparent by Default \ud83e\udde0","text":"<p>AI agents shouldn't be black boxes. Developers need to understand: - What the agent is thinking - Why it made certain decisions - How it arrived at its response</p> <p>Example: Reasoning visibility <pre><code>response = agent.run(\"Complex question\")\nprint(response.reasoning)  # Always available\n</code></pre></p>"},{"location":"philosophy/#3-production-ready-from-day-one","title":"3. Production-Ready from Day One \ud83d\udcca","text":"<p>Demos are easy. Production is hard. AgentiCraft bridges this gap: - Built-in observability - Error handling that makes sense - Templates for common use cases - Performance considerations baked in</p>"},{"location":"philosophy/#4-developer-joy","title":"4. Developer Joy \ud83d\udc9c","text":"<p>Writing agent code should be enjoyable: - Intuitive APIs that feel natural - Clear error messages that help, not frustrate - Excellent documentation with real examples - Fast feedback loops</p>"},{"location":"philosophy/#5-extensible-not-bloated","title":"5. Extensible, Not Bloated \ud83d\udd27","text":"<p>The core stays small (&lt;2000 lines), but the possibilities are endless: - Plugin architecture for custom needs - Standard interfaces for interoperability - Community-driven ecosystem</p>"},{"location":"philosophy/#design-decisions","title":"Design Decisions","text":""},{"location":"philosophy/#why-not-graph-based-workflows","title":"Why Not Graph-Based Workflows?","text":"<p>Many frameworks use complex graph structures for workflows. We chose simplicity:</p> <pre><code># AgentiCraft way - simple and clear\nworkflow.add_steps([\n    Step(\"research\", agent=researcher),\n    Step(\"write\", agent=writer, depends_on=[\"research\"])\n])\n\n# Not the AgentiCraft way - unnecessary complexity\nworkflow.add_node(\"research\", ResearchNode())\nworkflow.add_edge(\"research\", \"write\", condition=lambda x: x.success)\n</code></pre> <p>Graphs are powerful but rarely necessary. Our step-based approach handles 95% of use cases with 10% of the complexity.</p>"},{"location":"philosophy/#why-only-two-memory-types","title":"Why Only Two Memory Types?","text":"<p>Other frameworks offer 5+ memory types: - Short-term memory - Long-term memory - Episodic memory - Semantic memory - Procedural memory</p> <p>We provide just two: 1. ConversationMemory - Recent interactions 2. KnowledgeMemory - Persistent facts</p> <p>Why? Because that's all you need in practice. Additional complexity doesn't improve outcomes.</p>"},{"location":"philosophy/#why-reasoning-patterns-matter","title":"Why Reasoning Patterns Matter","text":"<p>LLMs can reason, but they need structure. We provide patterns, not prompts:</p> <pre><code>from agenticraft import Agent, ChainOfThought\n\nagent = Agent(\n    name=\"Analyst\",\n    reasoning_pattern=ChainOfThought()  # Structured thinking\n)\n</code></pre> <p>This ensures consistent, high-quality reasoning across all agents.</p>"},{"location":"philosophy/#what-were-not-building","title":"What We're NOT Building","text":"<p>Being clear about what we won't build is as important as what we will:</p>"},{"location":"philosophy/#not-another-langchain","title":"\u274c NOT Another LangChain","text":"<p>LangChain is powerful but complex. We're building something different: - Simpler APIs - Smaller core - Clearer abstractions - Better developer experience</p>"},{"location":"philosophy/#not-a-kitchen-sink","title":"\u274c NOT a Kitchen Sink","text":"<p>We resist the temptation to add every possible feature: - No 20 different memory types - No complex graph visualizations - No unnecessary abstractions - No features \"just in case\"</p>"},{"location":"philosophy/#not-a-research-project","title":"\u274c NOT a Research Project","text":"<p>This is production software: - Stability over novelty - Reliability over impressiveness - Documentation over demos - Real use cases over paper citations</p>"},{"location":"philosophy/#community-values","title":"Community Values","text":""},{"location":"philosophy/#open-source-open-community","title":"Open Source, Open Community","text":"<ul> <li>Contributions welcome - But simplicity is non-negotiable</li> <li>Feedback valued - Users shape the roadmap</li> <li>Transparency default - Development happens in the open</li> </ul>"},{"location":"philosophy/#quality-over-quantity","title":"Quality Over Quantity","text":"<ul> <li>Better 10 excellent tools than 100 mediocre ones</li> <li>Better clear docs than extensive ones</li> <li>Better stable API than feature-rich</li> </ul>"},{"location":"philosophy/#pragmatism-wins","title":"Pragmatism Wins","text":"<ul> <li>Real-world usage drives decisions</li> <li>Production experience matters</li> <li>Developer time is valuable</li> </ul>"},{"location":"philosophy/#the-agenticraft-way","title":"The AgentiCraft Way","text":"<p>When building with AgentiCraft, ask yourself:</p> <ol> <li>Is this the simplest solution?</li> <li>Can I understand what's happening?</li> <li>Will this work in production?</li> <li>Am I enjoying this?</li> </ol> <p>If you answer \"yes\" to all four, you're doing it the AgentiCraft way.</p>"},{"location":"philosophy/#future-vision","title":"Future Vision","text":"<p>As AgentiCraft grows, these principles remain constant:</p> <ul> <li>Core stays small - Complexity lives in plugins</li> <li>APIs stay simple - Power through composition</li> <li>Reasoning stays transparent - No black boxes</li> <li>Production stays first - Real-world focus</li> </ul>"},{"location":"philosophy/#join-us","title":"Join Us","text":"<p>If these principles resonate with you:</p> <ul> <li>\u2b50 Star the project</li> <li>\ud83d\udcac Join the discussion</li> <li>\ud83d\udee0\ufe0f Contribute code</li> <li>\ud83d\udcdd Share your story</li> </ul> <p>Together, we're making AI agent development accessible to every developer.</p> <p>\"Make it simple. Make it transparent. Make it work.\" - The AgentiCraft Motto</p>"},{"location":"quickstart/","title":"5-Minute Quickstart","text":"<p>Get your first AI agent running in less than 5 minutes. No complex setup, no configuration files, just Python.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install agenticraft\n</code></pre> <p>That's it. No additional dependencies to manually install.</p>"},{"location":"quickstart/#your-first-agent","title":"Your First Agent","text":""},{"location":"quickstart/#step-1-set-your-api-key","title":"Step 1: Set Your API Key","text":"<pre><code>export OPENAI_API_KEY=\"your-key-here\"\n</code></pre> <p>Or create a <code>.env</code> file: <pre><code>OPENAI_API_KEY=your-key-here\n</code></pre></p>"},{"location":"quickstart/#step-2-create-your-agent","title":"Step 2: Create Your Agent","text":"<p>Create a file called <code>hello_agent.py</code>:</p> <pre><code>from agenticraft import Agent\n\n# Create a simple agent\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful AI assistant.\"\n)\n\n# Run the agent\nresponse = agent.run(\"Tell me a fun fact about Python\")\nprint(response.content)\n</code></pre>"},{"location":"quickstart/#step-3-run-it","title":"Step 3: Run It","text":"<pre><code>python hello_agent.py\n</code></pre> <p>Congratulations! \ud83c\udf89 You've just created your first AI agent.</p>"},{"location":"quickstart/#adding-tools","title":"Adding Tools","text":"<p>Let's make your agent more capable by adding tools:</p> <pre><code>from agenticraft import Agent, tool\n\n# Define a simple tool\n@tool\ndef calculate(expression: str) -&gt; float:\n    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n    return eval(expression, {\"__builtins__\": {}}, {})\n\n@tool\ndef get_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    from datetime import datetime\n    return datetime.now().strftime(\"%I:%M %p\")\n\n# Create an agent with tools\nagent = Agent(\n    name=\"SmartAssistant\",\n    instructions=\"You are a helpful assistant with calculation and time abilities.\",\n    tools=[calculate, get_time]\n)\n\n# The agent will automatically use tools when needed\nresponse = agent.run(\"What's 15% of 847? Also, what time is it?\")\nprint(response.content)\n</code></pre>"},{"location":"quickstart/#understanding-agent-reasoning","title":"Understanding Agent Reasoning","text":"<p>One of AgentiCraft's core features is transparent reasoning:</p> <pre><code>response = agent.run(\"Help me plan a birthday party for 20 people\")\n\n# See what the agent is thinking\nprint(\"=== Agent's Reasoning ===\")\nprint(response.reasoning)\n\nprint(\"\\n=== Final Response ===\")\nprint(response.content)\n</code></pre>"},{"location":"quickstart/#creating-a-simple-workflow","title":"Creating a Simple Workflow","text":"<p>Chain multiple agents together:</p> <pre><code>from agenticraft import Agent, Workflow, Step\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"Researcher\",\n    instructions=\"You research topics thoroughly and provide detailed information.\"\n)\n\nwriter = Agent(\n    name=\"Writer\", \n    instructions=\"You write engaging content based on research.\"\n)\n\n# Create a workflow\nworkflow = Workflow(name=\"content_creation\")\n\n# Add steps - no complex graphs needed!\nworkflow.add_steps([\n    Step(\"research\", agent=researcher, inputs=[\"topic\"]),\n    Step(\"write\", agent=writer, depends_on=[\"research\"])\n])\n\n# Run the workflow\nresult = await workflow.run(topic=\"The future of AI agents\")\nprint(result[\"write\"])\n</code></pre>"},{"location":"quickstart/#memory-for-conversational-agents","title":"Memory for Conversational Agents","text":"<p>Make your agents remember context:</p> <pre><code>from agenticraft import Agent, ConversationMemory\n\nagent = Agent(\n    name=\"ChatBot\",\n    instructions=\"You are a friendly conversational AI.\",\n    memory=[ConversationMemory(max_turns=10)]\n)\n\n# First interaction\nresponse1 = agent.run(\"My name is Alice\")\nprint(response1.content)\n\n# The agent remembers!\nresponse2 = agent.run(\"What's my name?\")\nprint(response2.content)  # Will correctly recall \"Alice\"\n</code></pre>"},{"location":"quickstart/#using-different-llm-providers","title":"Using Different LLM Providers","text":"<p>AgentiCraft supports multiple providers:</p> <pre><code># OpenAI (default)\nagent = Agent(name=\"GPT4\", model=\"gpt-4\")\n\n# Anthropic Claude\nagent = Agent(name=\"Claude\", model=\"claude-3-opus\", api_key=\"anthropic-key\")\n\n# Google Gemini\nagent = Agent(name=\"Gemini\", model=\"gemini-pro\", api_key=\"google-key\")\n\n# Local Ollama\nagent = Agent(name=\"Local\", model=\"ollama/llama2\", base_url=\"http://localhost:11434\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>You've learned the basics! Here's what to explore next:</p>"},{"location":"quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Core Concepts - Understand how agents work</li> <li>Building Tools - Create powerful agent capabilities  </li> <li>Designing Workflows - Build complex systems</li> </ul>"},{"location":"quickstart/#see-examples","title":"See Examples","text":"<ul> <li>Hello World - Simple agent examples</li> <li>Provider Switching - Dynamic provider usage</li> <li>Advanced Agents - ReasoningAgent and WorkflowAgent</li> </ul>"},{"location":"quickstart/#production-ready","title":"Production Ready","text":"<ul> <li>Performance Tuning - Optimize your agents</li> <li>Best Practices - Use providers effectively</li> </ul>"},{"location":"quickstart/#quick-tips","title":"Quick Tips","text":"<p>Environment Variables</p> <p>Create a <code>.env</code> file in your project root to manage API keys: <pre><code>OPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> <p>Async Support</p> <p>All agent operations support async/await: <pre><code>response = await agent.arun(\"Your prompt\")\n</code></pre></p> <p>Error Handling</p> <p>AgentiCraft provides clear error messages: <pre><code>try:\n    response = agent.run(\"Do something\")\nexcept AgentError as e:\n    print(f\"Agent error: {e}\")\n</code></pre></p>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcac Join our Discord</li> <li>\ud83d\udc1b Report issues on GitHub</li> <li>\ud83d\udcda Read the full documentation</li> </ul> <p>Ready for more? Check out our comprehensive examples or dive into the API reference.</p>"},{"location":"concepts/agents/","title":"Agents","text":"<p>Agents are the core building blocks of AgentiCraft. An agent is an AI-powered entity that can reason, use tools, and maintain memory.</p>"},{"location":"concepts/agents/#what-is-an-agent","title":"What is an Agent?","text":"<p>An agent in AgentiCraft consists of:</p> <ul> <li>Identity: Name and instructions that define its purpose</li> <li>Reasoning: Transparent thought processes</li> <li>Tools: Capabilities it can use</li> <li>Memory: Context it maintains</li> <li>Provider: The LLM that powers it</li> </ul>"},{"location":"concepts/agents/#creating-agents","title":"Creating Agents","text":""},{"location":"concepts/agents/#basic-agent","title":"Basic Agent","text":"<pre><code>from agenticraft import Agent\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful AI assistant.\"\n)\n</code></pre>"},{"location":"concepts/agents/#agent-with-tools","title":"Agent with Tools","text":"<pre><code>from agenticraft import Agent, tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    # Implementation here\n    return f\"Results for: {query}\"\n\nagent = Agent(\n    name=\"Researcher\",\n    instructions=\"You help with research tasks.\",\n    tools=[search]\n)\n</code></pre>"},{"location":"concepts/agents/#agent-with-memory","title":"Agent with Memory","text":"<pre><code>from agenticraft import Agent, ConversationMemory\n\nagent = Agent(\n    name=\"ChatBot\",\n    instructions=\"You are a conversational assistant.\",\n    memory=[ConversationMemory(max_turns=10)]\n)\n</code></pre>"},{"location":"concepts/agents/#agent-configuration","title":"Agent Configuration","text":"<pre><code>agent = Agent(\n    name=\"Advanced\",\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=2000,\n    timeout=30,\n    max_retries=3\n)\n</code></pre>"},{"location":"concepts/agents/#using-agents","title":"Using Agents","text":""},{"location":"concepts/agents/#synchronous-usage","title":"Synchronous Usage","text":"<pre><code>response = agent.run(\"Your prompt here\")\nprint(response.content)\nprint(response.reasoning)\n</code></pre>"},{"location":"concepts/agents/#asynchronous-usage","title":"Asynchronous Usage","text":"<pre><code>response = await agent.arun(\"Your prompt here\")\n</code></pre>"},{"location":"concepts/agents/#understanding-agent-responses","title":"Understanding Agent Responses","text":"<p>Every agent response includes:</p> <ul> <li><code>content</code>: The final response</li> <li><code>reasoning</code>: The thought process</li> <li><code>tool_calls</code>: Any tools used</li> <li><code>usage</code>: Token usage information</li> </ul>"},{"location":"concepts/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Instructions: Be specific about the agent's role</li> <li>Appropriate Tools: Only include necessary tools</li> <li>Memory Management: Use memory judiciously</li> <li>Error Handling: Always handle potential errors</li> </ol>"},{"location":"concepts/agents/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Tools</li> <li>Explore Workflows</li> <li>Understand Memory</li> </ul>"},{"location":"concepts/memory/","title":"Memory","text":"<p>AgentiCraft provides flexible memory systems that allow agents to maintain context and learn from interactions.</p>"},{"location":"concepts/memory/#memory-types","title":"Memory Types","text":""},{"location":"concepts/memory/#conversation-memory","title":"Conversation Memory","text":"<p>Short-term memory for maintaining context within a conversation:</p> <pre><code>from agenticraft import Agent\n\n# Enable conversation memory\nagent = Agent(\n    name=\"MemoryBot\",\n    model=\"gpt-4\",\n    memory_enabled=True\n)\n\n# The agent remembers context\nagent.run(\"My name is Alice\")\nresponse = agent.run(\"What's my name?\")  # Remembers \"Alice\"\n</code></pre>"},{"location":"concepts/memory/#knowledge-memory","title":"Knowledge Memory","text":"<p>Long-term storage for facts and information:</p> <pre><code>from agenticraft import Agent, KnowledgeMemory\n\n# Create agent with knowledge memory\nknowledge = KnowledgeMemory()\nagent = Agent(\n    name=\"KnowledgeBot\",\n    model=\"gpt-4\",\n    knowledge_memory=knowledge\n)\n\n# Store facts\nagent.remember(\"The speed of light is 299,792,458 m/s\")\nagent.remember(\"Water boils at 100\u00b0C at sea level\")\n\n# Retrieve later\nresponse = agent.run(\"What's the speed of light?\")\n</code></pre>"},{"location":"concepts/memory/#memory-features","title":"Memory Features","text":""},{"location":"concepts/memory/#context-window-management","title":"Context Window Management","text":"<p>Automatically manages conversation history to fit within model limits:</p> <pre><code>agent = Agent(\n    name=\"SmartBot\",\n    memory_enabled=True,\n    memory_config={\n        \"max_messages\": 20,\n        \"summarize_after\": 15,\n        \"compression_model\": \"gpt-3.5-turbo\"\n    }\n)\n</code></pre>"},{"location":"concepts/memory/#semantic-search","title":"Semantic Search","text":"<p>Find relevant memories based on meaning:</p> <pre><code># Store various facts\nagent.remember(\"Python was created by Guido van Rossum\")\nagent.remember(\"JavaScript was created by Brendan Eich\")\n\n# Semantic search\nfacts = agent.recall(\"programming language creators\")\n# Returns relevant memories\n</code></pre>"},{"location":"concepts/memory/#memory-persistence","title":"Memory Persistence","text":"<p>Save and load memory across sessions:</p> <pre><code># Save memory to disk\nagent.save_memory(\"bot_memory.json\")\n\n# Load in a new session\nnew_agent = Agent(name=\"Bot\", memory_enabled=True)\nnew_agent.load_memory(\"bot_memory.json\")\n</code></pre>"},{"location":"concepts/memory/#advanced-memory-patterns","title":"Advanced Memory Patterns","text":""},{"location":"concepts/memory/#episodic-memory","title":"Episodic Memory","text":"<p>Remember specific interactions:</p> <pre><code>agent = Agent(\n    name=\"EpisodicBot\",\n    memory_config={\n        \"type\": \"episodic\",\n        \"remember_interactions\": True,\n        \"interaction_limit\": 100\n    }\n)\n</code></pre>"},{"location":"concepts/memory/#working-memory","title":"Working Memory","text":"<p>Temporary storage for complex tasks:</p> <pre><code># Agent uses working memory during problem-solving\nagent.run(\"Let's solve this step by step...\")\n# Automatically maintains intermediate results\n</code></pre>"},{"location":"concepts/memory/#memory-best-practices","title":"Memory Best Practices","text":"<ol> <li>Choose the Right Type: Use conversation memory for chat, knowledge memory for facts</li> <li>Set Appropriate Limits: Balance memory size with performance</li> <li>Regular Cleanup: Remove outdated or irrelevant memories</li> <li>Privacy Considerations: Be mindful of what information is stored</li> <li>Backup Important Data: Persist critical memories to disk</li> </ol>"},{"location":"concepts/memory/#memory-with-provider-switching","title":"Memory with Provider Switching","text":"<p>Memory persists across provider switches:</p> <pre><code>agent = Agent(name=\"Bot\", memory_enabled=True)\n\n# Chat with GPT-4\nagent.run(\"Remember that my favorite color is blue\")\n\n# Switch providers\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\n\n# Memory persists\nresponse = agent.run(\"What's my favorite color?\")  # Still remembers \"blue\"\n</code></pre>"},{"location":"concepts/memory/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about agents</li> <li>Explore reasoning systems</li> <li>Build memory-enabled agents</li> </ul>"},{"location":"concepts/reasoning/","title":"Reasoning","text":"<p>Reasoning systems in AgentiCraft provide transparency and explainability in how agents arrive at their conclusions.</p>"},{"location":"concepts/reasoning/#understanding-agent-reasoning","title":"Understanding Agent Reasoning","text":"<p>Traditional LLMs operate as black boxes. AgentiCraft's reasoning systems make the thought process visible and auditable.</p>"},{"location":"concepts/reasoning/#reasoningagent","title":"ReasoningAgent","text":"<p>The <code>ReasoningAgent</code> provides step-by-step reasoning traces:</p> <pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(\n    name=\"LogicalBot\",\n    model=\"gpt-4\"\n)\n\nresponse = agent.run(\"Should I invest in solar panels for my home?\")\n\nprint(\"Reasoning steps:\")\nfor i, step in enumerate(response.reasoning):\n    print(f\"{i+1}. {step}\")\n\nprint(f\"\\nConclusion: {response.content}\")\nprint(f\"Confidence: {response.confidence}\")\n</code></pre> <p>Output: <pre><code>Reasoning steps:\n1. Consider the initial investment cost of solar panels\n2. Evaluate average sunlight hours in the user's location\n3. Calculate potential energy savings over time\n4. Factor in available tax incentives and rebates\n5. Assess environmental impact and benefits\n6. Compare ROI with alternative investments\n\nConclusion: Based on these factors...\nConfidence: 0.85\n</code></pre></p>"},{"location":"concepts/reasoning/#reasoning-features","title":"Reasoning Features","text":""},{"location":"concepts/reasoning/#chain-of-thought","title":"Chain of Thought","text":"<p>Break down complex problems into logical steps:</p> <pre><code>agent = ReasoningAgent(\n    name=\"ProblemSolver\",\n    reasoning_style=\"chain_of_thought\"\n)\n</code></pre>"},{"location":"concepts/reasoning/#tree-of-thought","title":"Tree of Thought","text":"<p>Explore multiple reasoning paths:</p> <pre><code>agent = ReasoningAgent(\n    name=\"Explorer\",\n    reasoning_style=\"tree_of_thought\",\n    explore_branches=3\n)\n</code></pre>"},{"location":"concepts/reasoning/#self-reflection","title":"Self-Reflection","text":"<p>Agent critiques its own reasoning:</p> <pre><code>agent = ReasoningAgent(\n    name=\"ReflectiveBot\",\n    enable_self_critique=True\n)\n\nresponse = agent.run(\"Analyze this business proposal\")\n# Includes self-critique in reasoning steps\n</code></pre>"},{"location":"concepts/reasoning/#reasoning-transparency","title":"Reasoning Transparency","text":""},{"location":"concepts/reasoning/#assumption-tracking","title":"Assumption Tracking","text":"<p>Identify and list assumptions made:</p> <pre><code>response = agent.run(\"Predict next quarter's revenue\")\n\nprint(\"Assumptions made:\")\nfor assumption in response.assumptions:\n    print(f\"- {assumption}\")\n</code></pre>"},{"location":"concepts/reasoning/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<p>Express confidence levels:</p> <pre><code>response = agent.run(\"Diagnose this technical issue\")\n\nif response.confidence &lt; 0.7:\n    print(\"Low confidence - seeking additional information\")\n    # Gather more data\n</code></pre>"},{"location":"concepts/reasoning/#evidence-citation","title":"Evidence Citation","text":"<p>Link conclusions to evidence:</p> <pre><code>agent = ReasoningAgent(\n    name=\"ResearchBot\",\n    cite_sources=True\n)\n\nresponse = agent.run(\"What causes climate change?\")\n# Each reasoning step includes evidence\n</code></pre>"},{"location":"concepts/reasoning/#reasoning-patterns","title":"Reasoning Patterns","text":""},{"location":"concepts/reasoning/#deductive-reasoning","title":"Deductive Reasoning","text":"<p>From general to specific:</p> <pre><code>agent.run(\"If all birds can fly, and a penguin is a bird, can penguins fly?\")\n# Shows logical deduction process\n</code></pre>"},{"location":"concepts/reasoning/#inductive-reasoning","title":"Inductive Reasoning","text":"<p>From specific to general:</p> <pre><code>agent.run(\"Based on these customer reviews, what can we conclude?\")\n# Identifies patterns and generalizations\n</code></pre>"},{"location":"concepts/reasoning/#abductive-reasoning","title":"Abductive Reasoning","text":"<p>Best explanation for observations:</p> <pre><code>agent.run(\"The server is down and users report slow responses. What's the likely cause?\")\n# Generates plausible explanations\n</code></pre>"},{"location":"concepts/reasoning/#debugging-with-reasoning","title":"Debugging with Reasoning","text":"<p>Use reasoning traces to debug agent behavior:</p> <pre><code># Enable verbose reasoning\nagent = ReasoningAgent(\n    name=\"DebugBot\",\n    verbose_reasoning=True,\n    include_alternatives=True\n)\n\nresponse = agent.run(\"Complex task...\")\n\n# Analyze decision points\nfor decision in response.decision_points:\n    print(f\"Decision: {decision.question}\")\n    print(f\"Chosen: {decision.chosen}\")\n    print(f\"Alternatives: {decision.alternatives}\")\n</code></pre>"},{"location":"concepts/reasoning/#best-practices","title":"Best Practices","text":"<ol> <li>Use for Critical Decisions: Enable reasoning for high-stakes choices</li> <li>Balance Detail: More reasoning steps increase transparency but cost</li> <li>Validate Reasoning: Check logical consistency</li> <li>Document Assumptions: Make implicit assumptions explicit</li> <li>Monitor Confidence: Set thresholds for automated decisions</li> </ol>"},{"location":"concepts/reasoning/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"concepts/reasoning/#reasoning-provider-switching","title":"Reasoning + Provider Switching","text":"<pre><code># Use expensive model for complex reasoning\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\ncomplex_response = agent.run(\"Analyze this legal document\")\n\n# Switch to cheaper model for summary\nagent.set_provider(\"ollama\", model=\"llama2\")\nsummary = agent.run(\"Summarize the analysis\")\n</code></pre>"},{"location":"concepts/reasoning/#reasoning-workflows","title":"Reasoning + Workflows","text":"<pre><code>reasoning_workflow = [\n    Step(\"analyze\", \"Analyze the problem\"),\n    Step(\"reason\", \"Generate reasoning trace\"),\n    Step(\"critique\", \"Self-critique reasoning\"),\n    Step(\"conclude\", \"Form conclusion\")\n]\n</code></pre>"},{"location":"concepts/reasoning/#next-steps","title":"Next Steps","text":"<ul> <li>Explore ReasoningAgent</li> <li>Learn about memory systems</li> <li>See reasoning examples</li> </ul>"},{"location":"concepts/tools/","title":"Tools","text":"<p>Tools extend your agents' capabilities by allowing them to interact with external systems, APIs, and perform specialized tasks.</p>"},{"location":"concepts/tools/#creating-tools","title":"Creating Tools","text":"<p>The simplest way to create a tool is with the <code>@tool</code> decorator:</p> <pre><code>from agenticraft import tool\n\n@tool\ndef weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # Implementation here\n    return f\"Sunny in {location}, 72\u00b0F\"\n\n# Use the tool\nagent = Agent(name=\"WeatherBot\", tools=[weather])\nresponse = agent.run(\"What's the weather in San Francisco?\")\n</code></pre>"},{"location":"concepts/tools/#tool-parameters","title":"Tool Parameters","text":"<p>Tools support type hints and documentation:</p> <pre><code>@tool\ndef calculate(\n    expression: str,\n    precision: int = 2\n) -&gt; float:\n    \"\"\"\n    Evaluate a mathematical expression.\n\n    Args:\n        expression: The math expression to evaluate\n        precision: Decimal places for the result\n\n    Returns:\n        The calculated result\n    \"\"\"\n    result = eval(expression)\n    return round(result, precision)\n</code></pre>"},{"location":"concepts/tools/#tool-best-practices","title":"Tool Best Practices","text":"<ol> <li>Clear Descriptions: The docstring is used by the LLM to understand when to use the tool</li> <li>Type Hints: Always use type hints for better LLM understanding</li> <li>Error Handling: Tools should handle errors gracefully</li> <li>Single Purpose: Each tool should do one thing well</li> </ol>"},{"location":"concepts/tools/#advanced-tools","title":"Advanced Tools","text":"<p>For more complex tools, use the <code>Tool</code> class:</p> <pre><code>from agenticraft import Tool\n\nclass DatabaseTool(Tool):\n    def __init__(self, connection_string: str):\n        super().__init__(\n            name=\"database_query\",\n            description=\"Query the database\",\n            parameters={\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"SQL query to execute\"\n                }\n            }\n        )\n        self.conn = connect(connection_string)\n\n    def execute(self, query: str) -&gt; str:\n        # Execute query and return results\n        pass\n</code></pre>"},{"location":"concepts/tools/#built-in-tools","title":"Built-in Tools","text":"<p>AgentiCraft provides common tools out of the box: - Web search - File operations - API calls - Data processing</p>"},{"location":"concepts/tools/#tool-composition","title":"Tool Composition","text":"<p>Combine multiple tools for complex workflows:</p> <pre><code>@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    # Implementation\n\n@tool\ndef summarize(text: str) -&gt; str:\n    \"\"\"Summarize text.\"\"\"\n    # Implementation\n\n# Agent can use both tools together\nagent = Agent(\n    name=\"ResearchBot\",\n    tools=[search, summarize]\n)\n\nresponse = agent.run(\"Research and summarize recent AI developments\")\n# Agent will search, then summarize the results\n</code></pre>"},{"location":"concepts/tools/#next-steps","title":"Next Steps","text":"<ul> <li>Create your first tool</li> <li>Learn about workflows</li> <li>Explore MCP tools</li> </ul>"},{"location":"concepts/workflows/","title":"Workflows","text":"<p>Workflows enable agents to execute complex, multi-step processes with clear structure and error handling.</p>"},{"location":"concepts/workflows/#understanding-workflows","title":"Understanding Workflows","text":"<p>A workflow breaks down complex tasks into manageable steps that can be executed sequentially or in parallel.</p> <pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(name=\"DataProcessor\", model=\"gpt-4\")\n\nworkflow = [\n    Step(\"extract\", \"Extract data from the source\"),\n    Step(\"transform\", \"Clean and transform the data\"),\n    Step(\"analyze\", \"Perform analysis\"),\n    Step(\"report\", \"Generate a report\")\n]\n\nresult = agent.run_workflow(\"Process our Q4 sales data\", workflow)\n</code></pre>"},{"location":"concepts/workflows/#workflow-benefits","title":"Workflow Benefits","text":"<ol> <li>Clarity: Break complex tasks into clear steps</li> <li>Debugging: See exactly where issues occur</li> <li>Reusability: Save and reuse workflow patterns</li> <li>Progress Tracking: Monitor execution progress</li> <li>Error Recovery: Handle failures gracefully</li> </ol>"},{"location":"concepts/workflows/#step-dependencies","title":"Step Dependencies","text":"<p>Define relationships between steps:</p> <pre><code>workflow = [\n    Step(\"fetch_data\", \"Fetch data from API\"),\n    Step(\"validate\", \"Validate data\", depends_on=[\"fetch_data\"]),\n    Step(\"process\", \"Process data\", depends_on=[\"validate\"]),\n    Step(\"save\", \"Save results\", depends_on=[\"process\"])\n]\n</code></pre>"},{"location":"concepts/workflows/#parallel-execution","title":"Parallel Execution","text":"<p>Run independent steps simultaneously:</p> <pre><code>workflow = [\n    Step(\"fetch_users\", \"Get user data\"),\n    Step(\"fetch_orders\", \"Get order data\"),\n    Step(\"fetch_products\", \"Get product data\"),\n    Step(\"combine\", \"Combine all data\", \n         depends_on=[\"fetch_users\", \"fetch_orders\", \"fetch_products\"])\n]\n</code></pre>"},{"location":"concepts/workflows/#conditional-steps","title":"Conditional Steps","text":"<p>Execute steps based on conditions:</p> <pre><code>workflow = [\n    Step(\"check_data\", \"Check if data exists\"),\n    Step(\"fetch_data\", \"Fetch from API\", \n         condition=\"if no data found\"),\n    Step(\"process\", \"Process the data\")\n]\n</code></pre>"},{"location":"concepts/workflows/#error-handling","title":"Error Handling","text":"<p>Built-in error recovery:</p> <pre><code>workflow = [\n    Step(\"risky_operation\", \"Perform operation\",\n         retry_count=3,\n         fallback=\"safe_operation\"),\n    Step(\"safe_operation\", \"Fallback operation\", \n         skip_by_default=True)\n]\n</code></pre>"},{"location":"concepts/workflows/#workflowagent-features","title":"WorkflowAgent Features","text":"<p>The <code>WorkflowAgent</code> provides: - Automatic step orchestration - Progress callbacks - Result aggregation - Error propagation - Step timing and metrics</p>"},{"location":"concepts/workflows/#example-data-pipeline","title":"Example: Data Pipeline","text":"<pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(name=\"ETL\", model=\"gpt-4\")\n\netl_workflow = [\n    # Extract\n    Step(\"extract_csv\", \"Read data from CSV files\"),\n    Step(\"extract_api\", \"Fetch data from APIs\"),\n\n    # Transform\n    Step(\"merge\", \"Merge data sources\",\n         depends_on=[\"extract_csv\", \"extract_api\"]),\n    Step(\"clean\", \"Clean and validate data\",\n         depends_on=[\"merge\"]),\n    Step(\"enrich\", \"Enrich with additional data\",\n         depends_on=[\"clean\"]),\n\n    # Load\n    Step(\"load\", \"Load into database\",\n         depends_on=[\"enrich\"]),\n    Step(\"verify\", \"Verify data integrity\",\n         depends_on=[\"load\"])\n]\n\nresult = agent.run_workflow(\n    \"Run ETL for customer data\",\n    workflow=etl_workflow\n)\n\n# Access results\nfor step_name, step_result in result.steps.items():\n    print(f\"{step_name}: {step_result.status}\")\n</code></pre>"},{"location":"concepts/workflows/#best-practices","title":"Best Practices","text":"<ol> <li>Keep Steps Focused: Each step should have a single, clear purpose</li> <li>Use Descriptive Names: Step names should indicate their function</li> <li>Handle Failures: Plan for error scenarios</li> <li>Log Progress: Enable debugging and monitoring</li> <li>Test Steps Individually: Ensure each step works in isolation</li> </ol>"},{"location":"concepts/workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about WorkflowAgent</li> <li>See workflow examples</li> <li>Build complex workflows</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Learn by example with practical AgentiCraft demonstrations.</p>"},{"location":"examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"examples/#hello-world","title":"Hello World","text":"<p>The simplest possible agent - perfect for getting started.</p>"},{"location":"examples/#basic-chat","title":"Basic Chat","text":"<p>Build a conversational AI in minutes.</p>"},{"location":"examples/#feature-showcases","title":"Feature Showcases","text":""},{"location":"examples/#provider-switching","title":"Provider Switching","text":"<ul> <li>Runtime provider changes</li> <li>Cost optimization strategies</li> <li>Automatic failover</li> </ul>"},{"location":"examples/#advanced-agents","title":"Advanced Agents","text":"<ul> <li>ReasoningAgent with transparent thinking</li> <li>WorkflowAgent for complex processes</li> <li>Combining agent types</li> </ul>"},{"location":"examples/#real-world-applications","title":"Real-World Applications","text":""},{"location":"examples/#customer-support-bot","title":"Customer Support Bot","text":"<p>Multi-provider support agent with knowledge base integration.</p>"},{"location":"examples/#data-analysis-pipeline","title":"Data Analysis Pipeline","text":"<p>Workflow agent that processes data through multiple stages.</p>"},{"location":"examples/#content-generator","title":"Content Generator","text":"<p>ReasoningAgent that creates high-quality content with citations.</p>"},{"location":"examples/#code-snippets","title":"Code Snippets","text":""},{"location":"examples/#dynamic-model-selection","title":"Dynamic Model Selection","text":"<pre><code># Use expensive model for complex tasks\nif task.complexity &gt; 0.7:\n    agent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nelse:\n    agent.set_provider(\"ollama\", model=\"llama2\")\n</code></pre>"},{"location":"examples/#error-recovery","title":"Error Recovery","text":"<pre><code>try:\n    response = agent.run(prompt)\nexcept ProviderError:\n    # Automatic failover\n    agent.set_provider(\"ollama\", model=\"llama2\")\n    response = agent.run(prompt)\n</code></pre>"},{"location":"examples/#tool-integration","title":"Tool Integration","text":"<pre><code>@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    # Implementation\n\nagent = Agent(\"SearchBot\", tools=[search])\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/agenticraft/agenticraft\ncd agenticraft/examples\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install agenticraft\n</code></pre></p> </li> <li> <p>Set up API keys:    <pre><code>export OPENAI_API_KEY=\"your-key\"\nexport ANTHROPIC_API_KEY=\"your-key\"\n</code></pre></p> </li> <li> <p>Run examples:    <pre><code>python hello_world.py\npython provider_switching/basic.py\n</code></pre></p> </li> </ol>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a cool use case? We'd love to see it! Share your examples on GitHub.</p>"},{"location":"examples/advanced-agents/","title":"Advanced Agent Examples","text":"<p>Explore the power of ReasoningAgent and WorkflowAgent with practical examples.</p>"},{"location":"examples/advanced-agents/#reasoningagent-examples","title":"ReasoningAgent Examples","text":""},{"location":"examples/advanced-agents/#problem-solving-with-transparency","title":"Problem Solving with Transparency","text":"<pre><code>from agenticraft import ReasoningAgent\n\n# Create a reasoning agent\nagent = ReasoningAgent(\n    name=\"ProblemSolver\",\n    model=\"gpt-4\",\n    reasoning_style=\"chain_of_thought\"\n)\n\n# Solve a complex problem\nproblem = \"\"\"\nA company's revenue is declining by 15% quarterly. \nEmployee satisfaction is at 45%. \nCustomer churn increased by 30%. \nWhat should the CEO prioritize?\n\"\"\"\n\nresponse = agent.run(problem)\n\n# Display reasoning process\nprint(\"=== REASONING PROCESS ===\")\nfor i, step in enumerate(response.reasoning, 1):\n    print(f\"\\nStep {i}: {step}\")\n\nprint(f\"\\n=== RECOMMENDATION ===\")\nprint(response.content)\n\nprint(f\"\\n=== CONFIDENCE ===\")\nprint(f\"Confidence level: {response.confidence:.2%}\")\n</code></pre>"},{"location":"examples/advanced-agents/#multi-perspective-analysis","title":"Multi-Perspective Analysis","text":"<pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(\n    name=\"Analyst\",\n    model=\"gpt-4\",\n    reasoning_style=\"tree_of_thought\",\n    explore_branches=3\n)\n\n# Analyze from multiple angles\nquery = \"Should we launch our product in Europe or Asia first?\"\n\nresponse = agent.run(query)\n\n# Show different perspectives explored\nprint(\"=== PERSPECTIVES CONSIDERED ===\")\nfor branch in response.reasoning_branches:\n    print(f\"\\n{branch.perspective}:\")\n    print(f\"  Pros: {branch.pros}\")\n    print(f\"  Cons: {branch.cons}\")\n    print(f\"  Score: {branch.score}\")\n</code></pre>"},{"location":"examples/advanced-agents/#decision-making-with-criteria","title":"Decision Making with Criteria","text":"<pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(\n    name=\"DecisionMaker\",\n    model=\"gpt-4\"\n)\n\ndecision = agent.run(\"\"\"\n    Evaluate these job offers:\n    1. Startup: $120k, equity, high risk\n    2. Big Tech: $150k, stable, less growth\n    3. Remote: $130k, flexibility, isolation\n\n    Criteria: Career growth, work-life balance, financial security\n\"\"\")\n\n# Structured decision output\nprint(\"Decision Matrix:\")\nprint(decision.structured_output)\n</code></pre>"},{"location":"examples/advanced-agents/#workflowagent-examples","title":"WorkflowAgent Examples","text":""},{"location":"examples/advanced-agents/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(\n    name=\"DataProcessor\",\n    model=\"gpt-4\"\n)\n\n# Define a data processing workflow\ndata_workflow = [\n    Step(\"validate\", \"Validate input data format and completeness\"),\n    Step(\"clean\", \"Remove duplicates and fix inconsistencies\"),\n    Step(\"transform\", \"Convert data to analysis format\"),\n    Step(\"analyze\", \"Perform statistical analysis\"),\n    Step(\"visualize\", \"Create charts and graphs\"),\n    Step(\"report\", \"Generate executive summary\")\n]\n\n# Run the workflow\nresult = agent.run_workflow(\n    \"Process Q4 sales data from all regions\",\n    workflow=data_workflow\n)\n\n# Monitor progress\nfor step_name, step_result in result.steps.items():\n    print(f\"\\n{step_name.upper()}\")\n    print(f\"  Status: {step_result.status}\")\n    print(f\"  Duration: {step_result.duration:.2f}s\")\n    print(f\"  Output: {step_result.output[:100]}...\")\n</code></pre>"},{"location":"examples/advanced-agents/#content-creation-workflow","title":"Content Creation Workflow","text":"<pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(\n    name=\"ContentCreator\",\n    model=\"gpt-4\"\n)\n\nblog_workflow = [\n    Step(\"research\", \"Research the topic and gather sources\"),\n    Step(\"outline\", \"Create a detailed outline\"),\n    Step(\"draft\", \"Write the first draft\"),\n    Step(\"edit\", \"Edit for clarity and flow\"),\n    Step(\"optimize\", \"Optimize for SEO\"),\n    Step(\"format\", \"Format with headers and sections\")\n]\n\nresult = agent.run_workflow(\n    \"Create a blog post about AI safety best practices\",\n    workflow=blog_workflow\n)\n\n# Get the final content\nfinal_content = result.steps[\"format\"].output\nprint(final_content)\n</code></pre>"},{"location":"examples/advanced-agents/#parallel-processing-example","title":"Parallel Processing Example","text":"<pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(\n    name=\"ParallelProcessor\",\n    model=\"gpt-4\"\n)\n\n# Steps that can run in parallel\nanalysis_workflow = [\n    # These three run in parallel\n    Step(\"analyze_customers\", \"Analyze customer data\"),\n    Step(\"analyze_products\", \"Analyze product performance\"),\n    Step(\"analyze_market\", \"Analyze market trends\"),\n\n    # This depends on all three above\n    Step(\"synthesize\", \"Combine all analyses\",\n         depends_on=[\"analyze_customers\", \"analyze_products\", \"analyze_market\"]),\n\n    Step(\"recommend\", \"Generate recommendations\",\n         depends_on=[\"synthesize\"])\n]\n\nresult = agent.run_workflow(\n    \"Perform comprehensive business analysis\",\n    workflow=analysis_workflow,\n    parallel=True  # Enable parallel execution\n)\n</code></pre>"},{"location":"examples/advanced-agents/#combining-both-agent-types","title":"Combining Both Agent Types","text":""},{"location":"examples/advanced-agents/#research-assistant-with-reasoning","title":"Research Assistant with Reasoning","text":"<pre><code>from agenticraft import ReasoningAgent, WorkflowAgent, Step\n\n# Use ReasoningAgent for analysis\nreasoner = ReasoningAgent(name=\"Analyst\", model=\"gpt-4\")\n\n# Use WorkflowAgent for process\nworkflow_agent = WorkflowAgent(name=\"Researcher\", model=\"gpt-4\")\n\n# Research workflow that uses reasoning\nresearch_workflow = [\n    Step(\"gather\", \"Gather information on the topic\"),\n    Step(\"analyze\", \"Deep analysis with reasoning\"),\n    Step(\"synthesize\", \"Synthesize findings\"),\n    Step(\"conclude\", \"Draw conclusions\")\n]\n\n# Custom step handler for reasoning\nasync def analyze_with_reasoning(context):\n    data = context[\"gather_output\"]\n    reasoning_result = reasoner.run(f\"Analyze this data: {data}\")\n    return {\n        \"analysis\": reasoning_result.content,\n        \"reasoning\": reasoning_result.reasoning,\n        \"confidence\": reasoning_result.confidence\n    }\n\n# Attach custom handler\nworkflow_agent.set_step_handler(\"analyze\", analyze_with_reasoning)\n\n# Run the research\nresult = workflow_agent.run_workflow(\n    \"Research the impact of remote work on productivity\",\n    workflow=research_workflow\n)\n</code></pre>"},{"location":"examples/advanced-agents/#cost-optimized-complex-tasks","title":"Cost-Optimized Complex Tasks","text":"<pre><code>from agenticraft import ReasoningAgent, WorkflowAgent\n\n# Expensive reasoning agent\nreasoning_agent = ReasoningAgent(\n    name=\"DeepThinker\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\"\n)\n\n# Cheaper workflow agent\nworkflow_agent = WorkflowAgent(\n    name=\"Worker\",\n    provider=\"ollama\",\n    model=\"llama2\"\n)\n\n# Use reasoning for complex parts only\ndef smart_process(task):\n    # Simple steps with cheap model\n    workflow = [\n        Step(\"preprocess\", \"Prepare data\"),\n        Step(\"basic_analysis\", \"Basic analysis\")\n    ]\n\n    basic_result = workflow_agent.run_workflow(task, workflow)\n\n    # Complex reasoning with expensive model\n    if basic_result.requires_deep_analysis:\n        reasoning_result = reasoning_agent.run(\n            f\"Analyze: {basic_result.summary}\"\n        )\n        return reasoning_result\n\n    return basic_result\n</code></pre>"},{"location":"examples/advanced-agents/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Agent:</li> <li>ReasoningAgent for transparency and explainability</li> <li>WorkflowAgent for structured multi-step processes</li> <li> <p>Combine both for complex systems</p> </li> <li> <p>Optimize Resource Usage:</p> </li> <li>Use expensive models only for complex reasoning</li> <li>Switch to cheaper models for simple tasks</li> <li> <p>Cache intermediate results</p> </li> <li> <p>Design Clear Workflows:</p> </li> <li>Each step should have a single purpose</li> <li>Use dependencies to control flow</li> <li> <p>Enable parallel execution where possible</p> </li> <li> <p>Monitor and Debug:</p> </li> <li>Track step durations</li> <li>Log reasoning traces</li> <li>Set confidence thresholds</li> </ol>"},{"location":"examples/advanced-agents/#complete-example-ai-teaching-assistant","title":"Complete Example: AI Teaching Assistant","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAI Teaching Assistant using both ReasoningAgent and WorkflowAgent\n\"\"\"\n\nfrom agenticraft import ReasoningAgent, WorkflowAgent, Step\n\nclass TeachingAssistant:\n    def __init__(self):\n        # Reasoning agent for explanations\n        self.explainer = ReasoningAgent(\n            name=\"Explainer\",\n            model=\"gpt-4\",\n            reasoning_style=\"chain_of_thought\"\n        )\n\n        # Workflow agent for lesson planning\n        self.planner = WorkflowAgent(\n            name=\"LessonPlanner\",\n            model=\"gpt-3.5-turbo\"\n        )\n\n    def explain_concept(self, concept: str, student_level: str):\n        \"\"\"Explain a concept with reasoning.\"\"\"\n        prompt = f\"\"\"\n        Explain {concept} to a {student_level} student.\n        Show your reasoning for the explanation approach.\n        \"\"\"\n\n        response = self.explainer.run(prompt)\n\n        return {\n            \"explanation\": response.content,\n            \"reasoning\": response.reasoning,\n            \"confidence\": response.confidence,\n            \"assumptions\": response.assumptions\n        }\n\n    def create_lesson_plan(self, topic: str, duration: str):\n        \"\"\"Create a structured lesson plan.\"\"\"\n        lesson_workflow = [\n            Step(\"objectives\", \"Define learning objectives\"),\n            Step(\"prerequisites\", \"Identify prerequisites\"),\n            Step(\"content\", \"Structure main content\"),\n            Step(\"activities\", \"Design interactive activities\"),\n            Step(\"assessment\", \"Create assessment methods\"),\n            Step(\"resources\", \"List additional resources\")\n        ]\n\n        result = self.planner.run_workflow(\n            f\"Create a {duration} lesson plan for {topic}\",\n            workflow=lesson_workflow\n        )\n\n        return result\n\n    def adaptive_teaching(self, question: str, student_response: str):\n        \"\"\"Adapt teaching based on student understanding.\"\"\"\n        # Analyze student response with reasoning\n        analysis = self.explainer.run(\n            f\"Student asked: {question}\\n\"\n            f\"Student answered: {student_response}\\n\"\n            \"Analyze their understanding level.\"\n        )\n\n        # Create adaptive response workflow\n        if analysis.confidence &lt; 0.6:\n            # Student seems confused\n            workflow = [\n                Step(\"simplify\", \"Simplify the explanation\"),\n                Step(\"example\", \"Provide concrete example\"),\n                Step(\"check\", \"Check understanding\")\n            ]\n        else:\n            # Student understands basics\n            workflow = [\n                Step(\"deepen\", \"Deepen the explanation\"),\n                Step(\"connect\", \"Connect to related concepts\"),\n                Step(\"challenge\", \"Provide challenge question\")\n            ]\n\n        response = self.planner.run_workflow(\n            f\"Respond to student based on analysis\",\n            workflow=workflow\n        )\n\n        return response\n\n# Usage\nassistant = TeachingAssistant()\n\n# Explain a concept\nexplanation = assistant.explain_concept(\n    \"recursion\", \n    \"beginner programmer\"\n)\n\nprint(\"EXPLANATION:\")\nprint(explanation[\"explanation\"])\nprint(\"\\nTEACHING APPROACH:\")\nfor step in explanation[\"reasoning\"]:\n    print(f\"- {step}\")\n\n# Create lesson plan\nlesson = assistant.create_lesson_plan(\n    \"Introduction to Machine Learning\",\n    \"2 hours\"\n)\n\nprint(\"\\nLESSON PLAN:\")\nfor step_name, result in lesson.steps.items():\n    print(f\"\\n{step_name.upper()}:\")\n    print(result.output)\n</code></pre>"},{"location":"examples/advanced-agents/#next-steps","title":"Next Steps","text":"<ul> <li>Try the examples yourself</li> <li>Learn about provider switching</li> <li>Explore real-world applications</li> </ul>"},{"location":"examples/hello-world/","title":"Hello World","text":"<p>Welcome to AgentiCraft! Let's start with the simplest possible agent.</p>"},{"location":"examples/hello-world/#your-first-agent","title":"Your First Agent","text":"<pre><code>from agenticraft import Agent\n\n# Create an agent\nagent = Agent(name=\"HelloBot\", model=\"gpt-4\")\n\n# Run it!\nresponse = agent.run(\"Say hello to AgentiCraft!\")\nprint(response)\n</code></pre> <p>Output: <pre><code>Hello AgentiCraft! \ud83d\ude80 I'm excited to be your AI assistant powered by this amazing framework!\n</code></pre></p>"},{"location":"examples/hello-world/#basic-chat","title":"Basic Chat","text":"<p>Build a simple interactive chatbot:</p> <pre><code>from agenticraft import Agent\n\n# Create a conversational agent\nagent = Agent(\n    name=\"ChatBot\",\n    model=\"gpt-4\",\n    memory_enabled=True  # Remember conversation context\n)\n\nprint(\"ChatBot: Hello! I'm your AI assistant. Type 'quit' to exit.\")\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'quit':\n        break\n\n    response = agent.run(user_input)\n    print(f\"ChatBot: {response}\")\n</code></pre>"},{"location":"examples/hello-world/#adding-personality","title":"Adding Personality","text":"<pre><code>from agenticraft import Agent\n\n# Create an agent with personality\nagent = Agent(\n    name=\"FriendlyBot\",\n    model=\"gpt-4\",\n    system_prompt=\"You are a friendly, helpful assistant who loves using emojis and being encouraging!\"\n)\n\nresponse = agent.run(\"I'm learning Python\")\nprint(response)\n# Output: That's fantastic! \ud83c\udf89 Python is an amazing language to learn! \ud83d\udc0d ...\n</code></pre>"},{"location":"examples/hello-world/#using-different-providers","title":"Using Different Providers","text":"<pre><code>from agenticraft import Agent\n\n# Try different providers\nproviders = [\n    (\"openai\", \"gpt-4\"),\n    (\"anthropic\", \"claude-3-opus-20240229\"),\n    (\"ollama\", \"llama2\")\n]\n\nprompt = \"Write a haiku about coding\"\n\nfor provider, model in providers:\n    try:\n        agent = Agent(name=f\"{provider}-poet\", provider=provider, model=model)\n        response = agent.run(prompt)\n        print(f\"\\n{provider.upper()} ({model}):\")\n        print(response)\n    except Exception as e:\n        print(f\"Skipping {provider}: {e}\")\n</code></pre>"},{"location":"examples/hello-world/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first agent: - Add tools to your agent - Try provider switching - Explore advanced agents</p>"},{"location":"examples/hello-world/#complete-example","title":"Complete Example","text":"<p>Here's a complete example you can save and run:</p> <pre><code>#!/usr/bin/env python3\n\\\"\\\"\\\"\nhello_world.py - Your first AgentiCraft agent\n\\\"\\\"\\\"\n\nfrom agenticraft import Agent\n\ndef main():\n    # Create an agent\n    agent = Agent(\n        name=\"HelloBot\",\n        model=\"gpt-4\",\n        temperature=0.7\n    )\n\n    # Test various prompts\n    prompts = [\n        \"Introduce yourself\",\n        \"What's 2+2?\",\n        \"Tell me a joke\",\n        \"Explain AgentiCraft in one sentence\"\n    ]\n\n    for prompt in prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        response = agent.run(prompt)\n        print(f\"Response: {response}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Save this as <code>hello_world.py</code> and run: <pre><code>python hello_world.py\n</code></pre></p> <p>Happy coding with AgentiCraft! \ud83d\ude80</p>"},{"location":"examples/provider-switching/","title":"Provider Switching Examples","text":"<p>Dynamic provider switching is a powerful AgentiCraft feature that lets you optimize for cost, performance, and availability.</p>"},{"location":"examples/provider-switching/#basic-provider-switching","title":"Basic Provider Switching","text":"<pre><code>from agenticraft import Agent\n\n# Create an agent\nagent = Agent(name=\"FlexBot\", model=\"gpt-4\")\n\n# Use OpenAI for creative tasks\ncreative_response = agent.run(\"Write a creative story opening\")\nprint(f\"GPT-4: {creative_response}\")\n\n# Switch to Claude for analysis\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nanalysis_response = agent.run(\"Analyze the themes in the previous story\")\nprint(f\"Claude: {analysis_response}\")\n\n# Switch to local Ollama for simple tasks\nagent.set_provider(\"ollama\", model=\"llama2\")\nsummary_response = agent.run(\"Summarize in one sentence\")\nprint(f\"Llama2: {summary_response}\")\n</code></pre>"},{"location":"examples/provider-switching/#cost-optimization","title":"Cost Optimization","text":"<p>Use expensive models only when needed:</p> <pre><code>from agenticraft import Agent\n\nclass SmartAgent:\n    def __init__(self):\n        self.agent = Agent(name=\"CostOptimizer\", model=\"gpt-3.5-turbo\")\n\n    def run(self, prompt: str, complexity: str = \"simple\"):\n        if complexity == \"complex\":\n            # Use powerful model for complex tasks\n            self.agent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\n        elif complexity == \"simple\":\n            # Use efficient model for simple tasks\n            self.agent.set_provider(\"ollama\", model=\"llama2\")\n        else:\n            # Default to balanced option\n            self.agent.set_provider(\"openai\", model=\"gpt-3.5-turbo\")\n\n        return self.agent.run(prompt)\n\n# Usage\nsmart = SmartAgent()\nsmart.run(\"Count to 10\", complexity=\"simple\")  # Uses Llama2\nsmart.run(\"Explain quantum computing\", complexity=\"complex\")  # Uses Claude\n</code></pre>"},{"location":"examples/provider-switching/#automatic-failover","title":"Automatic Failover","text":"<p>Handle provider failures gracefully:</p> <pre><code>from agenticraft import Agent, ProviderError\nimport time\n\nclass ResilientAgent:\n    def __init__(self):\n        self.agent = Agent(name=\"Resilient\", model=\"gpt-4\")\n        self.providers = [\n            (\"openai\", \"gpt-4\"),\n            (\"anthropic\", \"claude-3-opus-20240229\"),\n            (\"ollama\", \"llama2\")\n        ]\n\n    def run_with_failover(self, prompt: str):\n        for provider, model in self.providers:\n            try:\n                self.agent.set_provider(provider, model)\n                return self.agent.run(prompt)\n            except ProviderError as e:\n                print(f\"Provider {provider} failed: {e}\")\n                continue\n        raise Exception(\"All providers failed\")\n\n# Usage\nresilient = ResilientAgent()\nresponse = resilient.run_with_failover(\"Hello world\")\n</code></pre>"},{"location":"examples/provider-switching/#performance-based-switching","title":"Performance-Based Switching","text":"<p>Switch providers based on response time:</p> <pre><code>from agenticraft import Agent\nimport time\n\nclass PerformanceAgent:\n    def __init__(self):\n        self.agent = Agent(name=\"SpeedyBot\", model=\"gpt-4\")\n        self.provider_stats = {}\n\n    def run_with_timing(self, prompt: str):\n        providers = [\n            (\"openai\", \"gpt-3.5-turbo\"),\n            (\"anthropic\", \"claude-3-haiku-20240307\"),\n            (\"ollama\", \"llama2\")\n        ]\n\n        fastest_time = float('inf')\n        fastest_provider = None\n        fastest_response = None\n\n        for provider, model in providers:\n            try:\n                self.agent.set_provider(provider, model)\n                start = time.time()\n                response = self.agent.run(prompt)\n                elapsed = time.time() - start\n\n                if elapsed &lt; fastest_time:\n                    fastest_time = elapsed\n                    fastest_provider = (provider, model)\n                    fastest_response = response\n\n                print(f\"{provider}: {elapsed:.2f}s\")\n            except:\n                continue\n\n        # Use fastest provider for subsequent calls\n        if fastest_provider:\n            self.agent.set_provider(*fastest_provider)\n\n        return fastest_response\n</code></pre>"},{"location":"examples/provider-switching/#task-specific-providers","title":"Task-Specific Providers","text":"<p>Different providers for different tasks:</p> <pre><code>from agenticraft import Agent\n\nclass TaskRouter:\n    def __init__(self):\n        self.agent = Agent(name=\"TaskBot\", model=\"gpt-4\")\n\n        # Define task-to-provider mapping\n        self.task_providers = {\n            \"code\": (\"openai\", \"gpt-4\"),\n            \"creative\": (\"anthropic\", \"claude-3-opus-20240229\"),\n            \"chat\": (\"ollama\", \"llama2\"),\n            \"analysis\": (\"anthropic\", \"claude-3-sonnet-20240229\"),\n            \"translation\": (\"openai\", \"gpt-3.5-turbo\")\n        }\n\n    def run_task(self, task_type: str, prompt: str):\n        if task_type in self.task_providers:\n            provider, model = self.task_providers[task_type]\n            self.agent.set_provider(provider, model)\n\n        return self.agent.run(prompt)\n\n# Usage\nrouter = TaskRouter()\nrouter.run_task(\"code\", \"Write a Python function to sort a list\")\nrouter.run_task(\"creative\", \"Write a poem about the ocean\")\nrouter.run_task(\"chat\", \"How's the weather?\")\n</code></pre>"},{"location":"examples/provider-switching/#provider-information","title":"Provider Information","text":"<p>Check current provider and available options:</p> <pre><code>from agenticraft import Agent\n\nagent = Agent(name=\"InfoBot\", model=\"gpt-4\")\n\n# Get current provider info\ninfo = agent.get_provider_info()\nprint(f\"Current provider: {info['provider']}\")\nprint(f\"Current model: {info['model']}\")\n\n# List all available providers\nproviders = agent.list_available_providers()\nprint(f\"Available providers: {providers}\")\n\n# Switch and verify\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nnew_info = agent.get_provider_info()\nprint(f\"Switched to: {new_info['provider']} - {new_info['model']}\")\n</code></pre>"},{"location":"examples/provider-switching/#complete-example-smart-assistant","title":"Complete Example: Smart Assistant","text":"<pre><code>#!/usr/bin/env python3\n\\\"\\\"\\\"\nSmart assistant that optimizes provider usage\n\\\"\\\"\\\"\n\nfrom agenticraft import Agent, ProviderError\nimport time\n\nclass SmartAssistant:\n    def __init__(self):\n        self.agent = Agent(name=\"SmartAssistant\", model=\"gpt-3.5-turbo\")\n        self.usage_cost = {\n            \"gpt-4\": 0.03,\n            \"gpt-3.5-turbo\": 0.001,\n            \"claude-3-opus-20240229\": 0.025,\n            \"claude-3-sonnet-20240229\": 0.003,\n            \"llama2\": 0.0  # Free local model\n        }\n\n    def estimate_complexity(self, prompt: str) -&gt; float:\n        \\\"\\\"\\\"Estimate prompt complexity (0-1).\\\"\\\"\\\"\n        factors = {\n            \"explain\": 0.3,\n            \"analyze\": 0.4,\n            \"create\": 0.3,\n            \"simple\": -0.2,\n            \"complex\": 0.3,\n            \"detailed\": 0.2\n        }\n\n        complexity = 0.5  # Base complexity\n        prompt_lower = prompt.lower()\n\n        for keyword, weight in factors.items():\n            if keyword in prompt_lower:\n                complexity += weight\n\n        return max(0, min(1, complexity))\n\n    def select_provider(self, prompt: str, max_cost: float = 0.01):\n        \\\"\\\"\\\"Select optimal provider based on task and budget.\\\"\\\"\\\"\n        complexity = self.estimate_complexity(prompt)\n\n        if complexity &gt; 0.7 and max_cost &gt;= 0.025:\n            return (\"anthropic\", \"claude-3-opus-20240229\")\n        elif complexity &gt; 0.5 and max_cost &gt;= 0.003:\n            return (\"openai\", \"gpt-4\")\n        elif complexity &gt; 0.3:\n            return (\"openai\", \"gpt-3.5-turbo\")\n        else:\n            return (\"ollama\", \"llama2\")\n\n    def run(self, prompt: str, max_cost: float = 0.01):\n        provider, model = self.select_provider(prompt, max_cost)\n\n        try:\n            self.agent.set_provider(provider, model)\n            response = self.agent.run(prompt)\n            cost = self.usage_cost.get(model, 0)\n\n            return {\n                \"response\": response,\n                \"provider\": provider,\n                \"model\": model,\n                \"estimated_cost\": cost\n            }\n        except ProviderError:\n            # Fallback to local model\n            self.agent.set_provider(\"ollama\", model=\"llama2\")\n            return {\n                \"response\": self.agent.run(prompt),\n                \"provider\": \"ollama\",\n                \"model\": \"llama2\",\n                \"estimated_cost\": 0\n            }\n\n# Usage\nassistant = SmartAssistant()\n\nprompts = [\n    \"What's 2+2?\",\n    \"Explain the theory of relativity\",\n    \"Write a complex business strategy\",\n    \"Translate 'hello' to Spanish\"\n]\n\nfor prompt in prompts:\n    result = assistant.run(prompt)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Provider: {result['provider']} ({result['model']})\")\n    print(f\"Cost: ${result['estimated_cost']}\")\n    print(f\"Response: {result['response'][:100]}...\")\n</code></pre>"},{"location":"examples/provider-switching/#best-practices","title":"Best Practices","text":"<ol> <li>Cache provider instances when switching frequently</li> <li>Handle provider-specific errors gracefully</li> <li>Monitor costs when using expensive models</li> <li>Test failover scenarios in development</li> <li>Log provider switches for debugging</li> </ol>"},{"location":"examples/provider-switching/#next-steps","title":"Next Steps","text":"<ul> <li>Explore advanced agents</li> <li>Learn about performance tuning</li> <li>Build real-world applications</li> </ul>"},{"location":"examples/real-world/","title":"Real-World Applications","text":"<p>See how AgentiCraft powers production applications across industries.</p>"},{"location":"examples/real-world/#customer-support","title":"Customer Support Bot","text":"<p>Build an intelligent support agent that handles customer inquiries with context awareness and provider optimization.</p> <pre><code>from agenticraft import Agent, ReasoningAgent, tool\nimport json\n\nclass CustomerSupportBot:\n    def __init__(self):\n        # Main conversational agent\n        self.chat_agent = Agent(\n            name=\"SupportChat\",\n            model=\"gpt-3.5-turbo\",  # Cost-effective for simple queries\n            memory_enabled=True,\n            system_prompt=\"\"\"You are a helpful customer support agent. \n            Be friendly, professional, and solution-oriented.\"\"\"\n        )\n\n        # Reasoning agent for complex issues\n        self.reasoning_agent = ReasoningAgent(\n            name=\"IssueAnalyzer\",\n            model=\"gpt-4\",  # More powerful for complex problems\n            reasoning_style=\"chain_of_thought\"\n        )\n\n        # Knowledge base tool\n        @tool\n        def search_knowledge_base(query: str) -&gt; str:\n            \"\"\"Search the company knowledge base.\"\"\"\n            # Simulate KB search\n            kb = {\n                \"refund\": \"Refunds are processed within 5-7 business days...\",\n                \"shipping\": \"Standard shipping takes 3-5 days...\",\n                \"warranty\": \"All products come with a 1-year warranty...\"\n            }\n            results = []\n            for key, value in kb.items():\n                if key in query.lower():\n                    results.append(value)\n            return \"\\n\".join(results) if results else \"No relevant articles found.\"\n\n        # Ticket creation tool\n        @tool\n        def create_ticket(issue: str, priority: str = \"normal\") -&gt; str:\n            \"\"\"Create a support ticket for complex issues.\"\"\"\n            ticket = {\n                \"id\": f\"TICK-{hash(issue) % 10000}\",\n                \"issue\": issue,\n                \"priority\": priority,\n                \"status\": \"open\"\n            }\n            return f\"Created ticket {ticket['id']} with {priority} priority\"\n\n        self.chat_agent.tools = [search_knowledge_base, create_ticket]\n\n    def handle_inquiry(self, customer_message: str) -&gt; dict:\n        \"\"\"Handle a customer inquiry with intelligent routing.\"\"\"\n\n        # First, try simple response\n        initial_response = self.chat_agent.run(customer_message)\n\n        # Check if we need deeper analysis\n        if any(word in customer_message.lower() \n               for word in [\"complex\", \"multiple\", \"urgent\", \"legal\", \"technical\"]):\n\n            # Switch to reasoning agent for analysis\n            analysis = self.reasoning_agent.run(\n                f\"Analyze this customer issue: {customer_message}\"\n            )\n\n            # If high complexity, create ticket\n            if analysis.confidence &lt; 0.7 or \"escalate\" in analysis.content:\n                ticket_result = self.chat_agent.run(\n                    f\"Create a ticket for: {customer_message}\"\n                )\n                return {\n                    \"response\": initial_response.content,\n                    \"ticket\": ticket_result.content,\n                    \"analysis\": analysis.reasoning\n                }\n\n        return {\n            \"response\": initial_response.content,\n            \"ticket\": None,\n            \"analysis\": None\n        }\n\n# Usage\nsupport_bot = CustomerSupportBot()\n\n# Handle various inquiries\ninquiries = [\n    \"How do I return a product?\",\n    \"I have multiple technical issues with my device and need urgent help\",\n    \"What's your refund policy?\"\n]\n\nfor inquiry in inquiries:\n    print(f\"\\nCustomer: {inquiry}\")\n    result = support_bot.handle_inquiry(inquiry)\n    print(f\"Bot: {result['response']}\")\n    if result['ticket']:\n        print(f\"Action: {result['ticket']}\")\n</code></pre>"},{"location":"examples/real-world/#data-analysis","title":"Data Analysis Pipeline","text":"<p>Process and analyze data using workflow agents with intelligent provider selection.</p> <pre><code>from agenticraft import WorkflowAgent, Step\nimport pandas as pd\n\nclass DataAnalysisPipeline:\n    def __init__(self):\n        # Use efficient model for data processing\n        self.processor = WorkflowAgent(\n            name=\"DataProcessor\",\n            provider=\"ollama\",\n            model=\"llama2\"\n        )\n\n        # Use powerful model for insights\n        self.analyzer = WorkflowAgent(\n            name=\"DataAnalyzer\",\n            provider=\"anthropic\",\n            model=\"claude-3-opus-20240229\"\n        )\n\n    def analyze_sales_data(self, data_path: str):\n        \"\"\"Complete sales data analysis pipeline.\"\"\"\n\n        # Data processing workflow\n        processing_workflow = [\n            Step(\"load\", \"Load data from CSV\"),\n            Step(\"clean\", \"Clean and validate data\"),\n            Step(\"transform\", \"Calculate metrics and aggregations\"),\n            Step(\"prepare\", \"Prepare data for analysis\")\n        ]\n\n        # Run processing with efficient model\n        processed = self.processor.run_workflow(\n            f\"Process sales data from {data_path}\",\n            workflow=processing_workflow\n        )\n\n        # Analysis workflow with powerful model\n        analysis_workflow = [\n            Step(\"trends\", \"Identify sales trends\"),\n            Step(\"anomalies\", \"Detect anomalies\"),\n            Step(\"segments\", \"Analyze customer segments\"),\n            Step(\"forecast\", \"Generate forecast\"),\n            Step(\"insights\", \"Extract actionable insights\"),\n            Step(\"report\", \"Create executive summary\")\n        ]\n\n        # Run deep analysis\n        results = self.analyzer.run_workflow(\n            f\"Analyze processed sales data: {processed.steps['prepare'].output}\",\n            workflow=analysis_workflow\n        )\n\n        return {\n            \"processing\": processed,\n            \"analysis\": results,\n            \"executive_summary\": results.steps[\"report\"].output\n        }\n\n# Usage\npipeline = DataAnalysisPipeline()\nresults = pipeline.analyze_sales_data(\"sales_2024_q4.csv\")\nprint(results[\"executive_summary\"])\n</code></pre>"},{"location":"examples/real-world/#content-generator","title":"Content Generator","text":"<p>Create high-quality content with source citations and fact-checking.</p> <pre><code>from agenticraft import ReasoningAgent, Agent, tool\nimport requests\n\nclass ContentGenerator:\n    def __init__(self):\n        # Research agent\n        self.researcher = Agent(\n            name=\"Researcher\",\n            model=\"gpt-4\",\n            system_prompt=\"You are a thorough researcher. Always cite sources.\"\n        )\n\n        # Writer with reasoning\n        self.writer = ReasoningAgent(\n            name=\"Writer\",\n            model=\"claude-3-opus-20240229\",\n            reasoning_style=\"chain_of_thought\"\n        )\n\n        # Fact checker\n        self.fact_checker = Agent(\n            name=\"FactChecker\",\n            model=\"gpt-4\",\n            system_prompt=\"You verify facts and check sources. Be skeptical.\"\n        )\n\n        # Web search tool\n        @tool\n        def web_search(query: str) -&gt; str:\n            \"\"\"Search the web for information.\"\"\"\n            # Simulate web search\n            return f\"Search results for: {query}\\n1. Result 1...\\n2. Result 2...\"\n\n        # Citation formatter\n        @tool\n        def format_citation(source: str, style: str = \"APA\") -&gt; str:\n            \"\"\"Format a citation in the specified style.\"\"\"\n            return f\"[{source}] - {style} formatted\"\n\n        self.researcher.tools = [web_search, format_citation]\n\n    def generate_article(self, topic: str, word_count: int = 1000):\n        \"\"\"Generate a well-researched article.\"\"\"\n\n        # Phase 1: Research\n        research_prompt = f\"\"\"\n        Research the topic: {topic}\n        Find credible sources and key information.\n        Focus on recent developments and expert opinions.\n        \"\"\"\n        research_results = self.researcher.run(research_prompt)\n\n        # Phase 2: Writing with reasoning\n        writing_prompt = f\"\"\"\n        Write a {word_count}-word article about: {topic}\n\n        Research findings:\n        {research_results.content}\n\n        Requirements:\n        - Engaging introduction\n        - Clear structure with sections\n        - Evidence-based arguments\n        - Compelling conclusion\n        - Include citations\n        \"\"\"\n\n        article = self.writer.run(writing_prompt)\n\n        # Phase 3: Fact checking\n        fact_check_prompt = f\"\"\"\n        Fact-check this article:\n        {article.content}\n\n        Verify:\n        - Accuracy of claims\n        - Source reliability\n        - Data correctness\n        - Logical consistency\n        \"\"\"\n\n        fact_check = self.fact_checker.run(fact_check_prompt)\n\n        # Phase 4: Final revision if needed\n        if \"inaccurate\" in fact_check.content.lower():\n            revision_prompt = f\"\"\"\n            Revise the article based on fact-checking feedback:\n            {fact_check.content}\n\n            Original article:\n            {article.content}\n            \"\"\"\n            article = self.writer.run(revision_prompt)\n\n        return {\n            \"article\": article.content,\n            \"reasoning\": article.reasoning,\n            \"research\": research_results.content,\n            \"fact_check\": fact_check.content,\n            \"confidence\": article.confidence\n        }\n\n# Usage\ngenerator = ContentGenerator()\nresult = generator.generate_article(\n    topic=\"The Future of Renewable Energy\",\n    word_count=1500\n)\n\nprint(\"ARTICLE:\")\nprint(result[\"article\"])\nprint(f\"\\nConfidence: {result['confidence']:.2%}\")\n</code></pre>"},{"location":"examples/real-world/#multi-language-customer-service","title":"Multi-Language Customer Service","text":"<p>Support customers in multiple languages with automatic translation and cultural adaptation.</p> <pre><code>from agenticraft import Agent, tool\n\nclass MultilingualSupport:\n    def __init__(self):\n        self.agents = {}\n\n        # Create specialized agents for different languages\n        languages = {\n            \"en\": (\"gpt-4\", \"You are a helpful English-speaking support agent.\"),\n            \"es\": (\"gpt-4\", \"Eres un agente de soporte \u00fatil que habla espa\u00f1ol.\"),\n            \"fr\": (\"gpt-4\", \"Vous \u00eates un agent de support utile qui parle fran\u00e7ais.\"),\n            \"de\": (\"gpt-4\", \"Sie sind ein hilfreicher deutschsprachiger Support-Agent.\"),\n            \"zh\": (\"gpt-4\", \"\u60a8\u662f\u4e00\u4f4d\u4e50\u4e8e\u52a9\u4eba\u7684\u4e2d\u6587\u5ba2\u670d\u4ee3\u8868\u3002\")\n        }\n\n        for lang, (model, prompt) in languages.items():\n            self.agents[lang] = Agent(\n                name=f\"Support_{lang}\",\n                model=model,\n                system_prompt=prompt,\n                memory_enabled=True\n            )\n\n        # Language detection tool\n        @tool\n        def detect_language(text: str) -&gt; str:\n            \"\"\"Detect the language of the text.\"\"\"\n            # Simple detection (in production, use a proper library)\n            if any(word in text.lower() for word in [\"hello\", \"help\", \"please\"]):\n                return \"en\"\n            elif any(word in text.lower() for word in [\"hola\", \"ayuda\", \"por favor\"]):\n                return \"es\"\n            elif any(word in text.lower() for word in [\"bonjour\", \"aide\", \"merci\"]):\n                return \"fr\"\n            elif any(word in text.lower() for word in [\"hallo\", \"hilfe\", \"bitte\"]):\n                return \"de\"\n            elif any(char in text for char in \"\u4f60\u597d\u5e2e\u52a9\u8bf7\"):\n                return \"zh\"\n            return \"en\"  # Default\n\n        # Cultural adaptation tool\n        @tool\n        def adapt_culturally(response: str, culture: str) -&gt; str:\n            \"\"\"Adapt response for cultural appropriateness.\"\"\"\n            adaptations = {\n                \"formal\": \"Please use formal language and titles.\",\n                \"casual\": \"Keep it friendly and casual.\",\n                \"direct\": \"Be direct and to the point.\",\n                \"indirect\": \"Be polite and indirect.\"\n            }\n            return f\"{response} [{adaptations.get(culture, 'Standard')}]\"\n\n        # Add tools to all agents\n        for agent in self.agents.values():\n            agent.tools = [detect_language, adapt_culturally]\n\n    def handle_query(self, message: str, user_id: str = None):\n        \"\"\"Handle a query in any supported language.\"\"\"\n\n        # Detect language\n        lang = self.agents[\"en\"].run(f\"Detect language: {message}\").content\n\n        # Select appropriate agent\n        agent = self.agents.get(lang, self.agents[\"en\"])\n\n        # Generate response\n        response = agent.run(message)\n\n        # Cultural adaptation based on language\n        cultural_styles = {\n            \"en\": \"casual\",\n            \"es\": \"casual\",\n            \"fr\": \"formal\",\n            \"de\": \"direct\",\n            \"zh\": \"formal\"\n        }\n\n        adapted_response = agent.run(\n            f\"Adapt culturally for {cultural_styles.get(lang, 'casual')}: {response.content}\"\n        )\n\n        return {\n            \"language\": lang,\n            \"response\": adapted_response.content,\n            \"original\": response.content\n        }\n\n# Usage\nsupport = MultilingualSupport()\n\nqueries = [\n    \"Hello, I need help with my order\",\n    \"Hola, necesito ayuda con mi pedido\",\n    \"Bonjour, j'ai besoin d'aide avec ma commande\",\n    \"\u4f60\u597d\uff0c\u6211\u9700\u8981\u8ba2\u5355\u5e2e\u52a9\"\n]\n\nfor query in queries:\n    result = support.handle_query(query)\n    print(f\"\\nQuery: {query}\")\n    print(f\"Language: {result['language']}\")\n    print(f\"Response: {result['response']}\")\n</code></pre>"},{"location":"examples/real-world/#best-practices-from-production","title":"Best Practices from Production","text":"<ol> <li>Provider Optimization</li> <li>Use GPT-3.5-Turbo for simple queries</li> <li>Switch to GPT-4 for complex reasoning</li> <li>Use Claude for long documents</li> <li> <p>Deploy Ollama for sensitive data</p> </li> <li> <p>Error Handling <pre><code>try:\n    response = agent.run(prompt)\nexcept ProviderError:\n    # Fallback to alternative provider\n    agent.set_provider(\"ollama\", model=\"llama2\")\n    response = agent.run(prompt)\n</code></pre></p> </li> <li> <p>Cost Management</p> </li> <li>Track token usage per request</li> <li>Set budget limits</li> <li>Use caching for repeated queries</li> <li> <p>Batch similar requests</p> </li> <li> <p>Performance</p> </li> <li>Enable parallel processing for workflows</li> <li>Cache tool results</li> <li>Use connection pooling</li> <li> <p>Implement request queuing</p> </li> <li> <p>Monitoring</p> </li> <li>Log all interactions</li> <li>Track response times</li> <li>Monitor error rates</li> <li>Set up alerts for anomalies</li> </ol>"},{"location":"examples/real-world/#next-steps","title":"Next Steps","text":"<ul> <li>Explore more examples</li> <li>Learn about performance tuning</li> <li>Read the API reference</li> </ul>"},{"location":"features/advanced_agents/","title":"Advanced Agents","text":"<p>AgentiCraft provides specialized agent types for complex use cases.</p>"},{"location":"features/advanced_agents/#reasoningagent","title":"ReasoningAgent","text":"<p>The ReasoningAgent makes its thought process transparent and explainable.</p> <pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(\n    name=\"ThoughtfulBot\",\n    model=\"gpt-4\"\n)\n\nresponse = agent.run(\"What are the pros and cons of solar energy?\")\nprint(\"Reasoning:\", response.reasoning)\nprint(\"Answer:\", response.content)\n</code></pre>"},{"location":"features/advanced_agents/#features","title":"Features","text":"<ul> <li>Step-by-step reasoning traces</li> <li>Explainable decision making</li> <li>Confidence scoring</li> <li>Assumption tracking</li> </ul>"},{"location":"features/advanced_agents/#use-cases","title":"Use Cases","text":"<ul> <li>Complex problem solving</li> <li>Educational applications</li> <li>Audit trails for decisions</li> <li>Debugging AI behavior</li> </ul>"},{"location":"features/advanced_agents/#workflowagent","title":"WorkflowAgent","text":"<p>The WorkflowAgent excels at multi-step processes and task orchestration.</p> <pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(\n    name=\"ProcessBot\",\n    model=\"gpt-4\"\n)\n\n# Define workflow\nworkflow = [\n    Step(\"analyze\", \"Analyze the user's request\"),\n    Step(\"plan\", \"Create an action plan\"),\n    Step(\"execute\", \"Execute the plan\"),\n    Step(\"verify\", \"Verify the results\")\n]\n\nresponse = agent.run_workflow(\n    \"Help me plan a dinner party for 8 people\",\n    workflow=workflow\n)\n\n# Access individual step results\nfor step_name, result in response.steps.items():\n    print(f\"{step_name}: {result}\")\n</code></pre>"},{"location":"features/advanced_agents/#features_1","title":"Features","text":"<ul> <li>Multi-step execution</li> <li>Step dependencies</li> <li>Parallel processing</li> <li>Progress tracking</li> <li>Error recovery</li> </ul>"},{"location":"features/advanced_agents/#use-cases_1","title":"Use Cases","text":"<ul> <li>Data processing pipelines</li> <li>Content generation workflows</li> <li>Multi-stage analysis</li> <li>Automated workflows</li> </ul>"},{"location":"features/advanced_agents/#combining-advanced-features","title":"Combining Advanced Features","text":"<pre><code>from agenticraft import ReasoningAgent\n\n# Create a reasoning agent that can switch providers\nagent = ReasoningAgent(\n    name=\"SmartBot\",\n    model=\"gpt-4\",\n    tools=[web_search, calculate]\n)\n\n# Use expensive model for complex reasoning\nresponse = agent.run(\"Analyze the environmental impact of electric vehicles\")\n\n# Switch to cheaper model for simple tasks\nagent.set_provider(\"ollama\", model=\"llama2\")\nresponse = agent.run(\"Summarize the previous analysis in 3 points\")\n</code></pre>"},{"location":"features/advanced_agents/#performance-tips","title":"Performance Tips","text":"<ol> <li>Choose the right agent type</li> <li>Use base Agent for simple tasks</li> <li>Use ReasoningAgent when transparency matters</li> <li> <p>Use WorkflowAgent for multi-step processes</p> </li> <li> <p>Optimize provider usage</p> </li> <li>Use powerful models for complex reasoning</li> <li>Switch to efficient models for simple tasks</li> <li> <p>Use local models for privacy-sensitive data</p> </li> <li> <p>Design efficient workflows</p> </li> <li>Break complex tasks into clear steps</li> <li>Parallelize independent steps</li> <li>Cache intermediate results</li> </ol>"},{"location":"features/advanced_agents/#next-steps","title":"Next Steps","text":"<ul> <li>See advanced examples</li> <li>Learn about workflows</li> <li>Optimize performance</li> </ul>"},{"location":"features/mcp_integration/","title":"MCP Integration","text":"<p>AgentiCraft seamlessly integrates with the Model Context Protocol (MCP) for enhanced tool capabilities.</p>"},{"location":"features/mcp_integration/#overview","title":"Overview","text":"<p>MCP enables agents to use tools from any MCP-compatible server, expanding capabilities without custom code.</p>"},{"location":"features/mcp_integration/#basic-usage","title":"Basic Usage","text":"<pre><code>from agenticraft import Agent\nfrom agenticraft.mcp import MCPClient\n\n# Connect to MCP server\nmcp = MCPClient(\"http://localhost:8080\")\n\n# Create agent with MCP tools\nagent = Agent(\n    name=\"MCPAgent\",\n    model=\"gpt-4\",\n    tools=mcp.get_tools()\n)\n\n# Use MCP tools naturally\nresponse = agent.run(\"Search for the latest AI news\")\n</code></pre>"},{"location":"features/mcp_integration/#available-mcp-servers","title":"Available MCP Servers","text":""},{"location":"features/mcp_integration/#file-system-tools","title":"File System Tools","text":"<pre><code>mcp = MCPClient(\"mcp://filesystem\")\nagent = Agent(name=\"FileBot\", tools=mcp.get_tools())\n\nagent.run(\"List all Python files in the current directory\")\nagent.run(\"Read the README.md file\")\n</code></pre>"},{"location":"features/mcp_integration/#database-tools","title":"Database Tools","text":"<pre><code>mcp = MCPClient(\"mcp://postgres\", \n    connection_string=\"postgresql://localhost/mydb\"\n)\nagent = Agent(name=\"DataBot\", tools=mcp.get_tools())\n\nagent.run(\"Show me all users created this month\")\n</code></pre>"},{"location":"features/mcp_integration/#web-tools","title":"Web Tools","text":"<pre><code>mcp = MCPClient(\"mcp://web\")\nagent = Agent(name=\"WebBot\", tools=mcp.get_tools())\n\nagent.run(\"Search for AgentiCraft tutorials\")\nagent.run(\"Get the content of https://example.com\")\n</code></pre>"},{"location":"features/mcp_integration/#custom-mcp-servers","title":"Custom MCP Servers","text":"<pre><code>from agenticraft.mcp import MCPServer, mcp_tool\n\nclass CustomMCPServer(MCPServer):\n    @mcp_tool\n    def get_weather(self, location: str) -&gt; str:\n        \"\"\"Get weather for a location.\"\"\"\n        # Implementation\n        return f\"Sunny in {location}\"\n\n# Start server\nserver = CustomMCPServer()\nserver.start(port=8080)\n\n# Use in agent\nmcp = MCPClient(\"http://localhost:8080\")\nagent = Agent(name=\"WeatherBot\", tools=mcp.get_tools())\n</code></pre>"},{"location":"features/mcp_integration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Tool Discovery <pre><code># List available tools\ntools = mcp.get_tools()\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\")\n</code></pre></p> </li> <li> <p>Error Handling <pre><code>try:\n    mcp = MCPClient(server_url)\nexcept MCPConnectionError:\n    # Fallback to local tools\n    agent = Agent(name=\"LocalBot\", tools=local_tools)\n</code></pre></p> </li> <li> <p>Performance</p> </li> <li>Use connection pooling for high-throughput</li> <li>Cache tool definitions</li> <li>Implement timeouts for reliability</li> </ol>"},{"location":"features/mcp_integration/#next-steps","title":"Next Steps","text":"<ul> <li>Tool examples</li> <li>Understanding tools</li> <li>Building MCP servers</li> </ul>"},{"location":"features/provider_switching/","title":"Provider Switching Guide","text":""},{"location":"features/provider_switching/#overview","title":"Overview","text":"<p>AgentiCraft v0.1.1 introduces dynamic provider switching, allowing agents to seamlessly switch between different LLM providers (OpenAI, Anthropic, Ollama) at runtime. This feature enables:</p> <ul> <li>Cost optimization by using appropriate models for different tasks</li> <li>Failover resilience with automatic fallback to alternative providers</li> <li>Model comparison for evaluating different LLMs on the same task</li> <li>Local/cloud flexibility by switching between cloud APIs and local models</li> </ul>"},{"location":"features/provider_switching/#quick-start","title":"Quick Start","text":"<pre><code>from agenticraft import Agent\n\n# Create an agent\nagent = Agent(name=\"FlexibleAgent\")\n\n# NEW in v0.1.1: Explicit provider specification\nagent = Agent(\n    name=\"ClaudeAgent\",\n    provider=\"anthropic\",  # Explicit provider\n    model=\"claude-3-opus-20240229\"\n)\n\n# Switch to different providers\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nagent.set_provider(\"ollama\", model=\"llama2\")\nagent.set_provider(\"openai\", model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"features/provider_switching/#available-providers","title":"Available Providers","text":""},{"location":"features/provider_switching/#openai","title":"OpenAI","text":"<ul> <li>Models: <code>gpt-4</code>, <code>gpt-4-turbo</code>, <code>gpt-3.5-turbo</code>, <code>o1-preview</code>, <code>o1-mini</code></li> <li>Features: Function calling, JSON mode, streaming</li> <li>Setup: Requires <code>OPENAI_API_KEY</code> environment variable</li> </ul>"},{"location":"features/provider_switching/#anthropic","title":"Anthropic","text":"<ul> <li>Models: <code>claude-3-opus-20240229</code>, <code>claude-3-sonnet-20240229</code>, <code>claude-3-haiku-20240307</code></li> <li>Features: Large context window, constitutional AI</li> <li>Setup: Requires <code>ANTHROPIC_API_KEY</code> environment variable</li> </ul>"},{"location":"features/provider_switching/#ollama-local","title":"Ollama (Local)","text":"<ul> <li>Models: <code>llama2</code>, <code>mistral</code>, <code>codellama</code>, <code>gemma</code>, <code>phi</code>, and more</li> <li>Features: Local inference, no API costs, privacy</li> <li>Setup: Requires Ollama running locally (<code>ollama serve</code>)</li> </ul>"},{"location":"features/provider_switching/#basic-usage","title":"Basic Usage","text":""},{"location":"features/provider_switching/#simple-provider-switching","title":"Simple Provider Switching","text":"<pre><code>from agenticraft import Agent\n\n# Method 1: Auto-detection from model name (backward compatible)\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt-4\",  # Auto-detects OpenAI\n    instructions=\"You are a helpful AI assistant.\"\n)\n\n# Method 2: Explicit provider specification (NEW in v0.1.1)\nagent = Agent(\n    name=\"Assistant\",\n    provider=\"openai\",  # Explicit provider\n    model=\"gpt-4\",\n    instructions=\"You are a helpful AI assistant.\"\n)\n\n# Benefits of explicit provider:\n# - No ambiguity about which provider is used\n# - Works with custom model names\n# - Better for configuration files\n# - Clearer intent in code\n\n# Get current provider info\ninfo = agent.get_provider_info()\nprint(f\"Current provider: {info['provider']}\")\nprint(f\"Model: {info['model']}\")\n\n# Switch to Anthropic\nagent.set_provider(\"anthropic\", model=\"claude-3-sonnet-20240229\")\n\n# Switch to local Ollama\nagent.set_provider(\"ollama\", model=\"llama2\", base_url=\"http://localhost:11434\")\n\n# List available providers\nproviders = agent.list_available_providers()\nprint(f\"Available providers: {providers}\")\n</code></pre>"},{"location":"features/provider_switching/#with-error-handling","title":"With Error Handling","text":"<pre><code>from agenticraft.core.exceptions import ProviderError\n\ntry:\n    agent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nexcept ProviderError as e:\n    print(f\"Failed to switch provider: {e}\")\n    # Fallback to another provider\n    agent.set_provider(\"openai\", model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"features/provider_switching/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"features/provider_switching/#cost-optimized-agent","title":"Cost-Optimized Agent","text":"<p>Use different models based on task complexity:</p> <pre><code>class SmartAgent:\n    def __init__(self):\n        self.agent = Agent(name=\"SmartAgent\")\n\n    def estimate_complexity(self, prompt: str) -&gt; str:\n        # Simple heuristic\n        if len(prompt.split()) &gt; 50 or \"analyze\" in prompt.lower():\n            return \"high\"\n        elif len(prompt.split()) &lt; 10:\n            return \"low\"\n        return \"medium\"\n\n    async def run(self, prompt: str) -&gt; str:\n        complexity = self.estimate_complexity(prompt)\n\n        if complexity == \"high\":\n            self.agent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\n        elif complexity == \"low\":\n            self.agent.set_provider(\"openai\", model=\"gpt-3.5-turbo\")\n        else:\n            self.agent.set_provider(\"openai\", model=\"gpt-4\")\n\n        response = await self.agent.arun(prompt)\n        return response.content\n</code></pre>"},{"location":"features/provider_switching/#resilient-agent-with-failover","title":"Resilient Agent with Failover","text":"<p>Automatically failover to backup providers:</p> <pre><code>class ResilientAgent:\n    def __init__(self):\n        self.agent = Agent(name=\"ResilientAgent\")\n        self.providers = [\n            (\"openai\", \"gpt-4\"),\n            (\"anthropic\", \"claude-3-sonnet-20240229\"),\n            (\"ollama\", \"llama2\"),\n        ]\n\n    async def run(self, prompt: str) -&gt; str:\n        for provider, model in self.providers:\n            try:\n                self.agent.set_provider(provider, model=model)\n                response = await self.agent.arun(prompt)\n                return response.content\n            except Exception as e:\n                print(f\"Provider {provider} failed: {e}\")\n                continue\n\n        raise Exception(\"All providers failed\")\n</code></pre>"},{"location":"features/provider_switching/#model-comparison","title":"Model Comparison","text":"<p>Compare responses from different models:</p> <pre><code>async def compare_models(prompt: str):\n    agent = Agent(name=\"Comparator\")\n    models = [\n        (\"openai\", \"gpt-4\"),\n        (\"anthropic\", \"claude-3-opus-20240229\"),\n        (\"ollama\", \"llama2\"),\n    ]\n\n    results = {}\n    for provider, model in models:\n        try:\n            agent.set_provider(provider, model=model)\n            response = await agent.arun(prompt)\n            results[f\"{provider}/{model}\"] = response.content\n        except Exception as e:\n            results[f\"{provider}/{model}\"] = f\"Error: {e}\"\n\n    return results\n</code></pre>"},{"location":"features/provider_switching/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"features/provider_switching/#openai_1","title":"OpenAI","text":"<pre><code># JSON response format\nresponse = await agent.arun(\n    \"List 3 colors\",\n    response_format={\"type\": \"json_object\"}\n)\n\n# Streaming (when implemented)\nasync for chunk in agent.astream(\"Tell me a story\"):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"features/provider_switching/#anthropic_1","title":"Anthropic","text":"<pre><code># Anthropic handles system messages differently\nagent.config.instructions = \"You are Claude, created by Anthropic.\"\nagent.set_provider(\"anthropic\")\n\n# Larger context window\nresponse = await agent.arun(very_long_prompt)  # Up to 200k tokens\n</code></pre>"},{"location":"features/provider_switching/#ollama","title":"Ollama","text":"<pre><code># Local model with custom parameters\nagent.set_provider(\"ollama\", model=\"llama2\")\n\nresponse = await agent.arun(\n    \"Generate text\",\n    temperature=0.9,\n    seed=42,  # Reproducible generation\n    num_predict=200  # Max tokens\n)\n</code></pre>"},{"location":"features/provider_switching/#configuration-options","title":"Configuration Options","text":""},{"location":"features/provider_switching/#using-provider-parameter","title":"Using Provider Parameter","text":"<p>The <code>provider</code> parameter in AgentConfig allows explicit provider specification:</p> <pre><code># Explicit provider specification\nagent = Agent(\n    name=\"MyAgent\",\n    provider=\"anthropic\",  # Explicit provider\n    model=\"claude-3-opus-20240229\"\n)\n\n# From configuration dictionary\nconfig = {\n    \"name\": \"ConfigAgent\",\n    \"provider\": \"ollama\",\n    \"model\": \"llama2\",\n    \"base_url\": \"http://localhost:11434\",\n    \"temperature\": 0.7\n}\nagent = Agent(**config)\n\n# Provider validation\ntry:\n    agent = Agent(provider=\"invalid_provider\")  # Raises ValueError\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"features/provider_switching/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>import os\n\n# Read from environment\nprovider = os.getenv(\"AGENT_PROVIDER\", \"openai\")\nmodel = os.getenv(\"AGENT_MODEL\", \"gpt-4\")\n\nagent = Agent(\n    name=\"EnvAgent\",\n    provider=provider,\n    model=model\n)\n</code></pre>"},{"location":"features/provider_switching/#whats-preserved-when-switching","title":"What's Preserved When Switching","text":"<p>When you switch providers, the following are preserved:</p> <ul> <li>\u2705 Agent configuration: name, instructions, temperature, max_tokens</li> <li>\u2705 Tools: All registered tools remain available</li> <li>\u2705 Memory: Conversation history and memory stores</li> <li>\u2705 Reasoning patterns: The agent's reasoning approach</li> <li>\u2705 Agent ID: The unique identifier remains the same</li> </ul> <p>What changes:</p> <ul> <li>\u274c Model: Updates to the new provider's model</li> <li>\u274c API credentials: Uses the new provider's credentials</li> <li>\u274c Provider client: A new provider instance is created</li> </ul>"},{"location":"features/provider_switching/#performance-considerations","title":"Performance Considerations","text":""},{"location":"features/provider_switching/#provider-latency","title":"Provider Latency","text":"<p>Typical response times (approximate):</p> <ul> <li>OpenAI GPT-3.5: 0.5-2 seconds</li> <li>OpenAI GPT-4: 2-10 seconds</li> <li>Anthropic Claude: 1-5 seconds</li> <li>Ollama (local): 0.1-5 seconds (depends on hardware)</li> </ul>"},{"location":"features/provider_switching/#switching-overhead","title":"Switching Overhead","text":"<p>Provider switching is lightweight: - Creating new provider instance: ~1ms - Validating credentials: ~10ms - Total switch time: &lt;50ms</p>"},{"location":"features/provider_switching/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Cache provider instances if switching frequently:    <pre><code># Future enhancement - provider pooling\nprovider_pool = {\n    \"openai\": OpenAIProvider(...),\n    \"anthropic\": AnthropicProvider(...)\n}\n</code></pre></p> </li> <li> <p>Use appropriate models for tasks:</p> </li> <li>Simple queries: <code>gpt-3.5-turbo</code>, <code>claude-3-haiku</code></li> <li>Complex reasoning: <code>gpt-4</code>, <code>claude-3-opus</code></li> <li> <p>Local/private: <code>ollama/llama2</code>, <code>ollama/mistral</code></p> </li> <li> <p>Handle provider differences:</p> </li> <li>Test tools with each provider</li> <li>Be aware of token limits</li> <li>Consider response format variations</li> </ol>"},{"location":"features/provider_switching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/provider_switching/#common-issues","title":"Common Issues","text":"<p>Provider not found: <pre><code>ProviderError: Unknown provider: xyz\n</code></pre> Solution: Check available providers with <code>agent.list_available_providers()</code></p> <p>Authentication failed: <pre><code>ProviderAuthError: Missing API key for anthropic\n</code></pre> Solution: Set environment variable <code>ANTHROPIC_API_KEY</code></p> <p>Ollama connection failed: <pre><code>ProviderError: Cannot connect to Ollama\n</code></pre> Solution: Ensure Ollama is running: <code>ollama serve</code></p> <p>Model not available: <pre><code>ProviderError: Model 'gpt-5' not found\n</code></pre> Solution: Check supported models for each provider</p>"},{"location":"features/provider_switching/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to troubleshoot:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Now provider switches will be logged\nagent.set_provider(\"anthropic\")\n# Logs: \"Agent 'Assistant' switched to anthropic (model: claude-3-opus-20240229)\"\n</code></pre>"},{"location":"features/provider_switching/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements for future versions:</p> <ul> <li>Provider pooling: Reuse provider instances</li> <li>Automatic model selection: Choose optimal model based on task</li> <li>Cost tracking: Monitor spending across providers</li> <li>Performance metrics: Compare provider response times</li> <li>Streaming support: Unified streaming interface</li> <li>Provider profiles: Save and load provider configurations</li> </ul>"},{"location":"features/provider_switching/#examples","title":"Examples","text":"<p>See the <code>examples/provider_switching/</code> directory for complete examples:</p> <ul> <li><code>basic_switching.py</code>: Simple provider switching examples</li> <li><code>cost_optimization.py</code>: Optimize costs with smart provider selection</li> <li><code>provider_failover.py</code>: Build resilient agents with automatic failover</li> </ul>"},{"location":"features/provider_switching/#api-reference","title":"API Reference","text":""},{"location":"features/provider_switching/#agentset_provider","title":"Agent.set_provider()","text":"<pre><code>def set_provider(\n    self, \n    provider_name: str,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Switch the agent's LLM provider dynamically.\n\n    Args:\n        provider_name: Name of the provider (\"openai\", \"anthropic\", \"ollama\")\n        model: Optional model override for the new provider\n        api_key: Optional API key for the new provider\n        base_url: Optional base URL (mainly for Ollama)\n        **kwargs: Additional provider-specific parameters\n\n    Raises:\n        ProviderError: If the provider name is invalid or setup fails\n    \"\"\"\n</code></pre>"},{"location":"features/provider_switching/#agentget_provider_info","title":"Agent.get_provider_info()","text":"<pre><code>def get_provider_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about the current provider.\n\n    Returns:\n        Dict containing provider name, model, and capabilities\n    \"\"\"\n</code></pre>"},{"location":"features/provider_switching/#agentlist_available_providers","title":"Agent.list_available_providers()","text":"<pre><code>def list_available_providers(self) -&gt; List[str]:\n    \"\"\"\n    List available LLM providers.\n\n    Returns:\n        List of provider names that can be used with set_provider\n    \"\"\"\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>AgentiCraft is designed to work out of the box with minimal configuration.</p>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from agenticraft import Agent, AgentConfig\n\n# Simple configuration\nagent = Agent(\n    name=\"MyAgent\",\n    model=\"gpt-4\",\n    provider=\"openai\"  # Optional - auto-detected from model\n)\n\n# Advanced configuration\nconfig = AgentConfig(\n    name=\"AdvancedAgent\",\n    model=\"claude-3-opus-20240229\",\n    provider=\"anthropic\",\n    temperature=0.7,\n    max_tokens=2000\n)\nagent = Agent(config=config)\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-key-here\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-key-here\"\n\n# Ollama (local)\nexport OLLAMA_HOST=\"http://localhost:11434\"\n</code></pre>"},{"location":"getting-started/configuration/#provider-configuration","title":"Provider Configuration","text":"<p>Each provider has specific configuration options:</p>"},{"location":"getting-started/configuration/#openai","title":"OpenAI","text":"<pre><code>agent = Agent(\n    name=\"GPTAgent\",\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=2000\n)\n</code></pre>"},{"location":"getting-started/configuration/#anthropic","title":"Anthropic","text":"<pre><code>agent = Agent(\n    name=\"ClaudeAgent\", \n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\",\n    max_tokens=4000\n)\n</code></pre>"},{"location":"getting-started/configuration/#ollama","title":"Ollama","text":"<pre><code>agent = Agent(\n    name=\"LocalAgent\",\n    provider=\"ollama\",\n    model=\"llama2\",\n    temperature=0.8\n)\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Create your first agent</li> <li>Learn about provider switching</li> </ul>"},{"location":"getting-started/first-agent/","title":"Your First Agent","text":"<p>Let's create your first AI agent with AgentiCraft in just a few lines of code.</p>"},{"location":"getting-started/first-agent/#basic-agent","title":"Basic Agent","text":"<pre><code>from agenticraft import Agent\n\n# Create an agent\nagent = Agent(name=\"Assistant\", model=\"gpt-4\")\n\n# Have a conversation\nresponse = agent.run(\"Hello! What can you help me with today?\")\nprint(response)\n</code></pre>"},{"location":"getting-started/first-agent/#agent-with-tools","title":"Agent with Tools","text":"<pre><code>from agenticraft import Agent, tool\n\n@tool\ndef calculate(expression: str) -&gt; float:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return eval(expression)\n\n# Create agent with tools\nagent = Agent(\n    name=\"MathBot\",\n    model=\"gpt-4\",\n    tools=[calculate]\n)\n\nresponse = agent.run(\"What's 42 * 17?\")\nprint(response)\n</code></pre>"},{"location":"getting-started/first-agent/#agent-with-memory","title":"Agent with Memory","text":"<pre><code>from agenticraft import Agent\n\n# Agent with conversation memory\nagent = Agent(\n    name=\"MemoryBot\",\n    model=\"gpt-4\",\n    memory_enabled=True\n)\n\n# First interaction\nagent.run(\"My name is Alice\")\n\n# Agent remembers context\nresponse = agent.run(\"What's my name?\")\nprint(response)  # Will remember \"Alice\"\n</code></pre>"},{"location":"getting-started/first-agent/#provider-switching","title":"Provider Switching","text":"<pre><code>from agenticraft import Agent\n\n# Start with GPT-4\nagent = Agent(name=\"FlexBot\", model=\"gpt-4\")\nresponse = agent.run(\"Write a haiku\")\n\n# Switch to Claude\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\nresponse = agent.run(\"Write another haiku\")\n\n# Switch to local Ollama\nagent.set_provider(\"ollama\", model=\"llama2\")\nresponse = agent.run(\"One more haiku\")\n</code></pre>"},{"location":"getting-started/first-agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore advanced agents</li> <li>Learn about tools</li> <li>Build workflows</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install agenticraft\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/agenticraft/agenticraft.git\ncd agenticraft\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import agenticraft\nprint(agenticraft.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure your environment</li> <li>Create your first agent</li> </ul>"},{"location":"guides/performance-tuning/","title":"Provider Switching Performance Optimization Guide","text":""},{"location":"guides/performance-tuning/#overview","title":"Overview","text":"<p>This guide covers performance considerations and optimization strategies for provider switching in AgentiCraft v0.1.1.</p>"},{"location":"guides/performance-tuning/#performance-metrics","title":"Performance Metrics","text":""},{"location":"guides/performance-tuning/#provider-switching-overhead","title":"Provider Switching Overhead","text":"Operation Time Impact Provider instance creation ~1ms Negligible Credential validation ~10ms Minimal First API call (cold) 100-500ms Noticeable Subsequent API calls Provider-dependent Varies"},{"location":"guides/performance-tuning/#provider-response-times","title":"Provider Response Times","text":"Provider Model First Token Full Response Tokens/sec OpenAI GPT-3.5 200-500ms 0.5-2s 50-100 OpenAI GPT-4 500-2000ms 2-10s 20-40 Anthropic Claude-3-Opus 300-1000ms 2-5s 30-60 Anthropic Claude-3-Sonnet 200-500ms 1-3s 40-80 Ollama Llama2 (M1 Mac) 50-200ms 0.5-5s 10-50 Ollama Llama2 (GPU) 10-50ms 0.1-1s 100-500"},{"location":"guides/performance-tuning/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"guides/performance-tuning/#1-provider-instance-caching","title":"1. Provider Instance Caching","text":"<p>Currently, AgentiCraft creates a new provider instance on each switch. For frequent switching, implement caching:</p> <pre><code>class CachedProviderAgent(Agent):\n    \"\"\"Agent with provider instance caching.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._provider_cache = {}\n        self._cache_size = 5  # Maximum cached providers\n\n    def set_provider(self, provider_name: str, **kwargs):\n        \"\"\"Set provider with caching.\"\"\"\n        cache_key = f\"{provider_name}:{kwargs.get('model', 'default')}\"\n\n        # Check cache\n        if cache_key in self._provider_cache:\n            self._provider = self._provider_cache[cache_key]\n            self.config.model = kwargs.get('model', self.config.model)\n            return\n\n        # Create new provider\n        super().set_provider(provider_name, **kwargs)\n\n        # Cache it\n        self._provider_cache[cache_key] = self._provider\n\n        # Evict oldest if cache is full\n        if len(self._provider_cache) &gt; self._cache_size:\n            oldest = next(iter(self._provider_cache))\n            del self._provider_cache[oldest]\n</code></pre>"},{"location":"guides/performance-tuning/#2-connection-pooling","title":"2. Connection Pooling","text":"<p>For high-throughput applications, use connection pooling:</p> <pre><code>import httpx\n\n# Global connection pools\n_connection_pools = {\n    \"openai\": httpx.AsyncClient(\n        limits=httpx.Limits(max_connections=100, max_keepalive=20),\n        timeout=httpx.Timeout(30.0)\n    ),\n    \"anthropic\": httpx.AsyncClient(\n        limits=httpx.Limits(max_connections=50, max_keepalive=10),\n        timeout=httpx.Timeout(60.0)\n    ),\n}\n\nclass PooledProvider(BaseProvider):\n    \"\"\"Provider using connection pooling.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.client = _connection_pools.get(self.name)\n</code></pre>"},{"location":"guides/performance-tuning/#3-parallel-provider-warm-up","title":"3. Parallel Provider Warm-up","text":"<p>Pre-warm providers during initialization:</p> <pre><code>class FastStartAgent(Agent):\n    \"\"\"Agent with pre-warmed providers.\"\"\"\n\n    async def initialize_providers(self, providers: List[Tuple[str, str]]):\n        \"\"\"Pre-warm multiple providers in parallel.\"\"\"\n        import asyncio\n\n        async def warm_provider(provider_name: str, model: str):\n            try:\n                self.set_provider(provider_name, model=model)\n                # Make a minimal request to establish connection\n                await self.arun(\"Hi\", max_tokens=1)\n            except Exception:\n                pass  # Ignore warm-up failures\n\n        # Warm all providers in parallel\n        await asyncio.gather(*[\n            warm_provider(provider, model)\n            for provider, model in providers\n        ])\n\n# Usage\nagent = FastStartAgent()\nawait agent.initialize_providers([\n    (\"openai\", \"gpt-3.5-turbo\"),\n    (\"anthropic\", \"claude-3-sonnet-20240229\"),\n    (\"ollama\", \"llama2\")\n])\n</code></pre>"},{"location":"guides/performance-tuning/#4-smart-model-selection","title":"4. Smart Model Selection","text":"<p>Choose models based on task requirements:</p> <pre><code>class PerformanceOptimizedAgent:\n    \"\"\"Agent that selects models for optimal performance.\"\"\"\n\n    # Model performance profiles\n    MODEL_PROFILES = {\n        # (provider, model): (latency_ms, tokens_per_sec, cost_per_1k)\n        (\"openai\", \"gpt-3.5-turbo\"): (500, 75, 0.002),\n        (\"openai\", \"gpt-4\"): (2000, 30, 0.03),\n        (\"anthropic\", \"claude-3-haiku-20240307\"): (300, 100, 0.00025),\n        (\"anthropic\", \"claude-3-sonnet-20240229\"): (1000, 60, 0.003),\n        (\"ollama\", \"llama2\"): (100, 30, 0),\n    }\n\n    def select_optimal_model(\n        self,\n        max_latency_ms: int = 5000,\n        min_quality_score: float = 0.7,\n        max_cost_per_1k: float = 0.01\n    ) -&gt; Tuple[str, str]:\n        \"\"\"Select optimal model based on constraints.\"\"\"\n        candidates = []\n\n        for (provider, model), (latency, tps, cost) in self.MODEL_PROFILES.items():\n            if (latency &lt;= max_latency_ms and cost &lt;= max_cost_per_1k):\n                # Simple quality score (you'd want a better metric)\n                quality = 0.5 if \"3.5\" in model or \"llama\" in model else 0.9\n                if quality &gt;= min_quality_score:\n                    candidates.append((provider, model, latency, cost))\n\n        # Sort by latency (or implement more complex scoring)\n        candidates.sort(key=lambda x: x[2])\n\n        if candidates:\n            return candidates[0][0], candidates[0][1]\n        return \"openai\", \"gpt-3.5-turbo\"  # Default fallback\n</code></pre>"},{"location":"guides/performance-tuning/#5-response-caching","title":"5. Response Caching","text":"<p>Cache responses for identical queries:</p> <pre><code>from functools import lru_cache\nimport hashlib\n\nclass CachedAgent(Agent):\n    \"\"\"Agent with response caching.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._response_cache = {}\n        self._cache_ttl = 3600  # 1 hour\n\n    def _get_cache_key(self, prompt: str, provider: str, model: str) -&gt; str:\n        \"\"\"Generate cache key for prompt.\"\"\"\n        content = f\"{provider}:{model}:{prompt}\"\n        return hashlib.md5(content.encode()).hexdigest()\n\n    async def arun(self, prompt: str, **kwargs) -&gt; AgentResponse:\n        \"\"\"Run with caching.\"\"\"\n        # Check if caching is appropriate\n        if kwargs.get(\"temperature\", 0.7) &gt; 0.1:\n            # Don't cache non-deterministic responses\n            return await super().arun(prompt, **kwargs)\n\n        # Check cache\n        provider_name = self.provider.__class__.__name__.lower().replace(\"provider\", \"\")\n        cache_key = self._get_cache_key(prompt, provider_name, self.config.model)\n\n        if cache_key in self._response_cache:\n            cached = self._response_cache[cache_key]\n            if time.time() - cached[\"timestamp\"] &lt; self._cache_ttl:\n                return cached[\"response\"]\n\n        # Get fresh response\n        response = await super().arun(prompt, **kwargs)\n\n        # Cache it\n        self._response_cache[cache_key] = {\n            \"response\": response,\n            \"timestamp\": time.time()\n        }\n\n        return response\n</code></pre>"},{"location":"guides/performance-tuning/#6-batch-processing","title":"6. Batch Processing","text":"<p>Process multiple requests efficiently:</p> <pre><code>class BatchProcessingAgent(Agent):\n    \"\"\"Agent optimized for batch processing.\"\"\"\n\n    async def arun_batch(\n        self,\n        prompts: List[str],\n        max_concurrent: int = 5\n    ) -&gt; List[AgentResponse]:\n        \"\"\"Process multiple prompts concurrently.\"\"\"\n        import asyncio\n\n        semaphore = asyncio.Semaphore(max_concurrent)\n\n        async def process_one(prompt: str) -&gt; AgentResponse:\n            async with semaphore:\n                return await self.arun(prompt)\n\n        # Process all prompts concurrently with rate limiting\n        responses = await asyncio.gather(*[\n            process_one(prompt) for prompt in prompts\n        ])\n\n        return responses\n\n# Usage\nagent = BatchProcessingAgent()\nprompts = [\"Question 1\", \"Question 2\", \"Question 3\", ...]\nresponses = await agent.arun_batch(prompts, max_concurrent=3)\n</code></pre>"},{"location":"guides/performance-tuning/#monitoring-performance","title":"Monitoring Performance","text":""},{"location":"guides/performance-tuning/#basic-metrics-collection","title":"Basic Metrics Collection","text":"<pre><code>import time\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass PerformanceMetrics:\n    provider: str\n    model: str\n    prompt_tokens: int\n    completion_tokens: int\n    latency_ms: float\n    first_token_ms: float\n    success: bool\n\nclass MonitoredAgent(Agent):\n    \"\"\"Agent with performance monitoring.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.metrics: List[PerformanceMetrics] = []\n\n    async def arun(self, prompt: str, **kwargs) -&gt; AgentResponse:\n        \"\"\"Run with performance monitoring.\"\"\"\n        start_time = time.perf_counter()\n        first_token_time = None\n\n        try:\n            # For streaming responses (future)\n            if kwargs.get(\"stream\"):\n                # Track time to first token\n                pass\n\n            response = await super().arun(prompt, **kwargs)\n\n            # Record metrics\n            latency = (time.perf_counter() - start_time) * 1000\n\n            metrics = PerformanceMetrics(\n                provider=self.provider.__class__.__name__,\n                model=self.config.model,\n                prompt_tokens=response.metadata.get(\"usage\", {}).get(\"prompt_tokens\", 0),\n                completion_tokens=response.metadata.get(\"usage\", {}).get(\"completion_tokens\", 0),\n                latency_ms=latency,\n                first_token_ms=first_token_time or latency,\n                success=True\n            )\n\n            self.metrics.append(metrics)\n            return response\n\n        except Exception as e:\n            # Record failure\n            latency = (time.perf_counter() - start_time) * 1000\n            metrics = PerformanceMetrics(\n                provider=self.provider.__class__.__name__,\n                model=self.config.model,\n                prompt_tokens=0,\n                completion_tokens=0,\n                latency_ms=latency,\n                first_token_ms=latency,\n                success=False\n            )\n            self.metrics.append(metrics)\n            raise\n\n    def get_performance_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get performance summary statistics.\"\"\"\n        if not self.metrics:\n            return {}\n\n        by_provider = {}\n        for metric in self.metrics:\n            key = f\"{metric.provider}/{metric.model}\"\n            if key not in by_provider:\n                by_provider[key] = {\n                    \"count\": 0,\n                    \"success_count\": 0,\n                    \"total_latency\": 0,\n                    \"total_tokens\": 0,\n                    \"latencies\": []\n                }\n\n            stats = by_provider[key]\n            stats[\"count\"] += 1\n            if metric.success:\n                stats[\"success_count\"] += 1\n            stats[\"total_latency\"] += metric.latency_ms\n            stats[\"total_tokens\"] += metric.prompt_tokens + metric.completion_tokens\n            stats[\"latencies\"].append(metric.latency_ms)\n\n        # Calculate statistics\n        summary = {}\n        for key, stats in by_provider.items():\n            latencies = sorted(stats[\"latencies\"])\n            summary[key] = {\n                \"requests\": stats[\"count\"],\n                \"success_rate\": stats[\"success_count\"] / stats[\"count\"],\n                \"avg_latency_ms\": stats[\"total_latency\"] / stats[\"count\"],\n                \"p50_latency_ms\": latencies[len(latencies) // 2] if latencies else 0,\n                \"p95_latency_ms\": latencies[int(len(latencies) * 0.95)] if latencies else 0,\n                \"total_tokens\": stats[\"total_tokens\"],\n                \"tokens_per_request\": stats[\"total_tokens\"] / stats[\"count\"]\n            }\n\n        return summary\n</code></pre>"},{"location":"guides/performance-tuning/#opentelemetry-integration","title":"OpenTelemetry Integration","text":"<pre><code>from opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\ntracer = trace.get_tracer(__name__)\n\nclass TelemetryAgent(Agent):\n    \"\"\"Agent with OpenTelemetry integration.\"\"\"\n\n    async def arun(self, prompt: str, **kwargs) -&gt; AgentResponse:\n        \"\"\"Run with distributed tracing.\"\"\"\n        provider_name = self.provider.__class__.__name__\n\n        with tracer.start_as_current_span(\n            \"agent.run\",\n            attributes={\n                \"agent.name\": self.name,\n                \"provider.name\": provider_name,\n                \"model.name\": self.config.model,\n                \"prompt.length\": len(prompt),\n            }\n        ) as span:\n            try:\n                response = await super().arun(prompt, **kwargs)\n\n                # Add response attributes\n                span.set_attributes({\n                    \"response.length\": len(response.content),\n                    \"tokens.prompt\": response.metadata.get(\"usage\", {}).get(\"prompt_tokens\", 0),\n                    \"tokens.completion\": response.metadata.get(\"usage\", {}).get(\"completion_tokens\", 0),\n                })\n\n                span.set_status(Status(StatusCode.OK))\n                return response\n\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                raise\n</code></pre>"},{"location":"guides/performance-tuning/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Cache Provider Instances for frequently used configurations</li> <li>Use Connection Pooling for high-throughput applications  </li> <li>Pre-warm Providers during initialization</li> <li>Select Models Intelligently based on task requirements</li> <li>Implement Response Caching for deterministic queries</li> <li>Process in Batches when handling multiple requests</li> <li>Monitor Performance to identify bottlenecks</li> <li>Use Distributed Tracing for production debugging</li> </ol>"},{"location":"guides/performance-tuning/#benchmarking-script","title":"Benchmarking Script","text":"<pre><code># benchmark_providers.py\nimport asyncio\nimport time\nfrom statistics import mean, stdev\n\nfrom agenticraft import Agent\n\nasync def benchmark_provider(provider: str, model: str, prompts: List[str]) -&gt; Dict:\n    \"\"\"Benchmark a specific provider/model combination.\"\"\"\n    agent = Agent()\n    agent.set_provider(provider, model=model)\n\n    latencies = []\n    errors = 0\n\n    for prompt in prompts:\n        try:\n            start = time.perf_counter()\n            await agent.arun(prompt)\n            latency = (time.perf_counter() - start) * 1000\n            latencies.append(latency)\n        except Exception:\n            errors += 1\n\n    return {\n        \"provider\": provider,\n        \"model\": model,\n        \"requests\": len(prompts),\n        \"errors\": errors,\n        \"avg_latency_ms\": mean(latencies) if latencies else 0,\n        \"std_dev_ms\": stdev(latencies) if len(latencies) &gt; 1 else 0,\n        \"min_latency_ms\": min(latencies) if latencies else 0,\n        \"max_latency_ms\": max(latencies) if latencies else 0,\n    }\n\n# Run benchmarks\nasync def main():\n    test_prompts = [\n        \"What is 2+2?\",\n        \"Explain quantum computing in one sentence\",\n        \"Write a haiku about programming\",\n    ]\n\n    configurations = [\n        (\"openai\", \"gpt-3.5-turbo\"),\n        (\"openai\", \"gpt-4\"),\n        (\"anthropic\", \"claude-3-sonnet-20240229\"),\n        (\"ollama\", \"llama2\"),\n    ]\n\n    results = []\n    for provider, model in configurations:\n        try:\n            result = await benchmark_provider(provider, model, test_prompts)\n            results.append(result)\n            print(f\"{provider}/{model}: {result['avg_latency_ms']:.0f}ms avg\")\n        except Exception as e:\n            print(f\"{provider}/{model}: Failed - {e}\")\n\n    return results\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This completes the performance optimization guide for provider switching in AgentiCraft v0.1.1.</p>"},{"location":"plugins/creating-plugins/","title":"Plugin Development Guide","text":"<p>This guide explains how to create custom plugins for AgentiCraft to extend its functionality with new tools, agents, providers, and capabilities.</p>"},{"location":"plugins/creating-plugins/#overview","title":"Overview","text":"<p>AgentiCraft's plugin system allows you to:</p> <ul> <li>Add custom tools and capabilities</li> <li>Create specialized agents</li> <li>Integrate new LLM providers</li> <li>Enhance existing agents with new features</li> <li>Hook into the agent lifecycle</li> <li>Add telemetry and monitoring</li> </ul>"},{"location":"plugins/creating-plugins/#quick-start","title":"Quick Start","text":"<p>Here's a minimal plugin example:</p> <pre><code>from agenticraft.plugins import BasePlugin, PluginInfo\n\nclass HelloPlugin(BasePlugin):\n    name = \"hello\"\n    version = \"1.0.0\"\n    description = \"A simple greeting plugin\"\n\n    def get_info(self) -&gt; PluginInfo:\n        return PluginInfo(\n            name=self.name,\n            version=self.version,\n            description=self.description\n        )\n</code></pre>"},{"location":"plugins/creating-plugins/#plugin-structure","title":"Plugin Structure","text":""},{"location":"plugins/creating-plugins/#basic-plugin-class","title":"Basic Plugin Class","text":"<p>Every plugin must inherit from <code>BasePlugin</code> and implement required methods:</p> <pre><code>from agenticraft.plugins import BasePlugin, PluginInfo, PluginConfig\nfrom typing import List, Dict, Any\n\nclass MyPlugin(BasePlugin):\n    # Required metadata\n    name = \"my_plugin\"\n    version = \"1.0.0\"\n    description = \"Does amazing things\"\n    author = \"Your Name\"\n\n    def __init__(self, config: PluginConfig = None):\n        super().__init__(config)\n        # Initialize your plugin\n\n    def get_info(self) -&gt; PluginInfo:\n        \"\"\"Return plugin metadata and capabilities.\"\"\"\n        return PluginInfo(\n            name=self.name,\n            version=self.version,\n            description=self.description,\n            author=self.author,\n            provides_tools=[\"tool1\", \"tool2\"],\n            provides_agents=[\"CustomAgent\"],\n            provides_providers=[\"custom_llm\"]\n        )\n\n    def initialize(self):\n        \"\"\"Set up resources, connections, etc.\"\"\"\n        super().initialize()\n        # Your initialization code\n\n    def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        # Your cleanup code\n        super().cleanup()\n</code></pre>"},{"location":"plugins/creating-plugins/#providing-tools","title":"Providing Tools","text":"<p>Plugins can provide tools that agents can use:</p> <pre><code>from agenticraft.core.tool import Tool\n\nclass CalculatorTool(Tool):\n    name = \"calculator\"\n    description = \"Performs mathematical calculations\"\n\n    async def execute(self, expression: str) -&gt; float:\n        # Safely evaluate math expression\n        return eval(expression, {\"__builtins__\": {}})\n\nclass MathPlugin(BasePlugin):\n    name = \"math\"\n    version = \"1.0.0\"\n\n    def get_tools(self) -&gt; List[Tool]:\n        return [CalculatorTool()]\n</code></pre>"},{"location":"plugins/creating-plugins/#providing-agents","title":"Providing Agents","text":"<p>Plugins can provide complete agent implementations:</p> <pre><code>from agenticraft.core.agent import Agent\n\nclass ResearchAgent(Agent):\n    \"\"\"Specialized agent for research tasks.\"\"\"\n\n    async def process(self, query: str) -&gt; str:\n        # Research implementation\n        return f\"Research results for: {query}\"\n\nclass ResearchPlugin(BasePlugin):\n    name = \"research\"\n    version = \"1.0.0\"\n\n    def get_agents(self) -&gt; List[type]:\n        return [ResearchAgent]\n</code></pre>"},{"location":"plugins/creating-plugins/#enhancing-existing-agents","title":"Enhancing Existing Agents","text":"<p>Plugins can modify or enhance agents:</p> <pre><code>class EnhancerPlugin(BasePlugin):\n    name = \"enhancer\"\n    version = \"1.0.0\"\n\n    def enhance_agent(self, agent):\n        \"\"\"Add capabilities to any agent.\"\"\"\n        # Add tools\n        for tool in self.get_tools():\n            agent.add_tool(tool)\n\n        # Add context\n        agent.add_context(\"You are enhanced with special abilities...\")\n\n        # Add capabilities\n        agent.add_capability(\"enhanced_reasoning\")\n\n        return agent\n</code></pre>"},{"location":"plugins/creating-plugins/#configuration","title":"Configuration","text":""},{"location":"plugins/creating-plugins/#plugin-configuration-schema","title":"Plugin Configuration Schema","text":"<p>Define configuration options for your plugin:</p> <pre><code>def get_config_schema(self) -&gt; Dict[str, Any]:\n    return {\n        \"type\": \"object\",\n        \"properties\": {\n            \"api_key\": {\n                \"type\": \"string\",\n                \"description\": \"API key for service\"\n            },\n            \"timeout\": {\n                \"type\": \"integer\",\n                \"default\": 30,\n                \"description\": \"Request timeout in seconds\"\n            },\n            \"retry_count\": {\n                \"type\": \"integer\",\n                \"default\": 3,\n                \"minimum\": 0,\n                \"maximum\": 10\n            }\n        },\n        \"required\": [\"api_key\"]\n    }\n</code></pre>"},{"location":"plugins/creating-plugins/#using-configuration","title":"Using Configuration","text":"<p>Access configuration in your plugin:</p> <pre><code>def initialize(self):\n    super().initialize()\n\n    # Get config values\n    api_key = self.config.config.get(\"api_key\")\n    timeout = self.config.config.get(\"timeout\", 30)\n\n    # Validate required config\n    if not api_key:\n        raise ValueError(\"API key is required\")\n\n    # Initialize with config\n    self.client = APIClient(api_key=api_key, timeout=timeout)\n</code></pre>"},{"location":"plugins/creating-plugins/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p>Plugins can hook into various lifecycle events:</p> <pre><code>from agenticraft.core.plugin import Plugin\n\nclass LifecyclePlugin(Plugin):\n    \"\"\"Example using core Plugin interface for lifecycle hooks.\"\"\"\n\n    def on_agent_created(self, agent):\n        \"\"\"Called when any agent is created.\"\"\"\n        print(f\"Agent created: {agent.name}\")\n\n    def on_agent_run_start(self, agent, prompt, context):\n        \"\"\"Called before agent processes input.\"\"\"\n        print(f\"Processing: {prompt[:50]}...\")\n\n    def on_agent_run_complete(self, agent, response):\n        \"\"\"Called after agent completes.\"\"\"\n        print(f\"Completed with {len(response.content)} chars\")\n\n    def on_tool_execution_start(self, tool_name, arguments):\n        \"\"\"Called before tool execution.\"\"\"\n        print(f\"Executing tool: {tool_name}\")\n\n    def on_response_generated(self, response):\n        \"\"\"Called to potentially modify responses.\"\"\"\n        # Add metadata\n        response.metadata[\"plugin_processed\"] = True\n        return response\n</code></pre>"},{"location":"plugins/creating-plugins/#best-practices","title":"Best Practices","text":""},{"location":"plugins/creating-plugins/#1-error-handling","title":"1. Error Handling","text":"<p>Always handle errors gracefully:</p> <pre><code>async def execute(self, **kwargs):\n    try:\n        result = await self.api_call(**kwargs)\n        return result\n    except APIError as e:\n        logger.error(f\"API error: {e}\")\n        return {\"error\": str(e)}\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"plugins/creating-plugins/#2-resource-management","title":"2. Resource Management","text":"<p>Use proper initialization and cleanup:</p> <pre><code>def initialize(self):\n    super().initialize()\n    self.connection = self._connect()\n    self.cache = {}\n\ndef cleanup(self):\n    if hasattr(self, 'connection'):\n        self.connection.close()\n    self.cache.clear()\n    super().cleanup()\n</code></pre>"},{"location":"plugins/creating-plugins/#3-async-support","title":"3. Async Support","text":"<p>Make tools and methods async when possible:</p> <pre><code>async def execute(self, query: str) -&gt; Dict[str, Any]:\n    # Async operations\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"{self.api_url}?q={query}\")\n        return response.json()\n</code></pre>"},{"location":"plugins/creating-plugins/#4-telemetry-integration","title":"4. Telemetry Integration","text":"<p>Add telemetry to track plugin usage:</p> <pre><code>from agenticraft.telemetry import track_metrics\n\nclass TelemetryPlugin(BasePlugin):\n    @track_metrics(\n        name=\"plugin.api_calls\",\n        labels=[\"endpoint\", \"status\"]\n    )\n    async def call_api(self, endpoint: str):\n        # Your API call\n        return result\n</code></pre>"},{"location":"plugins/creating-plugins/#5-documentation","title":"5. Documentation","text":"<p>Document your plugin thoroughly:</p> <pre><code>class WellDocumentedPlugin(BasePlugin):\n    \"\"\"\n    My Amazing Plugin\n\n    This plugin provides X, Y, and Z capabilities for AgentiCraft.\n\n    Configuration:\n        api_key (str): Required. Your API key\n        region (str): Optional. API region (default: \"us\")\n\n    Provides:\n        Tools: search, analyze, summarize\n        Agents: ResearchAgent, AnalysisAgent\n\n    Example:\n        plugin = WellDocumentedPlugin(PluginConfig(\n            config={\"api_key\": \"xxx\", \"region\": \"eu\"}\n        ))\n    \"\"\"\n</code></pre>"},{"location":"plugins/creating-plugins/#plugin-types","title":"Plugin Types","text":""},{"location":"plugins/creating-plugins/#tool-plugin","title":"Tool Plugin","text":"<p>For plugins that only provide tools:</p> <pre><code>from agenticraft.plugins import ToolPlugin\n\nclass UtilityPlugin(ToolPlugin):\n    name = \"utilities\"\n    version = \"1.0.0\"\n\n    def create_tools(self) -&gt; List[Tool]:\n        return [\n            DateTool(),\n            RandomTool(),\n            HashTool()\n        ]\n</code></pre>"},{"location":"plugins/creating-plugins/#agent-plugin","title":"Agent Plugin","text":"<p>For plugins that provide agents:</p> <pre><code>from agenticraft.plugins import AgentPlugin\n\nclass SpecialistPlugin(AgentPlugin):\n    name = \"specialists\"\n    version = \"1.0.0\"\n\n    def create_agents(self) -&gt; List[type]:\n        return [\n            CodeReviewAgent,\n            DocumentationAgent,\n            TestingAgent\n        ]\n</code></pre>"},{"location":"plugins/creating-plugins/#composite-plugin","title":"Composite Plugin","text":"<p>For comprehensive plugins:</p> <pre><code>from agenticraft.plugins import CompositePlugin\n\nclass FullFeaturePlugin(CompositePlugin):\n    name = \"full_feature\"\n    version = \"1.0.0\"\n\n    def get_tools(self) -&gt; List[Tool]:\n        return [Tool1(), Tool2()]\n\n    def get_agents(self) -&gt; List[type]:\n        return [Agent1, Agent2]\n\n    def get_providers(self) -&gt; Dict[str, type]:\n        return {\"custom\": CustomProvider}\n</code></pre>"},{"location":"plugins/creating-plugins/#testing-your-plugin","title":"Testing Your Plugin","text":""},{"location":"plugins/creating-plugins/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom agenticraft.plugins import PluginConfig\n\n@pytest.fixture\ndef plugin():\n    config = PluginConfig(config={\"api_key\": \"test\"})\n    return MyPlugin(config)\n\ndef test_plugin_info(plugin):\n    info = plugin.get_info()\n    assert info.name == \"my_plugin\"\n    assert \"tool1\" in info.provides_tools\n\nasync def test_tool_execution(plugin):\n    tools = plugin.get_tools()\n    result = await tools[0].execute(\"test input\")\n    assert result is not None\n</code></pre>"},{"location":"plugins/creating-plugins/#integration-tests","title":"Integration Tests","text":"<pre><code>async def test_plugin_with_agent():\n    # Load plugin\n    plugin = MyPlugin()\n    plugin.initialize()\n\n    # Create agent and enhance\n    agent = Agent(\"Test\")\n    enhanced = plugin.enhance_agent(agent)\n\n    # Test enhanced agent\n    response = await enhanced.run(\"test query\")\n    assert response.success\n\n    # Cleanup\n    plugin.cleanup()\n</code></pre>"},{"location":"plugins/creating-plugins/#distribution","title":"Distribution","text":""},{"location":"plugins/creating-plugins/#package-structure","title":"Package Structure","text":"<pre><code>my-plugin/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 my_plugin/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 plugin.py\n\u2502   \u251c\u2500\u2500 tools.py\n\u2502   \u2514\u2500\u2500 agents.py\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_plugin.py\n</code></pre>"},{"location":"plugins/creating-plugins/#pyprojecttoml","title":"pyproject.toml","text":"<pre><code>[project]\nname = \"agenticraft-my-plugin\"\nversion = \"1.0.0\"\ndescription = \"My plugin for AgentiCraft\"\ndependencies = [\n    \"agenticraft&gt;=0.1.0\",\n    \"httpx&gt;=0.25.0\"\n]\n\n[project.entry-points.\"agenticraft.plugins\"]\nmy_plugin = \"my_plugin:MyPlugin\"\n</code></pre>"},{"location":"plugins/creating-plugins/#installation","title":"Installation","text":"<p>Users can install your plugin with:</p> <pre><code>pip install agenticraft-my-plugin\n</code></pre> <p>Or from a directory:</p> <pre><code>agenticraft plugin install ./my-plugin\n</code></pre>"},{"location":"plugins/creating-plugins/#examples","title":"Examples","text":"<p>Plugin examples demonstrate common patterns:</p> <ul> <li>Weather Plugin - Basic tool plugin for weather data</li> <li>Research Plugin - Agent plugin for research capabilities</li> <li>Telemetry Plugin - Monitoring and metrics integration</li> <li>Composite Plugin - Full-featured plugin with tools, agents, and providers</li> </ul>"},{"location":"plugins/creating-plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>Plugins are discovered from:</p> <ol> <li>Built-in plugins directory</li> <li><code>~/.agenticraft/plugins/</code></li> <li>Directories in <code>AGENTICRAFT_PLUGIN_PATH</code></li> <li>Installed Python packages with entry points</li> </ol>"},{"location":"plugins/creating-plugins/#debugging","title":"Debugging","text":"<p>Enable debug logging to troubleshoot plugins:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(\"agenticraft.plugins\")\n\n# In your plugin\nlogger.debug(f\"Loading plugin: {self.name}\")\n</code></pre>"},{"location":"plugins/creating-plugins/#security-considerations","title":"Security Considerations","text":"<ol> <li>Validate all inputs from configuration and users</li> <li>Sanitize outputs before returning to agents</li> <li>Use minimal permissions for external services</li> <li>Don't store secrets in code or configs</li> <li>Follow secure coding practices</li> </ol>"},{"location":"plugins/creating-plugins/#faq","title":"FAQ","text":""},{"location":"plugins/creating-plugins/#q-can-plugins-depend-on-other-plugins","title":"Q: Can plugins depend on other plugins?","text":"<p>Yes, use the <code>depends_on</code> field in PluginInfo:</p> <pre><code>def get_info(self):\n    return PluginInfo(\n        name=\"my_plugin\",\n        depends_on=[\"base_plugin\", \"auth_plugin\"]\n    )\n</code></pre>"},{"location":"plugins/creating-plugins/#q-how-do-i-version-my-plugin","title":"Q: How do I version my plugin?","text":"<p>Use semantic versioning (MAJOR.MINOR.PATCH): - MAJOR: Breaking changes - MINOR: New features, backwards compatible - PATCH: Bug fixes</p>"},{"location":"plugins/creating-plugins/#q-can-i-use-external-libraries","title":"Q: Can I use external libraries?","text":"<p>Yes, declare them in your plugin's dependencies. Users will need to install them.</p>"},{"location":"plugins/creating-plugins/#q-how-do-i-handle-async-vs-sync-tools","title":"Q: How do I handle async vs sync tools?","text":"<p>AgentiCraft supports both. Use async when possible for better performance.</p>"},{"location":"plugins/creating-plugins/#getting-help","title":"Getting Help","text":"<ul> <li>Check plugin examples in the repository</li> <li>Join our Discord</li> <li>Open an issue</li> <li>Read the API Reference</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for AgentiCraft v0.1.1.</p>"},{"location":"reference/#core-apis","title":"Core APIs","text":""},{"location":"reference/#agent","title":"Agent","text":"<p>The foundation of AgentiCraft - create intelligent agents with tools, memory, and provider flexibility.</p> <pre><code>from agenticraft import Agent\n\nagent = Agent(name=\"Assistant\", model=\"gpt-4\")\nresponse = agent.run(\"Hello!\")\n</code></pre>"},{"location":"reference/#reasoningagent","title":"ReasoningAgent","text":"<p>Transparent reasoning with step-by-step thought processes.</p> <pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(name=\"Thinker\", model=\"gpt-4\")\nresponse = agent.run(\"Solve this problem...\")\n# Access reasoning: response.reasoning\n</code></pre>"},{"location":"reference/#workflowagent","title":"WorkflowAgent","text":"<p>Execute complex multi-step workflows with parallel processing.</p> <pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(name=\"Processor\", model=\"gpt-4\")\nresponse = agent.run_workflow(prompt, workflow=[...])\n</code></pre>"},{"location":"reference/#provider-apis","title":"Provider APIs","text":""},{"location":"reference/#openai","title":"OpenAI","text":"<ul> <li>GPT-4, GPT-3.5-Turbo</li> <li>Function calling</li> <li>Streaming support</li> </ul>"},{"location":"reference/#anthropic","title":"Anthropic","text":"<ul> <li>Claude 3 (Opus, Sonnet, Haiku)</li> <li>Large context windows</li> <li>Constitutional AI</li> </ul>"},{"location":"reference/#ollama","title":"Ollama","text":"<ul> <li>Local models (Llama2, Mistral, CodeLlama)</li> <li>Privacy-first</li> <li>No API costs</li> </ul>"},{"location":"reference/#tool-system","title":"Tool System","text":""},{"location":"reference/#tool-decorator","title":"@tool Decorator","text":"<p>Create tools with a simple decorator:</p> <pre><code>@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return results\n</code></pre>"},{"location":"reference/#tool-class","title":"Tool Class","text":"<p>Advanced tool configuration:</p> <pre><code>tool = Tool(\n    name=\"search\",\n    description=\"Search the web\",\n    function=search_function\n)\n</code></pre>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#agentconfig","title":"AgentConfig","text":"<p>Configure agents with type-safe dataclasses:</p> <pre><code>config = AgentConfig(\n    name=\"Bot\",\n    model=\"gpt-4\",\n    provider=\"openai\",\n    temperature=0.7\n)\n</code></pre>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#provider-switching","title":"Provider Switching","text":"<pre><code># Runtime provider changes\nagent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\n\n# Get current provider\ninfo = agent.get_provider_info()\n\n# List available providers\nproviders = agent.list_available_providers()\n</code></pre>"},{"location":"reference/#memory","title":"Memory","text":"<pre><code># Enable conversation memory\nagent = Agent(name=\"MemBot\", memory_enabled=True)\n\n# Access memory\nhistory = agent.memory.get_history()\n</code></pre>"},{"location":"reference/#error-handling","title":"Error Handling","text":"<pre><code>from agenticraft import ProviderError, ToolError\n\ntry:\n    response = agent.run(prompt)\nexcept ProviderError as e:\n    # Handle provider issues\n    agent.set_provider(\"ollama\", model=\"llama2\")\nexcept ToolError as e:\n    # Handle tool failures\n    pass\n</code></pre>"},{"location":"reference/#complete-examples","title":"Complete Examples","text":"<p>See the Examples section for complete working code: - Basic usage - Provider switching - Advanced agents</p>"},{"location":"reference/#api-versioning","title":"API Versioning","text":"<p>This documentation covers AgentiCraft v0.1.1. For detailed changes, see the Changelog.</p>"},{"location":"reference/agent/","title":"Agent API Reference","text":"<p>The Agent class is the core of AgentiCraft, providing intelligent AI capabilities with tool usage, memory, and provider flexibility.</p>"},{"location":"reference/agent/#agent","title":"Agent","text":"<pre><code>from agenticraft import Agent\n\nagent = Agent(\n    name=\"MyAgent\",\n    model=\"gpt-4\",\n    provider=\"openai\",  # Optional, auto-detected from model\n    **kwargs\n)\n</code></pre>"},{"location":"reference/agent/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> required Unique name for the agent <code>model</code> <code>str</code> <code>\"gpt-4\"</code> Model to use <code>provider</code> <code>str</code> <code>None</code> LLM provider (auto-detected if None) <code>tools</code> <code>List[Tool]</code> <code>[]</code> Tools available to the agent <code>memory_enabled</code> <code>bool</code> <code>False</code> Enable conversation memory <code>system_prompt</code> <code>str</code> <code>None</code> System instructions <code>temperature</code> <code>float</code> <code>0.7</code> Sampling temperature <code>max_tokens</code> <code>int</code> <code>None</code> Maximum response tokens"},{"location":"reference/agent/#methods","title":"Methods","text":""},{"location":"reference/agent/#runprompt-str-response","title":"run(prompt: str) -&gt; Response","text":"<p>Execute the agent with a prompt.</p> <pre><code>response = agent.run(\"Hello, how are you?\")\nprint(response.content)\n</code></pre>"},{"location":"reference/agent/#set_providerprovider-str-model-str-kwargs","title":"set_provider(provider: str, model: str, **kwargs)","text":"<p>Switch to a different LLM provider at runtime.</p> <pre><code>agent.set_provider(\"anthropic\", model=\"claude-3-opus-20240229\")\n</code></pre>"},{"location":"reference/agent/#get_provider_info-dictstr-any","title":"get_provider_info() -&gt; Dict[str, Any]","text":"<p>Get information about the current provider.</p> <pre><code>info = agent.get_provider_info()\n# {'provider': 'openai', 'model': 'gpt-4', 'temperature': 0.7}\n</code></pre>"},{"location":"reference/agent/#list_available_providers-liststr","title":"list_available_providers() -&gt; List[str]","text":"<p>List all available providers.</p> <pre><code>providers = agent.list_available_providers()\n# ['openai', 'anthropic', 'ollama']\n</code></pre>"},{"location":"reference/agent/#reasoningagent","title":"ReasoningAgent","text":"<p>An agent that provides transparent reasoning traces.</p> <pre><code>from agenticraft import ReasoningAgent\n\nagent = ReasoningAgent(\n    name=\"Thinker\",\n    model=\"gpt-4\",\n    reasoning_style=\"chain_of_thought\"\n)\n\nresponse = agent.run(\"Analyze this problem...\")\nprint(response.reasoning)  # List of reasoning steps\nprint(response.confidence)  # Confidence score\n</code></pre>"},{"location":"reference/agent/#additional-parameters","title":"Additional Parameters","text":"Parameter Type Default Description <code>reasoning_style</code> <code>str</code> <code>\"chain_of_thought\"</code> Reasoning approach <code>explore_branches</code> <code>int</code> <code>1</code> Branches for tree_of_thought <code>enable_self_critique</code> <code>bool</code> <code>False</code> Enable self-reflection"},{"location":"reference/agent/#workflowagent","title":"WorkflowAgent","text":"<p>An agent optimized for multi-step workflows.</p> <pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(name=\"Processor\", model=\"gpt-4\")\n\nworkflow = [\n    Step(\"analyze\", \"Analyze the data\"),\n    Step(\"process\", \"Process the results\"),\n    Step(\"report\", \"Generate report\")\n]\n\nresult = agent.run_workflow(\"Process sales data\", workflow)\n</code></pre>"},{"location":"reference/agent/#response-objects","title":"Response Objects","text":""},{"location":"reference/agent/#response","title":"Response","text":"<p>Basic response from an agent.</p> <pre><code>@dataclass\nclass Response:\n    content: str  # The response text\n    metadata: Dict[str, Any]  # Additional metadata\n</code></pre>"},{"location":"reference/agent/#reasoningresponse","title":"ReasoningResponse","text":"<p>Response from a ReasoningAgent.</p> <pre><code>@dataclass \nclass ReasoningResponse(Response):\n    reasoning: List[str]  # Reasoning steps\n    confidence: float  # Confidence score (0-1)\n    assumptions: List[str]  # Assumptions made\n</code></pre>"},{"location":"reference/agent/#workflowresponse","title":"WorkflowResponse","text":"<p>Response from a WorkflowAgent.</p> <pre><code>@dataclass\nclass WorkflowResponse(Response):\n    steps: Dict[str, StepResult]  # Results by step name\n    duration: float  # Total execution time\n</code></pre>"},{"location":"reference/agent/#examples","title":"Examples","text":""},{"location":"reference/agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from agenticraft import Agent\n\n# Simple agent\nagent = Agent(name=\"Assistant\", model=\"gpt-4\")\nresponse = agent.run(\"Tell me a joke\")\nprint(response.content)\n</code></pre>"},{"location":"reference/agent/#with-tools","title":"With Tools","text":"<pre><code>from agenticraft import Agent, tool\n\n@tool\ndef calculate(expression: str) -&gt; float:\n    return eval(expression)\n\nagent = Agent(name=\"MathBot\", tools=[calculate])\nresponse = agent.run(\"What's 15 * 23?\")\n</code></pre>"},{"location":"reference/agent/#provider-switching","title":"Provider Switching","text":"<pre><code># Start with GPT-4\nagent = Agent(name=\"Flex\", model=\"gpt-4\")\nresponse = agent.run(\"Complex analysis...\")\n\n# Switch to cheaper model\nagent.set_provider(\"ollama\", model=\"llama2\")\nresponse = agent.run(\"Simple summary...\")\n</code></pre>"},{"location":"reference/agent/#with-memory","title":"With Memory","text":"<pre><code>agent = Agent(name=\"MemBot\", memory_enabled=True)\n\nagent.run(\"My name is Alice\")\nresponse = agent.run(\"What's my name?\")\n# Agent remembers: \"Your name is Alice\"\n</code></pre>"},{"location":"reference/agent/#see-also","title":"See Also","text":"<ul> <li>Tool API - Creating and using tools</li> <li>Workflow API - Building workflows</li> <li>OpenAI Provider - OpenAI-specific details</li> <li>Anthropic Provider - Anthropic-specific details</li> <li>Ollama Provider - Ollama-specific details</li> </ul>"},{"location":"reference/api-v0.1.1/","title":"API Reference v0.1.1","text":""},{"location":"reference/api-v0.1.1/#core-classes","title":"Core Classes","text":""},{"location":"reference/api-v0.1.1/#agent","title":"Agent","text":"<p>The base agent class for all AgentiCraft agents.</p> <pre><code>class Agent:\n    def __init__(\n        self,\n        name: str,\n        model: str = \"gpt-4\",\n        provider: Optional[str] = None,\n        tools: Optional[List[Tool]] = None,\n        memory_enabled: bool = False,\n        **kwargs\n    )\n</code></pre>"},{"location":"reference/api-v0.1.1/#reasoningagent","title":"ReasoningAgent","text":"<p>Agent with transparent reasoning capabilities.</p> <pre><code>class ReasoningAgent(Agent):\n    def run(self, prompt: str) -&gt; ReasoningResponse:\n        \"\"\"Returns response with reasoning trace.\"\"\"\n</code></pre>"},{"location":"reference/api-v0.1.1/#workflowagent","title":"WorkflowAgent","text":"<p>Agent optimized for multi-step workflows.</p> <pre><code>class WorkflowAgent(Agent):\n    def run_workflow(\n        self, \n        prompt: str, \n        workflow: List[Step]\n    ) -&gt; WorkflowResponse:\n        \"\"\"Execute workflow and return step results.\"\"\"\n</code></pre>"},{"location":"reference/api-v0.1.1/#provider-management","title":"Provider Management","text":""},{"location":"reference/api-v0.1.1/#set_provider","title":"set_provider()","text":"<pre><code>agent.set_provider(\n    provider: str,\n    model: str,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Switch LLM provider at runtime.</p>"},{"location":"reference/api-v0.1.1/#get_provider_info","title":"get_provider_info()","text":"<pre><code>agent.get_provider_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get current provider information.</p>"},{"location":"reference/api-v0.1.1/#list_available_providers","title":"list_available_providers()","text":"<pre><code>agent.list_available_providers() -&gt; List[str]\n</code></pre> <p>List all available providers.</p>"},{"location":"reference/api-v0.1.1/#tools","title":"Tools","text":""},{"location":"reference/api-v0.1.1/#tool-decorator","title":"@tool decorator","text":"<pre><code>@tool\ndef my_tool(param: str) -&gt; str:\n    \"\"\"Tool description.\"\"\"\n    return result\n</code></pre>"},{"location":"reference/api-v0.1.1/#tool-class","title":"Tool class","text":"<pre><code>class Tool:\n    name: str\n    description: str\n    parameters: Dict[str, Any]\n    function: Callable\n</code></pre>"},{"location":"reference/api-v0.1.1/#configuration","title":"Configuration","text":""},{"location":"reference/api-v0.1.1/#agentconfig","title":"AgentConfig","text":"<pre><code>@dataclass\nclass AgentConfig:\n    name: str\n    model: str = \"gpt-4\"\n    provider: Optional[str] = None\n    temperature: float = 0.7\n    max_tokens: int = 2000\n    tools: List[Tool] = field(default_factory=list)\n    memory_enabled: bool = False\n</code></pre>"},{"location":"reference/api-v0.1.1/#responses","title":"Responses","text":""},{"location":"reference/api-v0.1.1/#response","title":"Response","text":"<pre><code>@dataclass\nclass Response:\n    content: str\n    metadata: Dict[str, Any]\n</code></pre>"},{"location":"reference/api-v0.1.1/#reasoningresponse","title":"ReasoningResponse","text":"<pre><code>@dataclass\nclass ReasoningResponse(Response):\n    reasoning: List[str]\n    confidence: float\n</code></pre>"},{"location":"reference/api-v0.1.1/#workflowresponse","title":"WorkflowResponse","text":"<pre><code>@dataclass\nclass WorkflowResponse(Response):\n    steps: Dict[str, StepResult]\n    duration: float\n</code></pre>"},{"location":"reference/api-v0.1.1/#exceptions","title":"Exceptions","text":""},{"location":"reference/api-v0.1.1/#agenticrafterror","title":"AgentiCraftError","text":"<p>Base exception for all AgentiCraft errors.</p>"},{"location":"reference/api-v0.1.1/#providererror","title":"ProviderError","text":"<p>Raised when provider operations fail.</p>"},{"location":"reference/api-v0.1.1/#toolerror","title":"ToolError","text":"<p>Raised when tool execution fails.</p>"},{"location":"reference/api-v0.1.1/#full-api-documentation","title":"Full API Documentation","text":"<p>For complete API documentation with all parameters and examples, see: - Agent API - Tool API - Workflow API - Provider APIs:   - OpenAI   - Anthropic   - Ollama</p>"},{"location":"reference/tool/","title":"Tool API Reference","text":"<p>Tools extend agent capabilities by providing functions they can call to interact with external systems.</p>"},{"location":"reference/tool/#creating-tools","title":"Creating Tools","text":""},{"location":"reference/tool/#tool-decorator","title":"@tool Decorator","text":"<p>The simplest way to create a tool:</p> <pre><code>from agenticraft import tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Implementation\n    return f\"Results for: {query}\"\n</code></pre>"},{"location":"reference/tool/#tool-function-requirements","title":"Tool Function Requirements","text":"<ol> <li>Type Hints: Always include type hints for parameters and return values</li> <li>Docstring: The docstring is used by the LLM to understand when to use the tool</li> <li>Return Values: Tools should return strings or JSON-serializable data</li> </ol>"},{"location":"reference/tool/#advanced-tool-definition","title":"Advanced Tool Definition","text":"<pre><code>@tool\ndef complex_tool(\n    required_param: str,\n    optional_param: int = 10,\n    another_param: bool = False\n) -&gt; dict:\n    \"\"\"\n    A complex tool with multiple parameters.\n\n    Args:\n        required_param: This parameter is required\n        optional_param: This one has a default value\n        another_param: A boolean flag\n\n    Returns:\n        A dictionary with results\n    \"\"\"\n    return {\n        \"result\": required_param,\n        \"count\": optional_param,\n        \"flag\": another_param\n    }\n</code></pre>"},{"location":"reference/tool/#tool-class","title":"Tool Class","text":"<p>For more control, use the Tool class directly:</p> <pre><code>from agenticraft import Tool\n\nclass DatabaseTool(Tool):\n    def __init__(self, connection_string: str):\n        super().__init__(\n            name=\"query_database\",\n            description=\"Execute SQL queries on the database\",\n            parameters={\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"SQL query to execute\"\n                },\n                \"limit\": {\n                    \"type\": \"integer\", \n                    \"description\": \"Maximum rows to return\",\n                    \"default\": 100\n                }\n            }\n        )\n        self.conn = self._connect(connection_string)\n\n    def execute(self, query: str, limit: int = 100) -&gt; str:\n        \"\"\"Execute the tool logic.\"\"\"\n        results = self.conn.execute(query).fetchmany(limit)\n        return str(results)\n</code></pre>"},{"location":"reference/tool/#using-tools-with-agents","title":"Using Tools with Agents","text":""},{"location":"reference/tool/#basic-usage","title":"Basic Usage","text":"<pre><code>from agenticraft import Agent, tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather for a location.\"\"\"\n    return f\"Sunny, 72\u00b0F in {location}\"\n\n@tool\ndef set_reminder(task: str, time: str) -&gt; str:\n    \"\"\"Set a reminder for a specific time.\"\"\"\n    return f\"Reminder set: {task} at {time}\"\n\n# Create agent with tools\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt-4\",\n    tools=[get_weather, set_reminder]\n)\n\n# Agent automatically uses tools when needed\nresponse = agent.run(\"What's the weather in NYC and remind me to bring an umbrella at 3pm\")\n</code></pre>"},{"location":"reference/tool/#dynamic-tool-addition","title":"Dynamic Tool Addition","text":"<pre><code>agent = Agent(name=\"Bot\", model=\"gpt-4\")\n\n# Add tools after creation\nagent.add_tool(my_tool)\nagent.add_tools([tool1, tool2, tool3])\n\n# Remove tools\nagent.remove_tool(\"tool_name\")\n</code></pre>"},{"location":"reference/tool/#tool-patterns","title":"Tool Patterns","text":""},{"location":"reference/tool/#error-handling","title":"Error Handling","text":"<pre><code>@tool\ndef safe_tool(param: str) -&gt; str:\n    \"\"\"A tool with error handling.\"\"\"\n    try:\n        # Tool logic\n        result = process(param)\n        return f\"Success: {result}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n</code></pre>"},{"location":"reference/tool/#async-tools","title":"Async Tools","text":"<pre><code>@tool\nasync def async_tool(url: str) -&gt; str:\n    \"\"\"An async tool for network operations.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.text()\n</code></pre>"},{"location":"reference/tool/#stateful-tools","title":"Stateful Tools","text":"<pre><code>class StatefulTool(Tool):\n    def __init__(self):\n        super().__init__(\n            name=\"counter\",\n            description=\"Increment and track a counter\"\n        )\n        self.count = 0\n\n    def execute(self) -&gt; str:\n        self.count += 1\n        return f\"Count is now: {self.count}\"\n</code></pre>"},{"location":"reference/tool/#composite-tools","title":"Composite Tools","text":"<pre><code>@tool\ndef research_and_summarize(topic: str) -&gt; str:\n    \"\"\"Research a topic and provide a summary.\"\"\"\n    # Use other tools internally\n    search_results = search_tool.execute(topic)\n    summary = summarize_tool.execute(search_results)\n    return summary\n</code></pre>"},{"location":"reference/tool/#tool-configuration","title":"Tool Configuration","text":""},{"location":"reference/tool/#tool-metadata","title":"Tool Metadata","text":"<pre><code>@tool(\n    name=\"custom_name\",  # Override function name\n    description=\"Custom description\",\n    tags=[\"search\", \"web\"],\n    version=\"1.0.0\"\n)\ndef my_tool(query: str) -&gt; str:\n    return \"result\"\n</code></pre>"},{"location":"reference/tool/#tool-permissions","title":"Tool Permissions","text":"<pre><code>@tool(\n    requires_confirmation=True,  # Ask user before executing\n    rate_limit=10,  # Max calls per minute\n    cost=0.01  # Cost per call for tracking\n)\ndef expensive_tool(data: str) -&gt; str:\n    return process_data(data)\n</code></pre>"},{"location":"reference/tool/#built-in-tools","title":"Built-in Tools","text":"<p>AgentiCraft provides several built-in tools:</p> <pre><code>from agenticraft.tools import (\n    web_search,\n    read_file,\n    write_file,\n    execute_code,\n    query_database\n)\n\nagent = Agent(\n    name=\"PowerUser\",\n    model=\"gpt-4\",\n    tools=[web_search, read_file, write_file]\n)\n</code></pre>"},{"location":"reference/tool/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions: Write detailed docstrings that explain what the tool does</li> <li>Type Safety: Always use type hints</li> <li>Error Handling: Handle exceptions gracefully</li> <li>Idempotency: Make tools idempotent when possible</li> <li>Security: Validate inputs and sanitize outputs</li> <li>Performance: Consider caching for expensive operations</li> </ol>"},{"location":"reference/tool/#examples","title":"Examples","text":""},{"location":"reference/tool/#web-scraping-tool","title":"Web Scraping Tool","text":"<pre><code>@tool\ndef scrape_website(url: str, selector: str = \"body\") -&gt; str:\n    \"\"\"\n    Scrape content from a website.\n\n    Args:\n        url: The URL to scrape\n        selector: CSS selector for content (default: body)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.select_one(selector)\n    return content.text if content else \"No content found\"\n</code></pre>"},{"location":"reference/tool/#api-integration-tool","title":"API Integration Tool","text":"<pre><code>@tool\ndef call_api(\n    endpoint: str,\n    method: str = \"GET\",\n    data: dict = None\n) -&gt; str:\n    \"\"\"\n    Make an API call.\n\n    Args:\n        endpoint: API endpoint URL\n        method: HTTP method (GET, POST, etc.)\n        data: Request data for POST/PUT\n    \"\"\"\n    if method == \"GET\":\n        response = requests.get(endpoint)\n    elif method == \"POST\":\n        response = requests.post(endpoint, json=data)\n\n    return response.json()\n</code></pre>"},{"location":"reference/tool/#data-processing-tool","title":"Data Processing Tool","text":"<pre><code>@tool\ndef analyze_csv(\n    file_path: str,\n    operation: str = \"summary\"\n) -&gt; str:\n    \"\"\"\n    Analyze a CSV file.\n\n    Args:\n        file_path: Path to CSV file\n        operation: Type of analysis (summary, stats, plot)\n    \"\"\"\n    df = pd.read_csv(file_path)\n\n    if operation == \"summary\":\n        return df.describe().to_string()\n    elif operation == \"stats\":\n        return {\n            \"rows\": len(df),\n            \"columns\": list(df.columns),\n            \"missing\": df.isnull().sum().to_dict()\n        }\n    elif operation == \"plot\":\n        # Generate and save plot\n        df.plot()\n        plt.savefig(\"output.png\")\n        return \"Plot saved to output.png\"\n</code></pre>"},{"location":"reference/tool/#see-also","title":"See Also","text":"<ul> <li>Agent API - Using tools with agents</li> <li>MCP Integration - Model Context Protocol tools</li> <li>Creating Custom Tools - Tool creation guide</li> </ul>"},{"location":"reference/workflow/","title":"Workflow API Reference","text":"<p>The Workflow system enables complex multi-step processes with dependencies, parallel execution, and error handling.</p>"},{"location":"reference/workflow/#workflowagent","title":"WorkflowAgent","text":"<p>The primary class for executing workflows.</p> <pre><code>from agenticraft import WorkflowAgent, Step\n\nagent = WorkflowAgent(\n    name=\"Processor\",\n    model=\"gpt-4\",\n    parallel=True  # Enable parallel step execution\n)\n</code></pre>"},{"location":"reference/workflow/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> required Agent name <code>model</code> <code>str</code> <code>\"gpt-4\"</code> LLM model <code>parallel</code> <code>bool</code> <code>False</code> Enable parallel execution <code>max_workers</code> <code>int</code> <code>4</code> Max parallel workers <code>timeout</code> <code>float</code> <code>300</code> Step timeout in seconds"},{"location":"reference/workflow/#methods","title":"Methods","text":""},{"location":"reference/workflow/#run_workflowprompt-str-workflow-liststep-workflowresponse","title":"run_workflow(prompt: str, workflow: List[Step]) -&gt; WorkflowResponse","text":"<p>Execute a workflow with the given prompt.</p> <pre><code>workflow = [\n    Step(\"step1\", \"Do first task\"),\n    Step(\"step2\", \"Do second task\", depends_on=[\"step1\"])\n]\n\nresult = agent.run_workflow(\"Process data\", workflow)\n</code></pre>"},{"location":"reference/workflow/#step","title":"Step","text":"<p>Define individual workflow steps.</p> <pre><code>from agenticraft import Step\n\nstep = Step(\n    name=\"process_data\",\n    description=\"Process the input data\",\n    depends_on=[\"previous_step\"],\n    condition=\"if data exists\",\n    retry_count=3,\n    timeout=60\n)\n</code></pre>"},{"location":"reference/workflow/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> required Unique step identifier <code>description</code> <code>str</code> required What the step does <code>depends_on</code> <code>List[str]</code> <code>[]</code> Steps that must complete first <code>condition</code> <code>str</code> <code>None</code> Condition for execution <code>retry_count</code> <code>int</code> <code>0</code> Number of retries on failure <code>timeout</code> <code>float</code> <code>None</code> Step timeout override <code>parallel</code> <code>bool</code> <code>True</code> Can run in parallel <code>fallback</code> <code>str</code> <code>None</code> Fallback step on failure"},{"location":"reference/workflow/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"reference/workflow/#sequential-workflow","title":"Sequential Workflow","text":"<pre><code>sequential_workflow = [\n    Step(\"fetch\", \"Fetch data from source\"),\n    Step(\"validate\", \"Validate data\", depends_on=[\"fetch\"]),\n    Step(\"transform\", \"Transform data\", depends_on=[\"validate\"]),\n    Step(\"save\", \"Save results\", depends_on=[\"transform\"])\n]\n</code></pre>"},{"location":"reference/workflow/#parallel-workflow","title":"Parallel Workflow","text":"<pre><code>parallel_workflow = [\n    # These run in parallel\n    Step(\"fetch_users\", \"Get user data\"),\n    Step(\"fetch_orders\", \"Get order data\"),\n    Step(\"fetch_products\", \"Get product data\"),\n\n    # This waits for all three\n    Step(\"combine\", \"Combine all data\",\n         depends_on=[\"fetch_users\", \"fetch_orders\", \"fetch_products\"])\n]\n</code></pre>"},{"location":"reference/workflow/#conditional-workflow","title":"Conditional Workflow","text":"<pre><code>conditional_workflow = [\n    Step(\"check_cache\", \"Check if data is cached\"),\n    Step(\"fetch_remote\", \"Fetch from API\",\n         condition=\"if not cached\"),\n    Step(\"process\", \"Process the data\",\n         depends_on=[\"check_cache\", \"fetch_remote\"])\n]\n</code></pre>"},{"location":"reference/workflow/#error-handling-workflow","title":"Error Handling Workflow","text":"<pre><code>resilient_workflow = [\n    Step(\"risky_operation\", \"Perform risky operation\",\n         retry_count=3,\n         fallback=\"safe_operation\"),\n    Step(\"safe_operation\", \"Fallback operation\",\n         skip_by_default=True),\n    Step(\"continue\", \"Continue processing\",\n         depends_on=[\"risky_operation\", \"safe_operation\"])\n]\n</code></pre>"},{"location":"reference/workflow/#workflowresponse","title":"WorkflowResponse","text":"<p>The response from workflow execution.</p> <pre><code>@dataclass\nclass WorkflowResponse:\n    content: str  # Final result\n    steps: Dict[str, StepResult]  # Results by step name\n    duration: float  # Total execution time\n    success: bool  # Overall success status\n</code></pre>"},{"location":"reference/workflow/#stepresult","title":"StepResult","text":"<pre><code>@dataclass\nclass StepResult:\n    name: str  # Step name\n    status: str  # \"success\", \"failed\", \"skipped\"\n    output: str  # Step output\n    error: Optional[str]  # Error message if failed\n    duration: float  # Execution time\n    retries: int  # Number of retries used\n</code></pre>"},{"location":"reference/workflow/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/workflow/#custom-step-handlers","title":"Custom Step Handlers","text":"<pre><code>def custom_handler(context: Dict[str, Any]) -&gt; str:\n    \"\"\"Custom step implementation.\"\"\"\n    previous_output = context.get(\"previous_step_output\")\n    # Custom logic\n    return \"Custom result\"\n\nagent.set_step_handler(\"custom_step\", custom_handler)\n</code></pre>"},{"location":"reference/workflow/#progress-callbacks","title":"Progress Callbacks","text":"<pre><code>def on_step_complete(step_name: str, result: StepResult):\n    print(f\"Completed {step_name}: {result.status}\")\n\nagent.on_step_complete = on_step_complete\n</code></pre>"},{"location":"reference/workflow/#workflow-templates","title":"Workflow Templates","text":"<pre><code>class DataPipelineTemplate:\n    @staticmethod\n    def create(source: str, destination: str) -&gt; List[Step]:\n        return [\n            Step(\"extract\", f\"Extract from {source}\"),\n            Step(\"transform\", \"Clean and transform\"),\n            Step(\"load\", f\"Load to {destination}\"),\n            Step(\"verify\", \"Verify data integrity\")\n        ]\n\n# Use template\nworkflow = DataPipelineTemplate.create(\"database\", \"warehouse\")\nresult = agent.run_workflow(\"Run ETL\", workflow)\n</code></pre>"},{"location":"reference/workflow/#dynamic-workflows","title":"Dynamic Workflows","text":"<pre><code>def build_workflow(task_count: int) -&gt; List[Step]:\n    \"\"\"Build workflow dynamically based on input.\"\"\"\n    workflow = []\n\n    # Create parallel tasks\n    for i in range(task_count):\n        workflow.append(\n            Step(f\"task_{i}\", f\"Process chunk {i}\")\n        )\n\n    # Add aggregation step\n    task_names = [f\"task_{i}\" for i in range(task_count)]\n    workflow.append(\n        Step(\"aggregate\", \"Combine results\",\n             depends_on=task_names)\n    )\n\n    return workflow\n\n# Use dynamic workflow\ndynamic = build_workflow(5)\nresult = agent.run_workflow(\"Process in parallel\", dynamic)\n</code></pre>"},{"location":"reference/workflow/#best-practices","title":"Best Practices","text":"<ol> <li>Step Granularity: Keep steps focused on single tasks</li> <li>Clear Dependencies: Explicitly define step relationships</li> <li>Error Handling: Use retries and fallbacks for reliability</li> <li>Timeouts: Set appropriate timeouts for long-running steps</li> <li>Logging: Enable detailed logging for debugging</li> </ol>"},{"location":"reference/workflow/#complete-example","title":"Complete Example","text":"<pre><code>from agenticraft import WorkflowAgent, Step\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\n\nclass ReportGenerator:\n    def __init__(self):\n        self.agent = WorkflowAgent(\n            name=\"ReportGen\",\n            model=\"gpt-4\",\n            parallel=True,\n            max_workers=3\n        )\n\n    def generate_report(self, company: str, quarter: str):\n        \"\"\"Generate quarterly report.\"\"\"\n\n        workflow = [\n            # Data collection (parallel)\n            Step(\"financial_data\", \n                 f\"Get financial data for {company} Q{quarter}\"),\n            Step(\"market_data\",\n                 f\"Get market analysis for Q{quarter}\"),\n            Step(\"competitor_data\",\n                 f\"Get competitor analysis\"),\n\n            # Analysis (depends on data)\n            Step(\"financial_analysis\",\n                 \"Analyze financial performance\",\n                 depends_on=[\"financial_data\"]),\n            Step(\"market_position\",\n                 \"Analyze market position\",\n                 depends_on=[\"market_data\", \"competitor_data\"]),\n\n            # Report sections (parallel after analysis)\n            Step(\"executive_summary\",\n                 \"Write executive summary\",\n                 depends_on=[\"financial_analysis\", \"market_position\"]),\n            Step(\"detailed_analysis\",\n                 \"Write detailed analysis\",\n                 depends_on=[\"financial_analysis\", \"market_position\"]),\n            Step(\"recommendations\",\n                 \"Generate recommendations\",\n                 depends_on=[\"financial_analysis\", \"market_position\"]),\n\n            # Final assembly\n            Step(\"assemble_report\",\n                 \"Combine all sections into final report\",\n                 depends_on=[\"executive_summary\", \n                           \"detailed_analysis\", \n                           \"recommendations\"]),\n\n            # Quality check\n            Step(\"quality_check\",\n                 \"Review and polish report\",\n                 depends_on=[\"assemble_report\"],\n                 retry_count=2)\n        ]\n\n        # Set up progress tracking\n        self.agent.on_step_complete = self._log_progress\n\n        # Run workflow\n        result = self.agent.run_workflow(\n            f\"Generate Q{quarter} report for {company}\",\n            workflow\n        )\n\n        return {\n            \"report\": result.steps[\"assemble_report\"].output,\n            \"summary\": result.steps[\"executive_summary\"].output,\n            \"duration\": result.duration,\n            \"success\": result.success\n        }\n\n    def _log_progress(self, step_name: str, result: StepResult):\n        \"\"\"Log step progress.\"\"\"\n        status_emoji = \"\u2705\" if result.status == \"success\" else \"\u274c\"\n        logging.info(\n            f\"{status_emoji} {step_name}: \"\n            f\"{result.duration:.2f}s\"\n        )\n\n# Usage\ngenerator = ReportGenerator()\nreport = generator.generate_report(\"TechCorp\", \"4\")\nprint(report[\"summary\"])\nprint(f\"Generated in {report['duration']:.2f} seconds\")\n</code></pre>"},{"location":"reference/workflow/#see-also","title":"See Also","text":"<ul> <li>WorkflowAgent - WorkflowAgent class details</li> <li>Workflow Concepts - Understanding workflows</li> <li>Advanced Examples - Complex workflow examples</li> </ul>"},{"location":"reference/providers/anthropic/","title":"Anthropic Provider Reference","text":"<p>The Anthropic provider supports Claude 3 models including Opus, Sonnet, and Haiku.</p>"},{"location":"reference/providers/anthropic/#configuration","title":"Configuration","text":""},{"location":"reference/providers/anthropic/#environment-variables","title":"Environment Variables","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"reference/providers/anthropic/#initialization","title":"Initialization","text":"<pre><code>from agenticraft import Agent\n\n# Explicit provider required for Anthropic\nagent = Agent(\n    name=\"Claude\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\"\n)\n</code></pre>"},{"location":"reference/providers/anthropic/#supported-models","title":"Supported Models","text":"Model Description Context Window Best For <code>claude-3-opus-20240229</code> Most capable 200K tokens Complex analysis, reasoning <code>claude-3-sonnet-20240229</code> Balanced performance 200K tokens General tasks <code>claude-3-haiku-20240307</code> Fast and efficient 200K tokens High-volume, simple tasks <code>claude-2.1</code> Previous generation 100K tokens Legacy support <code>claude-instant-1.2</code> Fastest 100K tokens Real-time applications"},{"location":"reference/providers/anthropic/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"reference/providers/anthropic/#constitutional-ai","title":"Constitutional AI","text":"<p>Claude models are trained with Constitutional AI for helpful, harmless, and honest responses:</p> <pre><code>agent = Agent(\n    name=\"SafeAssistant\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\",\n    system_prompt=\"You are a helpful, harmless, and honest assistant.\"\n)\n</code></pre>"},{"location":"reference/providers/anthropic/#large-context-window","title":"Large Context Window","text":"<p>Claude excels at processing long documents:</p> <pre><code>agent = Agent(\n    name=\"DocumentAnalyzer\",\n    provider=\"anthropic\", \n    model=\"claude-3-opus-20240229\"\n)\n\n# Process a long document (up to 200K tokens)\nwith open(\"long_document.txt\", \"r\") as f:\n    document = f.read()\n\nresponse = agent.run(f\"Analyze this document:\\n\\n{document}\")\n</code></pre>"},{"location":"reference/providers/anthropic/#xml-tags-support","title":"XML Tags Support","text":"<p>Claude works well with structured prompts using XML tags:</p> <pre><code>prompt = \"\"\"\n&lt;document&gt;\n{document_content}\n&lt;/document&gt;\n\n&lt;instructions&gt;\n1. Summarize the key points\n2. Identify any risks\n3. Suggest next steps\n&lt;/instructions&gt;\n\nPlease analyze the document according to the instructions.\n\"\"\"\n\nresponse = agent.run(prompt)\n</code></pre>"},{"location":"reference/providers/anthropic/#vision-capabilities","title":"Vision Capabilities","text":"<p>Claude 3 models support image analysis:</p> <pre><code>agent = Agent(\n    name=\"VisionClaude\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\"\n)\n\nresponse = agent.run(\n    prompt=\"Describe this image in detail\",\n    images=[\"path/to/image.jpg\"]\n)\n</code></pre>"},{"location":"reference/providers/anthropic/#configuration-options","title":"Configuration Options","text":"<pre><code>agent = Agent(\n    name=\"ConfiguredClaude\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\",\n\n    # Anthropic-specific options\n    temperature=0.7,        # 0.0-1.0\n    max_tokens=4000,       # Max response length\n    top_p=0.9,            # Nucleus sampling\n    top_k=0,              # Top-k sampling (0 = disabled)\n\n    # Safety settings\n    stop_sequences=[\"\\n\\nHuman:\"],\n\n    # Retry configuration\n    max_retries=3,\n    timeout=60  # Claude can take longer for complex tasks\n)\n</code></pre>"},{"location":"reference/providers/anthropic/#tool-usage","title":"Tool Usage","text":"<p>Claude supports tool use through a specific format:</p> <pre><code>from agenticraft import Agent, tool\n\n@tool\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return str(eval(expression, {\"__builtins__\": {}}))\n\n@tool \ndef search(query: str) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\nagent = Agent(\n    name=\"ClaudeWithTools\",\n    provider=\"anthropic\",\n    model=\"claude-3-opus-20240229\",\n    tools=[calculate, search]\n)\n\n# Claude will use tools when appropriate\nresponse = agent.run(\"What's 15% of 2500? Also search for tax rates.\")\n</code></pre>"},{"location":"reference/providers/anthropic/#error-handling","title":"Error Handling","text":"<pre><code>from agenticraft import Agent, ProviderError\n\ntry:\n    agent = Agent(\n        name=\"Claude\",\n        provider=\"anthropic\",\n        model=\"claude-3-opus-20240229\"\n    )\n    response = agent.run(\"Hello\")\nexcept ProviderError as e:\n    if \"rate_limit\" in str(e):\n        print(\"Rate limit reached\")\n    elif \"invalid_api_key\" in str(e):\n        print(\"Check your Anthropic API key\")\n    else:\n        print(f\"Anthropic error: {e}\")\n</code></pre>"},{"location":"reference/providers/anthropic/#cost-optimization","title":"Cost Optimization","text":""},{"location":"reference/providers/anthropic/#model-selection-strategy","title":"Model Selection Strategy","text":"<pre><code>class AnthropicOptimizer:\n    def __init__(self):\n        self.agent = Agent(\n            name=\"Optimizer\",\n            provider=\"anthropic\",\n            model=\"claude-3-haiku-20240307\"  # Start with cheapest\n        )\n\n    def process(self, task: str, requirements: dict):\n        # Determine model based on requirements\n        if requirements.get(\"complexity\") == \"high\":\n            model = \"claude-3-opus-20240229\"\n        elif requirements.get(\"speed\") == \"fast\":\n            model = \"claude-3-haiku-20240307\"\n        else:\n            model = \"claude-3-sonnet-20240229\"\n\n        self.agent.model = model\n        return self.agent.run(task)\n</code></pre>"},{"location":"reference/providers/anthropic/#token-estimation","title":"Token Estimation","text":"<pre><code>def estimate_tokens(text: str) -&gt; int:\n    \"\"\"Rough estimation of Claude tokens.\"\"\"\n    # Claude uses a similar tokenization to ~4 chars per token\n    return len(text) // 4\n\ndef check_context_fit(text: str, model: str) -&gt; bool:\n    \"\"\"Check if text fits in model context.\"\"\"\n    limits = {\n        \"claude-3-opus-20240229\": 200000,\n        \"claude-3-sonnet-20240229\": 200000,\n        \"claude-3-haiku-20240307\": 200000,\n        \"claude-2.1\": 100000\n    }\n    return estimate_tokens(text) &lt; limits.get(model, 100000)\n</code></pre>"},{"location":"reference/providers/anthropic/#best-practices","title":"Best Practices","text":"<ol> <li>Prompt Engineering: Use clear, structured prompts with Claude</li> <li>XML Tags: Leverage XML tags for better organization</li> <li>Context Management: Take advantage of large context windows</li> <li>Safety: Claude is designed to be helpful, harmless, and honest</li> <li>Rate Limits: Anthropic has strict rate limits - implement backoff</li> </ol>"},{"location":"reference/providers/anthropic/#complete-example","title":"Complete Example","text":"<pre><code>import os\nfrom agenticraft import Agent, tool\nfrom typing import Dict, List\n\nclass ClaudeAssistant:\n    def __init__(self):\n        if not os.getenv(\"ANTHROPIC_API_KEY\"):\n            raise ValueError(\"ANTHROPIC_API_KEY not set\")\n\n        self.agent = Agent(\n            name=\"ClaudeAssistant\",\n            provider=\"anthropic\",\n            model=\"claude-3-opus-20240229\",\n            temperature=0.7,\n            max_tokens=4000,\n            system_prompt=\"\"\"You are Claude, a helpful AI assistant.\n            You excel at analysis, writing, and reasoning.\n            Always strive to be helpful, harmless, and honest.\"\"\"\n        )\n\n        self._setup_tools()\n\n    def _setup_tools(self):\n        @tool\n        def analyze_data(data: str) -&gt; str:\n            \"\"\"Analyze provided data.\"\"\"\n            # Simulate data analysis\n            return f\"Analysis of data: {len(data)} characters processed\"\n\n        @tool\n        def generate_report(topic: str, sections: List[str]) -&gt; str:\n            \"\"\"Generate a structured report.\"\"\"\n            outline = \"\\n\".join(f\"- {s}\" for s in sections)\n            return f\"Report outline for {topic}:\\n{outline}\"\n\n        self.agent.tools = [analyze_data, generate_report]\n\n    def analyze_document(self, document: str) -&gt; Dict:\n        \"\"\"Analyze a document with structured output.\"\"\"\n\n        prompt = f\"\"\"\n        &lt;document&gt;\n        {document}\n        &lt;/document&gt;\n\n        &lt;task&gt;\n        Please analyze this document and provide:\n        1. A brief summary (2-3 sentences)\n        2. Key points (bullet list)\n        3. Potential concerns or risks\n        4. Recommended actions\n        5. Confidence level in your analysis\n        &lt;/task&gt;\n\n        Format your response with clear sections.\n        \"\"\"\n\n        response = self.agent.run(prompt)\n\n        # Parse structured response\n        return {\n            \"analysis\": response.content,\n            \"model\": \"claude-3-opus-20240229\",\n            \"tokens\": estimate_tokens(document + response.content)\n        }\n\n    def creative_writing(self, prompt: str, style: str = \"professional\"):\n        \"\"\"Generate creative content.\"\"\"\n\n        style_prompts = {\n            \"professional\": \"Write in a clear, professional tone\",\n            \"casual\": \"Write in a friendly, conversational tone\",\n            \"academic\": \"Write in a formal, academic style with citations\",\n            \"creative\": \"Write creatively with vivid descriptions\"\n        }\n\n        full_prompt = f\"\"\"\n        {style_prompts.get(style, style_prompts['professional'])}\n\n        Task: {prompt}\n\n        Please create high-quality content that engages the reader.\n        \"\"\"\n\n        # Use Sonnet for creative tasks (good balance)\n        self.agent.model = \"claude-3-sonnet-20240229\"\n        response = self.agent.run(full_prompt)\n\n        # Switch back to Opus\n        self.agent.model = \"claude-3-opus-20240229\"\n\n        return response.content\n\n# Usage\nassistant = ClaudeAssistant()\n\n# Document analysis\ndoc = \"Your long document text here...\"\nanalysis = assistant.analyze_document(doc)\nprint(analysis[\"analysis\"])\n\n# Creative writing\nstory = assistant.creative_writing(\n    \"Write a short story about AI and humanity\",\n    style=\"creative\"\n)\nprint(story)\n</code></pre>"},{"location":"reference/providers/anthropic/#anthropic-specific-tips","title":"Anthropic-Specific Tips","text":"<ol> <li>Prompt Structure: Claude responds well to clear structure and formatting</li> <li>Chain of Thought: Ask Claude to think step-by-step for complex problems  </li> <li>Constitutional AI: Claude will refuse harmful requests by design</li> <li>Context Usage: Don't hesitate to use the full 200K context when needed</li> <li>Model Selection: Opus for complex tasks, Haiku for speed, Sonnet for balance</li> </ol>"},{"location":"reference/providers/anthropic/#see-also","title":"See Also","text":"<ul> <li>Agent API - Core agent functionality</li> <li>Provider Switching - Dynamic provider changes</li> <li>Anthropic Docs - Official Anthropic documentation</li> </ul>"},{"location":"reference/providers/ollama/","title":"Ollama Provider Reference","text":"<p>The Ollama provider enables running LLMs locally with complete privacy and no API costs.</p>"},{"location":"reference/providers/ollama/#configuration","title":"Configuration","text":""},{"location":"reference/providers/ollama/#prerequisites","title":"Prerequisites","text":"<p>Install Ollama: <pre><code># macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows\n# Download from https://ollama.ai/download\n</code></pre></p>"},{"location":"reference/providers/ollama/#start-ollama-service","title":"Start Ollama Service","text":"<pre><code># Start Ollama\nollama serve\n\n# Pull models you want to use\nollama pull llama2\nollama pull mistral\nollama pull codellama\n</code></pre>"},{"location":"reference/providers/ollama/#environment-variables","title":"Environment Variables","text":"<pre><code>export OLLAMA_HOST=\"http://localhost:11434\"  # Default\nexport OLLAMA_TIMEOUT=\"300\"  # Timeout in seconds\n</code></pre>"},{"location":"reference/providers/ollama/#initialization","title":"Initialization","text":"<pre><code>from agenticraft import Agent\n\n# Explicit provider required for Ollama\nagent = Agent(\n    name=\"LocalBot\",\n    provider=\"ollama\",\n    model=\"llama2\"\n)\n\n# Custom host\nagent = Agent(\n    name=\"RemoteBot\",\n    provider=\"ollama\",\n    model=\"mistral\",\n    base_url=\"http://192.168.1.100:11434\"\n)\n</code></pre>"},{"location":"reference/providers/ollama/#supported-models","title":"Supported Models","text":"Model Size Description Best For <code>llama2</code> 7B Meta's Llama 2 General purpose <code>llama2:13b</code> 13B Larger Llama 2 Better quality <code>llama2:70b</code> 70B Largest Llama 2 Best quality <code>mistral</code> 7B Mistral AI model Fast, efficient <code>mixtral</code> 8x7B MoE model High quality <code>codellama</code> 7B Code-focused Programming tasks <code>phi-2</code> 2.7B Microsoft's small model Resource-constrained <code>neural-chat</code> 7B Intel's fine-tuned Conversational <code>starling-lm</code> 7B Berkeley's model Instruction following"},{"location":"reference/providers/ollama/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"reference/providers/ollama/#model-management","title":"Model Management","text":"<pre><code>from agenticraft.providers.ollama import OllamaProvider\n\n# List available models\nprovider = OllamaProvider()\nmodels = provider.list_models()\nfor model in models:\n    print(f\"{model['name']}: {model['size']}\")\n\n# Pull a new model\nprovider.pull_model(\"llama2:13b\")\n\n# Delete a model\nprovider.delete_model(\"old-model\")\n</code></pre>"},{"location":"reference/providers/ollama/#custom-models","title":"Custom Models","text":"<p>Load custom GGUF models:</p> <pre><code># Create custom modelfile\nmodelfile = \"\"\"\nFROM ./my-model.gguf\nPARAMETER temperature 0.8\nPARAMETER top_p 0.9\nSYSTEM You are a helpful assistant.\n\"\"\"\n\n# Create model\nprovider.create_model(\"my-custom-model\", modelfile)\n\n# Use it\nagent = Agent(\n    name=\"CustomBot\",\n    provider=\"ollama\",\n    model=\"my-custom-model\"\n)\n</code></pre>"},{"location":"reference/providers/ollama/#embedding-models","title":"Embedding Models","text":"<pre><code># Use embedding models\nagent = Agent(\n    name=\"EmbeddingBot\",\n    provider=\"ollama\",\n    model=\"nomic-embed-text\",\n    task=\"embedding\"\n)\n\nembeddings = agent.embed([\"text1\", \"text2\", \"text3\"])\n</code></pre>"},{"location":"reference/providers/ollama/#streaming","title":"Streaming","text":"<pre><code>agent = Agent(\n    name=\"StreamBot\",\n    provider=\"ollama\",\n    model=\"llama2\",\n    stream=True\n)\n\n# Stream responses\nfor chunk in agent.run_stream(\"Tell me a story\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/providers/ollama/#configuration-options","title":"Configuration Options","text":"<pre><code>agent = Agent(\n    name=\"ConfiguredOllama\",\n    provider=\"ollama\",\n    model=\"llama2\",\n\n    # Ollama-specific options\n    temperature=0.8,        # 0.0-1.0\n    top_p=0.9,             # Nucleus sampling\n    top_k=40,              # Top-k sampling\n    repeat_penalty=1.1,    # Penalize repetition\n    seed=42,               # Reproducible outputs\n    num_predict=2048,      # Max tokens to generate\n    num_ctx=4096,          # Context window size\n    num_batch=512,         # Batch size for prompt eval\n    num_gpu=1,             # GPUs to use\n    main_gpu=0,            # Main GPU\n    low_vram=False,        # Low VRAM mode\n    f16_kv=True,           # Use f16 for K,V cache\n    vocab_only=False,      # Only load vocabulary\n    use_mmap=True,         # Use memory mapping\n    use_mlock=False,       # Lock model in memory\n\n    # Connection settings\n    timeout=300,           # Request timeout\n    keep_alive=\"5m\"        # Keep model loaded\n)\n</code></pre>"},{"location":"reference/providers/ollama/#performance-optimization","title":"Performance Optimization","text":""},{"location":"reference/providers/ollama/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># Use GPU acceleration\nagent = Agent(\n    name=\"GPUBot\",\n    provider=\"ollama\",\n    model=\"llama2\",\n    num_gpu=1  # Number of GPU layers\n)\n\n# Check GPU usage\ninfo = agent.get_model_info()\nprint(f\"GPU layers: {info.get('gpu_layers')}\")\n</code></pre>"},{"location":"reference/providers/ollama/#memory-management","title":"Memory Management","text":"<pre><code># Low memory configuration\nagent = Agent(\n    name=\"LowMemBot\",\n    provider=\"ollama\",\n    model=\"phi-2\",  # Smaller model\n    num_ctx=2048,   # Smaller context\n    num_batch=256,  # Smaller batch\n    low_vram=True   # Enable low VRAM mode\n)\n</code></pre>"},{"location":"reference/providers/ollama/#model-preloading","title":"Model Preloading","text":"<pre><code># Keep model loaded in memory\nagent = Agent(\n    name=\"FastBot\",\n    provider=\"ollama\",\n    model=\"mistral\",\n    keep_alive=\"30m\"  # Keep loaded for 30 minutes\n)\n\n# Preload model\nagent.preload()\n</code></pre>"},{"location":"reference/providers/ollama/#privacy-features","title":"Privacy Features","text":""},{"location":"reference/providers/ollama/#fully-offline-operation","title":"Fully Offline Operation","text":"<pre><code>class PrivateAssistant:\n    def __init__(self):\n        # Ensure Ollama is running locally\n        self.agent = Agent(\n            name=\"PrivateBot\",\n            provider=\"ollama\",\n            model=\"llama2\",\n            base_url=\"http://localhost:11434\"\n        )\n\n        # Verify no external connections\n        self.agent.verify_local_only = True\n\n    def process_sensitive_data(self, data: str):\n        \"\"\"Process data with complete privacy.\"\"\"\n        # Data never leaves your machine\n        return self.agent.run(f\"Analyze this confidential data: {data}\")\n</code></pre>"},{"location":"reference/providers/ollama/#custom-privacy-models","title":"Custom Privacy Models","text":"<pre><code># Create a privacy-focused model\nmodelfile = \"\"\"\nFROM llama2\nPARAMETER temperature 0.7\nSYSTEM You are a privacy-focused assistant. Never ask for or store personal information.\n\"\"\"\n\nprovider = OllamaProvider()\nprovider.create_model(\"privacy-llama\", modelfile)\n</code></pre>"},{"location":"reference/providers/ollama/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection: Choose model size based on available resources</li> <li>Resource Management: Monitor CPU/GPU usage and memory</li> <li>Context Length: Adjust context size to fit your hardware</li> <li>Batch Processing: Use appropriate batch sizes for your system</li> <li>Model Caching: Keep frequently used models loaded</li> </ol>"},{"location":"reference/providers/ollama/#complete-example","title":"Complete Example","text":"<pre><code>import psutil\nfrom agenticraft import Agent, tool\nfrom typing import Dict\n\nclass LocalAssistant:\n    def __init__(self):\n        # Check system resources\n        self._check_resources()\n\n        # Select model based on available resources\n        model = self._select_model()\n\n        self.agent = Agent(\n            name=\"LocalAssistant\",\n            provider=\"ollama\",\n            model=model,\n            temperature=0.7,\n            num_ctx=4096,\n            num_gpu=1 if self._has_gpu() else 0,\n            tools=self._create_tools()\n        )\n\n    def _check_resources(self):\n        \"\"\"Check available system resources.\"\"\"\n        ram = psutil.virtual_memory().total / (1024**3)  # GB\n        print(f\"Available RAM: {ram:.1f} GB\")\n\n        try:\n            import torch\n            if torch.cuda.is_available():\n                print(f\"GPU available: {torch.cuda.get_device_name()}\")\n        except ImportError:\n            print(\"No GPU detected\")\n\n    def _has_gpu(self) -&gt; bool:\n        \"\"\"Check if GPU is available.\"\"\"\n        try:\n            import torch\n            return torch.cuda.is_available()\n        except ImportError:\n            return False\n\n    def _select_model(self) -&gt; str:\n        \"\"\"Select model based on resources.\"\"\"\n        ram = psutil.virtual_memory().total / (1024**3)\n\n        if ram &gt;= 32:\n            return \"llama2:13b\"  # 13B model\n        elif ram &gt;= 16:\n            return \"llama2\"      # 7B model\n        else:\n            return \"phi-2\"       # 2.7B model\n\n    def _create_tools(self):\n        @tool\n        def system_info() -&gt; str:\n            \"\"\"Get current system information.\"\"\"\n            cpu = psutil.cpu_percent()\n            ram = psutil.virtual_memory().percent\n            return f\"CPU: {cpu}%, RAM: {ram}%\"\n\n        @tool\n        def read_local_file(path: str) -&gt; str:\n            \"\"\"Read a local file.\"\"\"\n            try:\n                with open(path, 'r') as f:\n                    return f.read()\n            except Exception as e:\n                return f\"Error reading file: {e}\"\n\n        return [system_info, read_local_file]\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"Process a chat message locally.\"\"\"\n        response = self.agent.run(message)\n        return response.content\n\n    def analyze_file(self, file_path: str, analysis_type: str = \"summary\"):\n        \"\"\"Analyze a local file privately.\"\"\"\n        prompt = f\"\"\"\n        Please {analysis_type} the following file content:\n\n        {self.agent.run(f\"Read file: {file_path}\").content}\n\n        Provide a detailed {analysis_type}.\n        \"\"\"\n\n        return self.agent.run(prompt).content\n\n    def switch_model(self, model: str):\n        \"\"\"Switch to a different local model.\"\"\"\n        try:\n            # Test if model is available\n            test_agent = Agent(\n                name=\"Test\",\n                provider=\"ollama\",\n                model=model\n            )\n            test_agent.run(\"test\")\n\n            # Switch main agent\n            self.agent.model = model\n            print(f\"Switched to {model}\")\n        except Exception as e:\n            print(f\"Model {model} not available: {e}\")\n            print(\"Run: ollama pull {model}\")\n\n# Usage\nassistant = LocalAssistant()\n\n# Private conversation\nresponse = assistant.chat(\"Help me analyze my personal finances\")\nprint(response)\n\n# Analyze local files\nanalysis = assistant.analyze_file(\n    \"/path/to/document.txt\",\n    analysis_type=\"detailed summary\"\n)\n\n# Switch models based on task\nassistant.switch_model(\"codellama\")  # For coding tasks\ncode = assistant.chat(\"Write a Python function to sort a list\")\n</code></pre>"},{"location":"reference/providers/ollama/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/providers/ollama/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Connection Refused <pre><code># Ensure Ollama is running\nollama serve\n</code></pre></p> </li> <li> <p>Model Not Found <pre><code># Pull the model first\nollama pull llama2\n</code></pre></p> </li> <li> <p>Out of Memory</p> </li> <li>Use smaller models (phi-2, tinyllama)</li> <li>Reduce context size</li> <li> <p>Enable low_vram mode</p> </li> <li> <p>Slow Generation</p> </li> <li>Enable GPU acceleration</li> <li>Use smaller models</li> <li>Reduce context window</li> </ol>"},{"location":"reference/providers/ollama/#model-recommendations","title":"Model Recommendations","text":"Use Case Recommended Model Min RAM General chat llama2 (7B) 8GB Code generation codellama 8GB Fast responses mistral 8GB Resource-constrained phi-2 4GB High quality llama2:13b 16GB Best quality llama2:70b 64GB"},{"location":"reference/providers/ollama/#see-also","title":"See Also","text":"<ul> <li>Agent API - Core agent functionality</li> <li>Provider Switching - Dynamic provider changes</li> <li>Ollama Docs - Official Ollama documentation</li> </ul>"},{"location":"reference/providers/openai/","title":"OpenAI Provider Reference","text":"<p>The OpenAI provider supports GPT-4, GPT-3.5, and other OpenAI models.</p>"},{"location":"reference/providers/openai/#configuration","title":"Configuration","text":""},{"location":"reference/providers/openai/#environment-variables","title":"Environment Variables","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_ORG_ID=\"org-...\"  # Optional\n</code></pre>"},{"location":"reference/providers/openai/#initialization","title":"Initialization","text":"<pre><code>from agenticraft import Agent\n\n# Auto-detection from model name\nagent = Agent(name=\"GPT\", model=\"gpt-4\")\n\n# Explicit provider\nagent = Agent(\n    name=\"GPT\",\n    provider=\"openai\",\n    model=\"gpt-4\",\n    api_key=\"sk-...\"  # Optional, uses env var if not provided\n)\n</code></pre>"},{"location":"reference/providers/openai/#supported-models","title":"Supported Models","text":"Model Description Context Window Best For <code>gpt-4</code> Most capable model 8K tokens Complex reasoning, analysis <code>gpt-4-32k</code> Extended context 32K tokens Long documents <code>gpt-4-turbo</code> Faster, cheaper GPT-4 128K tokens Balanced performance <code>gpt-3.5-turbo</code> Fast and efficient 16K tokens Simple tasks, high volume <code>gpt-3.5-turbo-16k</code> Extended context 16K tokens Longer conversations"},{"location":"reference/providers/openai/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"reference/providers/openai/#function-calling","title":"Function Calling","text":"<p>OpenAI models support native function calling:</p> <pre><code>from agenticraft import Agent, tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    return f\"Weather in {location}: Sunny, 72\u00b0F\"\n\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt-4\",\n    tools=[get_weather]\n)\n\n# OpenAI automatically handles function calling\nresponse = agent.run(\"What's the weather in NYC?\")\n</code></pre>"},{"location":"reference/providers/openai/#streaming-responses","title":"Streaming Responses","text":"<pre><code>agent = Agent(\n    name=\"Streamer\",\n    model=\"gpt-4\",\n    stream=True\n)\n\nfor chunk in agent.run_stream(\"Tell me a story\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/providers/openai/#response-format","title":"Response Format","text":"<pre><code># JSON mode\nagent = Agent(\n    name=\"JSONBot\",\n    model=\"gpt-4-turbo\",\n    response_format={\"type\": \"json_object\"}\n)\n\nresponse = agent.run(\"List 3 colors as JSON\")\n# Returns valid JSON\n</code></pre>"},{"location":"reference/providers/openai/#vision-capabilities","title":"Vision Capabilities","text":"<pre><code># GPT-4 Vision\nagent = Agent(\n    name=\"VisionBot\",\n    model=\"gpt-4-vision-preview\"\n)\n\nresponse = agent.run(\n    prompt=\"What's in this image?\",\n    images=[\"path/to/image.jpg\"]\n)\n</code></pre>"},{"location":"reference/providers/openai/#configuration-options","title":"Configuration Options","text":"<pre><code>agent = Agent(\n    name=\"Configured\",\n    provider=\"openai\",\n    model=\"gpt-4\",\n\n    # OpenAI-specific options\n    temperature=0.7,        # 0.0-2.0\n    max_tokens=2000,       # Max response length\n    top_p=1.0,            # Nucleus sampling\n    frequency_penalty=0.0, # -2.0 to 2.0\n    presence_penalty=0.0,  # -2.0 to 2.0\n    stop=[\"\\n\\n\"],        # Stop sequences\n    n=1,                  # Number of completions\n    logprobs=True,        # Include log probabilities\n\n    # Retry configuration\n    max_retries=3,\n    timeout=30\n)\n</code></pre>"},{"location":"reference/providers/openai/#error-handling","title":"Error Handling","text":"<pre><code>from agenticraft import Agent, ProviderError\n\ntry:\n    agent = Agent(name=\"Bot\", model=\"gpt-4\")\n    response = agent.run(\"Hello\")\nexcept ProviderError as e:\n    if \"rate_limit\" in str(e):\n        print(\"Rate limit reached, waiting...\")\n    elif \"api_key\" in str(e):\n        print(\"Invalid API key\")\n    else:\n        print(f\"OpenAI error: {e}\")\n</code></pre>"},{"location":"reference/providers/openai/#cost-optimization","title":"Cost Optimization","text":""},{"location":"reference/providers/openai/#model-selection-by-task","title":"Model Selection by Task","text":"<pre><code>class SmartAssistant:\n    def __init__(self):\n        self.agent = Agent(name=\"Smart\", model=\"gpt-3.5-turbo\")\n\n    def process(self, task: str, complexity: str):\n        if complexity == \"high\":\n            self.agent.model = \"gpt-4\"\n        elif complexity == \"medium\":\n            self.agent.model = \"gpt-4-turbo\"\n        else:\n            self.agent.model = \"gpt-3.5-turbo\"\n\n        return self.agent.run(task)\n</code></pre>"},{"location":"reference/providers/openai/#token-usage-tracking","title":"Token Usage Tracking","text":"<pre><code>response = agent.run(\"Generate a report\")\n\n# Access token usage\ntokens = response.metadata.get(\"usage\", {})\nprint(f\"Prompt tokens: {tokens.get('prompt_tokens')}\")\nprint(f\"Completion tokens: {tokens.get('completion_tokens')}\")\nprint(f\"Total tokens: {tokens.get('total_tokens')}\")\n</code></pre>"},{"location":"reference/providers/openai/#best-practices","title":"Best Practices","text":"<ol> <li>API Key Security: Use environment variables, never hardcode keys</li> <li>Rate Limiting: Implement exponential backoff for retries</li> <li>Context Management: Monitor token usage to stay within limits</li> <li>Model Selection: Use GPT-3.5-Turbo for simple tasks, GPT-4 for complex ones</li> <li>Error Handling: Always handle API errors gracefully</li> </ol>"},{"location":"reference/providers/openai/#complete-example","title":"Complete Example","text":"<pre><code>import os\nfrom agenticraft import Agent, tool\nfrom typing import List\n\nclass OpenAIAssistant:\n    def __init__(self):\n        # Ensure API key is set\n        if not os.getenv(\"OPENAI_API_KEY\"):\n            raise ValueError(\"OPENAI_API_KEY not set\")\n\n        # Create agent with optimal settings\n        self.agent = Agent(\n            name=\"OpenAIAssistant\",\n            provider=\"openai\",\n            model=\"gpt-4-turbo\",\n            temperature=0.7,\n            max_tokens=2000,\n            tools=self._create_tools()\n        )\n\n    def _create_tools(self) -&gt; List:\n        @tool\n        def search_docs(query: str) -&gt; str:\n            \"\"\"Search internal documentation.\"\"\"\n            # Implementation\n            return f\"Found docs about: {query}\"\n\n        @tool\n        def calculate(expression: str) -&gt; str:\n            \"\"\"Perform calculations.\"\"\"\n            try:\n                result = eval(expression, {\"__builtins__\": {}})\n                return str(result)\n            except:\n                return \"Invalid expression\"\n\n        return [search_docs, calculate]\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"Process a chat message.\"\"\"\n        try:\n            response = self.agent.run(message)\n\n            # Track usage for cost monitoring\n            usage = response.metadata.get(\"usage\", {})\n            self._log_usage(usage)\n\n            return response.content\n\n        except ProviderError as e:\n            # Handle specific errors\n            if \"rate_limit\" in str(e):\n                # Switch to cached responses or queue\n                return \"I'm a bit busy right now, please try again in a moment.\"\n            raise\n\n    def _log_usage(self, usage: dict):\n        \"\"\"Log token usage for monitoring.\"\"\"\n        total = usage.get(\"total_tokens\", 0)\n        cost = self._estimate_cost(total)\n        print(f\"Tokens: {total}, Est. cost: ${cost:.4f}\")\n\n    def _estimate_cost(self, tokens: int) -&gt; float:\n        \"\"\"Estimate cost based on current pricing.\"\"\"\n        # GPT-4-Turbo pricing (example)\n        return (tokens / 1000) * 0.01\n\n# Usage\nassistant = OpenAIAssistant()\nresponse = assistant.chat(\"Help me analyze this data and create a chart\")\nprint(response)\n</code></pre>"},{"location":"reference/providers/openai/#see-also","title":"See Also","text":"<ul> <li>Agent API - Core agent functionality</li> <li>Provider Switching - Dynamic provider changes</li> <li>OpenAI API Docs - Official OpenAI documentation</li> </ul>"}]}